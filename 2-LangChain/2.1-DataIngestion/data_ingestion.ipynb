{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89f0191",
   "metadata": {},
   "source": [
    "#### **Data Ingestion or Data Loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.document_loaders.text.TextLoader"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text loader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "\n",
    "loader = TextLoader('../speech.txt')\n",
    "type(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85aa3345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_docs = loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85fb8a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Using cached pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9495b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 0, 'page_label': '1'}, page_content='MACHINE\\nLEARNING\\nDEEP\\nLEARNING\\nPYTHON +\\nSTATS\\nCOMPUTER VISIONNATURAL LANGUAGE PROCESSING\\nGENERATIVE AI\\nRETRIEVAL AUGUMENT GENERATION\\nVECTOR DB'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 1, 'page_label': '2'}, page_content='This course is designed for aspiring data scientists, machine learning enthusiasts, and\\nprofessionals looking to build expertise in Python programming, data analysis, machine learning,\\nand deep learning. Whether you are just starting or have some experience, this comprehensive\\ncourse will equip you with the skills needed to work with real-world datasets, apply machine\\nlearning algorithms, and deploy AI solutions. By the end of the course, you’ll have a solid\\nfoundation in AI, a portfolio of end-to-end projects, and the confidence to tackle complex\\nchallenges in data science and AI.\\nLearning Objectives\\nMaster Python Programming: Understand Python fundamentals, including data types,\\ncontrol structures, and object-oriented programming, to write efficient and reusable\\ncode.\\nHandle Data with Pandas and NumPy: Acquire skills to manipulate, clean, and\\npreprocess large datasets using Pandas and NumPy for data analysis tasks.\\nVisualize Data: Create compelling data visualizations using libraries such as Matplotlib,\\nSeaborn, and Plotly to present insights effectively.\\nUnderstand SQL & NoSQL: Gain expertise in both relational (SQL) and non-relational\\n(NoSQL) databases, including MongoDB, for storing, querying, and managing data.\\nGrasp Statistics and Probability: Understand the core concepts of statistics,\\nprobability, and hypothesis testing, applying them to data analysis and machine\\nlearning.\\nMaster Machine Learning Techniques: Learn key machine learning algorithms,\\nincluding supervised, unsupervised, and ensemble methods, and apply them to real-\\nworld problems.\\nDive into Deep Learning: Develop a strong understanding of neural networks, CNNs,\\nRNNs, and transformers, with hands-on implementation for advanced AI tasks.\\nExplore Generative AI & Vector Databases: Learn the concepts and applications of\\ngenerative models, vector databases, and retrieval-augmented generation to handle\\ncomplex AI systems.\\nBuild Real-World Projects: Implement end-to-end machine learning and AI projects,\\nfrom data preprocessing to model deployment, integrating concepts from multiple\\nmodules.\\nUltimate Data Science & GenAI Bootcamp       Page  2'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 2, 'page_label': '3'}, page_content=\"Course Information\\nNo prerequisites are required for this course. The curriculum covers everything from the\\nbasics of Python programming, statistics, and machine learning to advanced topics in deep\\nlearning, NLP, and generative AI. Whether you're a beginner or have some prior experience,\\nthe course will ensure you gain the skills needed to succeed.\\nEstimated Time\\n8 months 6hrs/week*\\nRequired Skill Level\\nBegineer\\nThe course is designed to be completed over a duration of approximately 7 to 8 months, providing\\nan in-depth exploration from Python basics to GenAI, with plenty of time for practical\\nimplementation and real-world applications.\\nPrerequisites\\nUltimate Data Science & GenAI Bootcamp       Page  3\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 3, 'page_label': '4'}, page_content='Course Instructors\\nSunny SavitaGenAI Engineer\\nLinkedin\\nKrish NaikChief AI Engineer\\nLinkedin\\nUltimate Data Science & GenAI Bootcamp       Page  4\\nSourangshu PalSenior Data Scientist\\nLinkedin\\nMonal KumarData Scientist\\nLinkedin\\nMayank AggrawalSenior ML Engineer\\nLinkedin\\nDarius B.Head of Product\\nLinkedin'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 4, 'page_label': '5'}, page_content=\"In this module, you’ll get a solid introduction to Python, covering essential programming concepts\\nsuch as variables, data types, operators, and control flow. You’ll learn how to manipulate strings,\\nlists, dictionaries, and other basic data structures. The module will also guide you through writing\\nsimple functions and using loops and conditionals effectively. By the end, you'll have a strong\\nunderstanding of Python syntax, preparing you to tackle more complex programming challenges\\nand form a foundation for learning advanced concepts.\\nUltimate Data Science & GenAI Bootcamp       Page  5\\nPython Foundations\\nModule 1\\nTopics\\nIntroduction to Python Comparison with other programming\\nlanguages, Python objects: Numbers,\\nBooleans, and Strings\\nData Structures & Operations Container objects and mutability,\\nOperators, Operator precedence and\\nassociativity\\nControl Flow Conditional statements, Loops, break\\nand continue statements\\nString Manipulation Basics of string objects, Inbuilt string\\nmethods, Splitting and joining strings,\\nString formatting functions\\nLists & Collections List methods, list comprehension, Lists as\\nstacks and queues, Tuples, sets, and\\ndictionaries, Dictionary comprehensions\\nand view objects\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 5, 'page_label': '6'}, page_content='Topics\\nFunctions & Iterators Function basics and parameter passing,\\nIterators and generator functions,\\nLambda functions, map(), reduce(),\\nfilter()\\nPython Foundations\\nModule 1\\nUltimate Data Science & GenAI Bootcamp       Page  6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 6, 'page_label': '7'}, page_content='This module takes your Python skills further by diving into object-oriented programming (OOP)\\nconcepts like classes, inheritance, and polymorphism. You’ll also explore more advanced topics\\nsuch as decorators, lambda functions, iterators, and generator functions. Additionally, we cover\\nexception handling, file operations, and working with modules and libraries. By the end, you will be\\ncomfortable building more sophisticated Python applications and writing efficient, reusable code.\\nAdvanced Python Programming\\nModule 2\\nTopics\\nObject-Oriented Programming (OOP) OOP basics and class creation,\\nInheritance, Polymorphism,\\nEncapsulation, and Abstraction,\\nDecorators, class methods, and static\\nmethods, Special (Magic/Dunder)\\nmethods, Property decorators: Getters,\\nsetters, and delete methods\\nFile Handling & Logging Reading and writing files, Buffered read\\nand write operations, more file methods,\\nLogging and debugging\\nModules & Exception Handling Importing modules and using them\\neffectively, Exception handling\\nConcurrency & Parallelism Introduction to multithreading,\\nMultiprocessing for performance\\noptimization\\nUltimate Data Science & GenAI Bootcamp       Page  7'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 7, 'page_label': '8'}, page_content='In this module, you will master the core aspects of data manipulation using Pandas. You’ll learn\\nhow to work with Series, DataFrames, and Panels, as well as perform data selection, filtering, and\\nsorting. The module covers critical tasks like handling missing data, reindexing, and applying\\nstatistical functions to datasets. You’ll also gain hands-on experience with data visualization and\\nadvanced indexing techniques, empowering you to efficiently analyze and manipulate complex\\ndatasets.\\nMastering Data Handling with Pandas\\nTopics\\nData Structures & Fundamentals Series, DataFrame, Panel, Basic\\nFunctionality, Indexing & Selecting, Re-\\nindexing, Iteration\\nData Operations & Transformations Sorting, Working with Text Data, Options\\n& Customization, Categorical Data, Date\\nFunctionality, Time Delta\\nData Analysis & Statistical Functions Data Statistical Functions, Window\\nFunctions\\nReading, Writing & Visualization Reading Data from Different File\\nSystems, Visualization, Tools\\nUltimate Data Science & GenAI Bootcamp       Page  8\\nModule 3'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 8, 'page_label': '9'}, page_content='This module introduces you to NumPy, a key library for numerical computing in Python. You’ll learn\\nhow to create and manipulate NumPy arrays, perform advanced indexing, and understand\\nbroadcasting. The module covers essential mathematical and statistical functions, including array\\nmanipulations, binary operations, and vectorized operations. By the end, you’ll have the skills to\\nefficiently perform complex numerical computations and leverage NumPy for machine learning\\nand deep learning applications.\\nMastering NumPy\\nTopics\\nNumPy Basics & Array Creation NdArray Object, Data Types, Array\\nAttributes, Array Creation Routines,\\nArray from Existing Data, Data Array from\\nNumerical Ranges\\nIndexing, Slicing & Advanced Indexing Indexing & Slicing, Advanced Indexing\\nArray Operations & Manipulation Array Manipulation, Binary Operators,\\nString Functions, Arithmetic Operations,\\nMathematical Functions\\nMathematical & Statistical Analysis Statistical Functions, Sort, Search &\\nCounting Functions, Matrix Library,\\nLinear Algebra\\nAdvanced Concepts Broadcasting, Iterating Over Array, Byte\\nSwapping, Copies & Views\\nUltimate Data Science & GenAI Bootcamp       Page  9\\nModule 4'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 9, 'page_label': '10'}, page_content=\"In this module, you’ll learn how to visualize data effectively using Python's popular libraries,\\nMatplotlib, Seaborn, and Plotly. You’ll cover essential plot types like line charts, bar graphs, and\\nscatter plots, and learn how to customize these visualizations to highlight key insights. Additionally,\\nthe module teaches you how to visualize statistical data, correlations, and distributions, helping\\nyou communicate data-driven findings in a visually compelling way.\\nData Visualization with Python\\nTopics\\nIntroduction to Data Visualization Overview of Data Visualization, Principles\\nof Good Visualization\\nMatplotlib Introduction to Matplotlib, Creating\\nBasic Plots (Line, Bar, Scatter),\\nCustomizing Axes, Titles, Legends, and\\nLabels, Working with Subplots, Saving\\nand Exporting Figures\\nSeaborn Introduction to Seaborn, Visualizing\\nDistributions, Relationship Plots\\n(Pairplots, Heatmaps), Categorical Data\\nVisualization, Advanced Plot\\nCustomizations\\nPlotly Introduction to Plotly, Creating\\nInteractive Plots (Line, Bar, Scatter),\\nCustomizing Plots, Dashboards and\\nInteractive Layouts, Plotly Express\\nUltimate Data Science & GenAI Bootcamp       Page  10\\nModule 5\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 10, 'page_label': '11'}, page_content='This module dives into advanced SQL techniques, including complex queries, joins, and indexing\\nfor efficient data retrieval. You’ll learn how to implement stored procedures, triggers, and\\nfunctions, and explore the use of window functions and partitions. The module covers key\\ndatabase design concepts like primary and foreign keys and normalization. By the end, you’ll be\\nproficient in managing large-scale databases and optimizing SQL queries for performance.\\nAdvanced SQL and Database Management\\nTopics\\nIntroduction to SQL Introduction to SQL, SQL Queries:\\nSELECT, INSERT, UPDATE, DELETE\\nSQL Functions and Procedures SQL Functions (Aggregate, Scalar),\\nStored Procedures, User-defined\\nFunctions (UDFs), Function and\\nProcedure Syntax\\nDatabase Constraints Primary and Foreign Keys, Data Integrity,\\nReferential Integrity\\nAdvanced SQL Techniques Window Functions, Partitioning, CTE\\n(Common Table Expressions), Indexing\\nSQL Joins and Unions Inner Join, Left Join, Right Join, Full Outer\\nJoin, Cross Join, Union\\nTriggers and Case Statements Triggers (Before, After), CASE\\nStatements, Conditional Logic\\nUltimate Data Science & GenAI Bootcamp       Page  11\\nModule 6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 11, 'page_label': '12'}, page_content='Advanced SQL and Database Management\\nTopics\\nNormalization and Pivoting Normalization Forms (1NF, 2NF, 3NF),\\nPivot Tables, Data Aggregation\\nUltimate Data Science & GenAI Bootcamp       Page  12\\nModule 6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 12, 'page_label': '13'}, page_content=\"In this module, you'll explore the world of NoSQL databases with MongoDB. You'll learn how to\\ncreate and manage databases, collections, and documents, and perform CRUD operations. The\\nmodule covers querying, sorting, and indexing, providing a comprehensive understanding of\\nMongoDB's flexible data model. By the end, you’ll be able to efficiently work with NoSQL\\ndatabases, particularly for use cases that involve unstructured or semi-structured data.\\nIntroduction to NoSQL with MongoDB\\nTopics\\nGetting Started with MongoDB MongoDB Introduction, Setting up\\nMongoDB, MongoDB Shell Commands\\nDatabase and Collection Management MongoDB Create Database, MongoDB\\nCreate Collection\\nCRUD Operations MongoDB Insert, MongoDB Find,\\nMongoDB Update, MongoDB Delete\\nQuerying MongoDB MongoDB Query, MongoDB Sort,\\nMongoDB Limit\\nManaging Collections MongoDB Drop Collection, MongoDB\\nDelete (Specific)\\nUltimate Data Science & GenAI Bootcamp       Page  13\\nModule 7\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 13, 'page_label': '14'}, page_content='This module provides a foundation in statistics and probability, covering essential terms, concepts,\\nand methods. You’ll learn about different types of data, levels of measurement, and key statistical\\nmeasures like mean, median, variance, and standard deviation. The module introduces random\\nvariables, probability distributions, and various types of probability functions, giving you a strong\\nbase to analyze and interpret data from a statistical perspective.\\nFoundations of Statistics and Probability\\nTopics\\nIntroduction to Statistics Introduction to Basic Statistics Terms,\\nTypes of Statistics, Types of Data, Levels\\nof Measurement, Measures of Central\\nTendency, Measures of Dispersion\\nExploring Random Variables and\\nProbability\\nRandom Variables, Set Theory,\\nSkewness, Covariance and Correlation,\\nProbability Density/Distribution Function\\nDistributions and Their Applications Types of Probability Distributions,\\nBinomial Distribution, Poisson\\nDistribution, Normal Distribution\\n(Gaussian Distribution), Probability\\nDensity Function and Mass Function,\\nCumulative Density Function, Examples\\nof Normal Distribution, Bernoulli\\nDistribution, Uniform Distribution\\nStatistical Inference Z-Statistics, Central Limit Theorem,\\nEstimation, Hypothesis Testing\\nUltimate Data Science & GenAI Bootcamp       Page  14\\nModule 8'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 14, 'page_label': '15'}, page_content=\"In this module, you'll delve deeper into statistical inference techniques, including hypothesis\\ntesting, confidence intervals, and the types of errors in statistical tests. You’ll explore advanced\\nconcepts like P-values, T-tests, and Chi-square tests, learning how to interpret results in the\\ncontext of real-world data. By the end, you’ll be equipped to conduct sophisticated statistical\\nanalysis and make informed decisions based on data-driven evidence.\\nAdvanced Statistical Inference and\\nHypothesis Testing\\nTopics\\nHypothesis Testing and Errors Hypothesis Testing Mechanism, Type 1 &\\nType 2 Error, T-Tests vs. Z-Tests:\\nOverview, When to Use a T-Test vs. Z-\\nTest\\nStatistical Distributions and Tests T-Stats, Student T Distribution, Chi-\\nSquare Test, Chi-Square Distribution\\nUsing Python, Chi-Square for Goodness\\nof Fit Test\\nBayesian Statistics and Confidence\\nIntervals\\nBayes Statistics (Bayes Theorem),\\nConfidence Interval (CI), Confidence\\nIntervals and the Margin of Error,\\nInterpreting Confidence Levels and\\nConfidence Intervals\\nStatistical Significance and\\nInterpretation\\nP-Value, T-Stats vs. Z-Stats: Overview\\nUltimate Data Science & GenAI Bootcamp       Page  15\\nModule 9\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 15, 'page_label': '16'}, page_content='This module covers essential techniques for preparing and transforming data before applying\\nmachine learning models. You’ll learn how to handle missing values, deal with imbalanced data,\\nand scale or encode features. The module also explores methods for handling outliers, feature\\nselection (including forward/backward elimination), and dimensionality reduction techniques. By\\nthe end, you’ll be proficient in preparing high-quality datasets that are ready for modeling.\\nFeature Engineering and Data\\nPreprocessing\\nTopics\\nHandling Missing and Imbalanced\\nData\\nHandling Missing Data, Handling\\nImbalanced Data\\nOutliers and Scaling Handling Outliers, Feature Scaling\\nData Transformation and Encoding Data Encoding\\nFeature Selection Techniques Backward Elimination, Forward\\nElimination, Recursive Feature\\nElimination\\nCorrelation and Multicollinearity Covariance and Correlation, VIF\\nUltimate Data Science & GenAI Bootcamp       Page  16\\nModule 10'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 16, 'page_label': '17'}, page_content='In this module, you’ll learn how to perform Exploratory Data Analysis (EDA) to uncover patterns,\\ntrends, and relationships in your data. You’ll master techniques for visualizing distributions,\\nidentifying correlations, and detecting anomalies. The module emphasizes the importance of\\nsummary statistics, data cleaning, and feature engineering. By the end, you’ll be able to extract\\nmeaningful insights from raw data and prepare it for further analysis or modeling.\\nExploratory Data Analysis (EDA) for\\nDetailed Insights\\nTopics\\nTrend Analysis and Segmentation Analyzing Bike Sharing Trends,\\nCustomer Segmentation and Effective\\nCross-Selling\\nSentiment and Quality Analysis Analyzing Movie Reviews Sentiment,\\nAnalyzing Wine Types and Quality\\nRecommendation and Forecasting Analyzing Music Trends and\\nRecommendations, Forecasting Stock\\nand Commodity Prices\\nUltimate Data Science & GenAI Bootcamp       Page  17\\nModule 11'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 17, 'page_label': '18'}, page_content='This module provides a comprehensive introduction to machine learning, covering key algorithms\\nand techniques. You’ll learn the differences between supervised and unsupervised learning, as\\nwell as the core concepts of regression, classification, and clustering. The module introduces\\nmodel evaluation metrics like accuracy, precision, recall, and F1-score, giving you the foundation to\\nunderstand and implement machine learning models in real-world scenarios.\\nMachine Learning Foundations and\\nTechniques\\nTopics\\nIntroduction to Machine Learning AI vs ML vs DL vs DS, Types of ML\\nTechniques, Supervised vs Unsupervised\\nvs Semi-Supervised vs Reinforcement\\nLearning\\nLinear Regression Simple Linear Regression, Multiple Linear\\nRegression, MSE, MAE, RMSE, R-\\nsquared, Adjusted R-squared, Linear\\nRegression with OLS\\nRegularization Techniques Ridge Regression, Lasso Regression,\\nElasticNet\\nLogistic Regression Logistic Regression, Performance\\nMetrics: Confusion Matrix, Accuracy,\\nPrecision, Recall, F-Beta Score, ROC-\\nAUC Curve\\nSupport Vector Machines (SVM) Support Vector Classifiers, Support\\nVector Regressor, Support Vector\\nKernels\\nUltimate Data Science & GenAI Bootcamp       Page  18\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 18, 'page_label': '19'}, page_content='Machine Learning Foundations and\\nTechniques\\nTopics\\nBayes Theorem and Naive Bayes Introduction to Bayes Theorem, Naive\\nBayes Classifier\\nK-Nearest Neighbors (KNN) KNN Classifier, KNN Regressor\\nDecision Trees Decision Tree Classifier, Decision Tree\\nRegressor\\nEnsemble Methods Bagging, Boosting, Random Forest\\nClassifier, Random Forest Regressor,\\nOut-of-Bag Evaluation, XGBoost\\nClassifier, XGBoost Regressor\\nSupport Vector Machines (SVM) Support Vector Classifiers, Support\\nVector Regressor, Support Vector\\nKernels\\nIntroduction to Unsupervised Learning Overview of Unsupervised Learning, Use\\nCases, and Applications\\nClustering Techniques KMeans Clustering, Hierarchical\\nClustering, DBSCAN Clustering\\nUltimate Data Science & GenAI Bootcamp       Page  19\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 19, 'page_label': '20'}, page_content='Machine Learning Foundations and\\nTechniques\\nTopics\\nClustering Evaluation Silhouette Coefficient, Evaluation\\nMetrics for Clustering Algorithms\\nUltimate Data Science & GenAI Bootcamp       Page  20\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 20, 'page_label': '21'}, page_content='In this module, you’ll explore the basics of Natural Language Processing (NLP) for machine\\nlearning applications. Topics include text preprocessing (stemming, lemmatization), tokenization,\\nand POS tagging. You’ll also learn how to implement key NLP techniques like Named Entity\\nRecognition, word embeddings (Word2Vec), and TF-IDF. By the end of this module, you’ll have the\\nskills to work with textual data and apply machine learning models to solve NLP tasks.\\nNatural Language Processing for\\nMachine Learning\\nTopics\\nIntroduction to NLP for ML Roadmap to Learn NLP for ML, Practical\\nUse Cases of NLP in Machine Learning\\nText Preprocessing Tokenization, Basic Terminology,\\nStemming, Lemmatization, Stopwords\\nText Representation One-Hot Encoding, N-Gram, Bag of\\nWords (BoW), TF-IDF Intuition\\nPart of Speech (POS) Tagging POS Tagging using NLTK, Understanding\\nPOS Tags\\nNamed Entity Recognition (NER) Introduction to NER, Implementing NER\\nwith NLTK\\nWord Embeddings Introduction to Word Embeddings,\\nBenefits of Using Word Embeddings in\\nML\\nUltimate Data Science & GenAI Bootcamp       Page  21\\nModule 13'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 21, 'page_label': '22'}, page_content='Natural Language Processing for\\nMachine Learning\\nTopics\\nWord2Vec Intuition behind Word2Vec, Training\\nWord2Vec Models, Skip-gram and\\nCBOW Architectures\\nUltimate Data Science & GenAI Bootcamp       Page  22\\nModule 13'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 22, 'page_label': '23'}, page_content='This module introduces you to deep learning and the fundamental concepts behind artificial\\nneural networks (ANNs). You’ll learn about the architecture and workings of a neural network,\\nincluding activation functions, loss functions, and optimization techniques. The module also covers\\nbackpropagation and the vanishing gradient problem. By the end, you’ll be equipped to build and\\ntrain basic neural networks and understand how deep learning models are used in AI applications.\\nIntroduction to Deep Learning and Neural\\nNetworks\\nTopics\\nIntroduction to Deep Learning Why Deep Learning Is Becoming\\nPopular?\\nPerceptron Intuition Understanding the Perceptron Model,\\nBasic Working Principle\\nArtificial Neural Network (ANN)\\nWorking\\nStructure of ANN, Neurons, Layers, and\\nHow Data Passes Through the Network\\nBackpropagation in ANN The Backpropagation Process, Gradient\\nDescent, and Training Networks\\nVanishing Gradient Problem Explanation, Causes, and Solutions\\nExploding Gradient Problem Causes and Mitigation Techniques\\nUltimate Data Science & GenAI Bootcamp       Page  23\\nModule 14'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 23, 'page_label': '24'}, page_content=\"Introduction to Deep Learning and Neural\\nNetworks\\nTopics\\nActivation Functions Different Types of Activation Functions\\n(Sigmoid, ReLU, Tanh, etc.)\\nLoss Functions Common Loss Functions for Regression\\nand Classification\\nOptimizers Types of Optimizers (SGD, Adam,\\nRMSprop, etc.)\\nWeight Initialization Techniques Methods for Initializing Weights (Xavier,\\nHe Initialization)\\nDropout Layer Concept of Dropout and its Role in\\nRegularization\\nBatch Normalization How Batch Normalization Works and\\nWhy It's Important\\nKeras Framework Fundamentals Introduction to Keras, Building Models\\nwith Keras, Basic Operations\\nPyTorch Framework Fundamentals Introduction to PyTorch, Tensor\\nOperations, Building Models with\\nPyTorch\\nUltimate Data Science & GenAI Bootcamp       Page  24\\nModule 14\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 24, 'page_label': '25'}, page_content='In this module, you’ll dive into Convolutional Neural Networks (CNNs), a cornerstone of deep\\nlearning in computer vision. You’ll learn the architecture of CNNs, including convolution layers,\\npooling layers, and fully connected layers. The module covers practical applications like image\\nclassification, object detection, and segmentation using CNNs. By the end, you’ll have hands-on\\nexperience building and training CNNs for real-world vision tasks.\\nDeep Learning : Convolutional Neural\\nNetworks (CNN) Fundamentals and\\nApplications\\nTopics\\nIntroduction to CNN CNN Fundamentals, What is\\nConvolutional Neural Network, CNN\\nArchitecture Overview\\nExplaining CNN in Detail CNN Explained in Detail, Understanding\\nTensor Space, CNN Explainer\\nCNN-Based Architectures Various CNN Architectures, Deep Dive\\ninto ResNet and its Variants\\nTraining CNN from Scratch Steps to Train CNNs, Hyperparameter\\nTuning, Overfitting, and Underfitting\\nBuilding Web Apps for CNN Deploying CNN Models into Web\\nApplications, Using Flask or Django,\\nServing Models with TensorFlow.js\\nExploding Gradient Problem Causes and Mitigation Techniques\\nUltimate Data Science & GenAI Bootcamp       Page  25\\nModule 15'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 25, 'page_label': '26'}, page_content='Deep Learning : Convolutional Neural\\nNetworks (CNN) Fundamentals and\\nApplications\\nTopics\\nObject Detection Using YOLO Introduction to YOLO (You Only Look\\nOnce), YOLO Architecture, Training and\\nDeployment\\nObject Detection Using Detectron2 Understanding Detectron2 for Object\\nDetection, Using Pre-trained Models and\\nFine-tuning\\nSegmentation Using YOLO Semantic and Instance Segmentation\\nwith YOLO, Implementing YOLO for\\nSegmentation Tasks\\nSegmentation Using Detectron2 Using Detectron2 for Semantic and\\nInstance Segmentation, Implementing\\nPre-trained Models for Image\\nSegmentation\\nUltimate Data Science & GenAI Bootcamp       Page  26\\nModule 15'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 26, 'page_label': '27'}, page_content=\"This module covers Recurrent Neural Networks (RNNs) and Transformer models, focusing on their\\napplications in sequential data processing. You’ll learn how RNNs and LSTMs are used for time\\nseries analysis, speech recognition, and language modeling. The module also explores the\\nTransformer architecture, which powers models like BERT and GPT. By the end, you'll have a\\nstrong grasp of these advanced neural network architectures and their applications in NLP and\\nbeyond.\\nDeep Learning : Recurrent Neural\\nNetworks (RNN) and Transformer\\nModels\\nTopics\\nIntroduction to RNNs Recurrent Neural Networks (RNN)\\nFundamentals, How RNNs Work,\\nApplications of RNN\\nLong Short Term Memory (LSTM) LSTM Cells, How LSTM Solves Vanishing\\nGradient Problem, LSTM for Sequence\\nModeling, Training and Tuning LSTM\\nGated Recurrent Units (GRU) GRU vs LSTM, Understanding GRU\\nArchitecture, Advantages of GRU in\\nSequence Modeling\\nEncoders and Decoders Encoder-Decoder Architecture,\\nApplications in Machine Translation,\\nSequence-to-Sequence Models\\nAttention Mechanism What is Attention, Types of Attention\\nMechanisms, Soft and Hard Attention\\nUltimate Data Science & GenAI Bootcamp       Page  27\\nModule 16\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 27, 'page_label': '28'}, page_content='Deep Learning : Recurrent Neural\\nNetworks (RNN) and Transformer\\nModels\\nTopics\\nAttention Neural Networks Self-Attention in Neural Networks,\\nApplying Attention to RNNs, Transformer\\nvs RNN\\nBERT Model BERT (Bidirectional Encoder\\nRepresentations from Transformers),\\nPre-training and Fine-tuning BERT,\\nApplications of BERT in NLP\\nGPT-2 Model GPT-2 (Generative Pre-trained\\nTransformer 2), Autoregressive\\nLanguage Modeling, Fine-tuning GPT-2\\nfor Text Generation\\nUltimate Data Science & GenAI Bootcamp       Page  28\\nModule 16'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 28, 'page_label': '29'}, page_content='In this module, you’ll explore the world of Generative AI, understanding how these models\\ngenerate new data based on patterns learned from existing data. You’ll compare generative and\\ndiscriminative models and discover their applications in text, image, and audio generation. The\\nmodule also covers advancements in generative models, including GANs and VAEs. By the end,\\nyou’ll be familiar with key concepts and applications of Generative AI.\\nIntroduction to Generative AI\\nTopics\\nOverview of Generative AI What is Generative AI?, Overview of\\nGenerative vs. Discriminative Models,\\nSignificance and Applications of\\nGenerative AI\\nUnderstanding Generative Models How Generative Models Work, Key\\nTypes of Generative Models (e.g., GANs,\\nVAEs), Advantages of Generative Models\\nGenerative AI vs. Discriminative\\nModels\\nKey Differences, Use Cases,\\nPerformance Comparison\\nRecent Advancements and Research Latest Breakthroughs in Generative AI,\\nState-of-the-Art Models and\\nTechniques, Future Trends in Generative\\nAI\\nKey Applications of Generative\\nModels\\nApplications in Art and Creativity (e.g.,\\nImage Synthesis), Healthcare (e.g., Drug\\nDiscovery), Natural Language\\nProcessing, and More\\nUltimate Data Science & GenAI Bootcamp       Page  29\\nModule 17'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 29, 'page_label': '30'}, page_content='This module introduces you to the concept of vector databases, which are designed to store and\\nretrieve high-dimensional data vectors. You’ll learn how vector databases differ from traditional\\nSQL and NoSQL databases, and explore their use cases, including similarity searches and machine\\nlearning applications. The module also covers popular vector databases like Faiss, Pinecone, and\\nChromaDB. By the end, you’ll be equipped to work with vector databases for handling complex\\ndata queries.\\nIntroduction to Vector Databases\\nTopics\\nOverview of Vector Databases What are Vector Databases?, Key\\nConcepts and Use Cases of Vector\\nDatabases, Difference Between Vector\\nDatabases and Traditional Databases\\nComparison with SQL and NoSQL\\nDatabases\\nSQL vs. NoSQL vs. Vector Databases:\\nKey Differences, Use Cases, and\\nPerformance Considerations\\nCapabilities of Vector Databases Handling High-Dimensional Data, Fast\\nSimilarity Search, Efficient Storage and\\nQuerying, Real-Time Processing\\nData Storage and Architecture of\\nVector Databases\\nStructure of Vector Data, Indexing\\nTechniques, Optimizations for Vector\\nSearch, Performance Considerations\\nTypes of Vector Databases In-Memory Vector Databases: Benefits\\nand Limitations, Local Disk-based Vector\\nDatabases, Cloud-Based Vector\\nDatabases and Their Use Cases\\nUltimate Data Science & GenAI Bootcamp       Page  30\\nModule 18'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 30, 'page_label': '31'}, page_content='Introduction to Vector Databases\\nTopics\\nExploring Popular Vector Databases Chroma DB, Faiss, Quadrant, Pinecone,\\nLanceDB: Overview, Features, and Use\\nCases\\nVector Search with NoSQL Databases Integrating Vector Search with\\nMongoDB and Cassandra, Best\\nPractices for Implementing Vector\\nSearch in NoSQL Databases\\nUltimate Data Science & GenAI Bootcamp       Page  31\\nModule 18'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 31, 'page_label': '32'}, page_content='This module introduces the concept of Retrieval-Augmented Generation (RAG), which combines\\nretrieval-based search with generative models for enhanced language generation tasks. You’ll\\nlearn about the end-to-end RAG pipeline, including how to implement it with tools like LangChain,\\nvector databases, and LLMs. The module also covers hybrid search, reranking, and multimodal\\nretrieval techniques. By the end, you’ll understand how to implement advanced RAG systems for\\nvarious use cases.\\nIntroduction to Retrieval-Augmented\\nGeneration (RAG)\\nTopics\\nOverview of Retrieval-Augmented\\nGeneration (RAG)\\nWhat is RAG?, Key Components of a\\nRAG System, Why RAG is Important for\\nAdvanced AI Systems\\nUnderstanding the End-to-End RAG\\nPipeline\\nOverview of the RAG Workflow, Data\\nRetrieval, Contextualization, and\\nGeneration Phases, Challenges and\\nOpportunities in RAG\\nIntegrating LangChain in RAG Introduction to LangChain Framework,\\nBuilding End-to-End RAG Pipelines with\\nLangChain\\nLeveraging Vector Databases in RAG Using Vector Databases for Efficient\\nRetrieval in RAG, Popular Vector\\nDatabases for RAG (e.g., Pinecone,\\nFAISS, Chroma DB)\\nRole of LLMs in RAG How LLMs (Large Language Models)\\nEnhance Generation in RAG, Fine-\\nTuning LLMs for Retrieval-Augmented\\nTasks\\nUltimate Data Science & GenAI Bootcamp       Page  32\\nModule 19'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 32, 'page_label': '33'}, page_content='Introduction to Retrieval-Augmented\\nGeneration (RAG)\\nTopics\\nRAG with Hybrid Search and\\nReranking\\nCombining Multiple Retrieval Methods,\\nReranking Results for Improved\\nRelevance, Hybrid Search\\nImplementation Techniques\\nRAG with Various Retrieval Methods Exact vs Approximate Retrieval Methods,\\nFiltering and Ranking Retrieved Data,\\nCustomizing Retrieval Approaches for\\nSpecific Applications\\nIntegrating Memory in RAG Systems How Memory Can Improve RAG,\\nPersisting and Recalling Information for\\nConsistent Results, Implementing Long-\\nTerm Memory in RAG\\nMultimodal Retrieval-Augmented\\nGeneration\\nCombining Text, Images, and Other\\nModalities in RAG, Techniques for\\nMultimodal Retrieval and Generation,\\nPractical Applications of Multimodal\\nRAG Systems\\nUltimate Data Science & GenAI Bootcamp       Page  33\\nModule 19'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'moddate': '2025-01-30T20:26:59+00:00', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'author': 'monal singh', 'containsaigeneratedcontent': 'Yes', 'source': './syllabus.pdf', 'total_pages': 34, 'page': 33, 'page_label': '34'}, page_content='In this course, you’ll gain hands-on experience in implementing end-to-end AI projects. You’ll learn\\nhow to manage the entire project lifecycle, from data collection and preprocessing to model\\ndevelopment, evaluation, and deployment. The module includes working on real-world AI projects,\\nwith a focus on best practices for integration, testing, and scalability. By the end, you’ll be\\nprepared to take on AI projects from start to finish, applying machine learning and deep learning\\ntechniques to solve real-world problems.\\nEnd-to-End AI Project Implementation\\nTopics\\nPython Project: Building End-to-End\\nApplications\\nOverview of Python Projects, Project\\nDesign and Architecture, Key\\nConsiderations in Python Projects\\n(Performance, Scalability, etc.), Best\\nPractices for Code Quality\\nEnd-to-End Machine Learning\\nProjects\\nUnderstanding End-to-End ML Projects,\\nKey Components of an End-to-End ML\\nProject, Project Example: Real-World ML\\nApplication\\nDeep Learning Projects Deep Learning Fundamentals in Projects,\\nEnd-to-End Deep Learning Projects \\nGenerative AI End-to-End Projects Introduction to Generative AI Projects,\\nSteps in Building Generative AI Projects\\nUltimate Data Science & GenAI Bootcamp       Page  34\\nPROJECT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read a pdf file \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader('../syllabus.pdf')\n",
    "pdf_docs = pdf_loader.load()\n",
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fbf734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.1-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.1-cp39-abi3-win_amd64.whl (18.5 MB)\n",
      "   ---------------------------------------- 0.0/18.5 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 4.7/18.5 MB 25.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 8.9/18.5 MB 24.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 13.1/18.5 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.5 MB 22.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.5/18.5 MB 21.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f7257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 0}, page_content='MACHINE\\nLEARNING\\nDEEP\\nLEARNING\\nPYTHON +\\nSTATS\\nCOMPUTER VISION\\nNATURAL LANGUAGE PROCESSING\\nGENERATIVE AI\\nRETRIEVAL AUGUMENT GENERATION\\nVECTOR DB'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 1}, page_content='This course is designed for aspiring data scientists, machine learning enthusiasts, and\\nprofessionals looking to build expertise in Python programming, data analysis, machine learning,\\nand deep learning. Whether you are just starting or have some experience, this comprehensive\\ncourse will equip you with the skills needed to work with real-world datasets, apply machine\\nlearning algorithms, and deploy AI solutions. By the end of the course, you’ll have a solid\\nfoundation in AI, a portfolio of end-to-end projects, and the confidence to tackle complex\\nchallenges in data science and AI.\\nLearning Objectives\\nMaster Python Programming: Understand Python fundamentals, including data types,\\ncontrol structures, and object-oriented programming, to write efficient and reusable\\ncode.\\nHandle Data with Pandas and NumPy: Acquire skills to manipulate, clean, and\\npreprocess large datasets using Pandas and NumPy for data analysis tasks.\\nVisualize Data: Create compelling data visualizations using libraries such as Matplotlib,\\nSeaborn, and Plotly to present insights effectively.\\nUnderstand SQL & NoSQL: Gain expertise in both relational (SQL) and non-relational\\n(NoSQL) databases, including MongoDB, for storing, querying, and managing data.\\nGrasp Statistics and Probability: Understand the core concepts of statistics,\\nprobability, and hypothesis testing, applying them to data analysis and machine\\nlearning.\\nMaster Machine Learning Techniques: Learn key machine learning algorithms,\\nincluding supervised, unsupervised, and ensemble methods, and apply them to real-\\nworld problems.\\nDive into Deep Learning: Develop a strong understanding of neural networks, CNNs,\\nRNNs, and transformers, with hands-on implementation for advanced AI tasks.\\nExplore Generative AI & Vector Databases: Learn the concepts and applications of\\ngenerative models, vector databases, and retrieval-augmented generation to handle\\ncomplex AI systems.\\nBuild Real-World Projects: Implement end-to-end machine learning and AI projects,\\nfrom data preprocessing to model deployment, integrating concepts from multiple\\nmodules.\\nUltimate Data Science & GenAI Bootcamp       Page  2'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 2}, page_content=\"Course Information\\nNo prerequisites are required for this course. The curriculum covers everything from the\\nbasics of Python programming, statistics, and machine learning to advanced topics in deep\\nlearning, NLP, and generative AI. Whether you're a beginner or have some prior experience,\\nthe course will ensure you gain the skills needed to succeed.\\nEstimated Time\\n8 months 6hrs/week*\\nRequired Skill Level\\nBegineer\\nThe course is designed to be completed over a duration of approximately 7 to 8 months, providing\\nan in-depth exploration from Python basics to GenAI, with plenty of time for practical\\nimplementation and real-world applications.\\nPrerequisites\\nUltimate Data Science & GenAI Bootcamp       Page  3\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 3}, page_content='Course Instructors\\nSunny Savita\\nGenAI Engineer\\nLinkedin\\nKrish Naik\\nChief AI Engineer\\nLinkedin\\nUltimate Data Science & GenAI Bootcamp       Page  4\\nSourangshu Pal\\nSenior Data Scientist\\nLinkedin\\nMonal Kumar\\nData Scientist\\nLinkedin\\nMayank Aggrawal\\nSenior ML Engineer\\nLinkedin\\nDarius B.\\nHead of Product\\nLinkedin'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 4}, page_content=\"In this module, you’ll get a solid introduction to Python, covering essential programming concepts\\nsuch as variables, data types, operators, and control flow. You’ll learn how to manipulate strings,\\nlists, dictionaries, and other basic data structures. The module will also guide you through writing\\nsimple functions and using loops and conditionals effectively. By the end, you'll have a strong\\nunderstanding of Python syntax, preparing you to tackle more complex programming challenges\\nand form a foundation for learning advanced concepts.\\nUltimate Data Science & GenAI Bootcamp       Page  5\\nPython Foundations\\nModule 1\\nTopics\\nIntroduction to Python\\nComparison with other programming\\nlanguages, Python objects: Numbers,\\nBooleans, and Strings\\nData Structures & Operations\\nContainer objects and mutability,\\nOperators, Operator precedence and\\nassociativity\\nControl Flow\\nConditional statements, Loops, break\\nand continue statements\\nString Manipulation\\nBasics of string objects, Inbuilt string\\nmethods, Splitting and joining strings,\\nString formatting functions\\nLists & Collections\\nList methods, list comprehension, Lists as\\nstacks and queues, Tuples, sets, and\\ndictionaries, Dictionary comprehensions\\nand view objects\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 5}, page_content='Topics\\nFunctions & Iterators\\nFunction basics and parameter passing,\\nIterators and generator functions,\\nLambda functions, map(), reduce(),\\nfilter()\\nPython Foundations\\nModule 1\\nUltimate Data Science & GenAI Bootcamp       Page  6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 6}, page_content='This module takes your Python skills further by diving into object-oriented programming (OOP)\\nconcepts like classes, inheritance, and polymorphism. You’ll also explore more advanced topics\\nsuch as decorators, lambda functions, iterators, and generator functions. Additionally, we cover\\nexception handling, file operations, and working with modules and libraries. By the end, you will be\\ncomfortable building more sophisticated Python applications and writing efficient, reusable code.\\nAdvanced Python Programming\\nModule 2\\nTopics\\nObject-Oriented Programming (OOP)\\nOOP basics and class creation,\\nInheritance, Polymorphism,\\nEncapsulation, and Abstraction,\\nDecorators, class methods, and static\\nmethods, Special (Magic/Dunder)\\nmethods, Property decorators: Getters,\\nsetters, and delete methods\\nFile Handling & Logging\\nReading and writing files, Buffered read\\nand write operations, more file methods,\\nLogging and debugging\\nModules & Exception Handling\\nImporting modules and using them\\neffectively, Exception handling\\nConcurrency & Parallelism\\nIntroduction to multithreading,\\nMultiprocessing for performance\\noptimization\\nUltimate Data Science & GenAI Bootcamp       Page  7'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 7}, page_content='In this module, you will master the core aspects of data manipulation using Pandas. You’ll learn\\nhow to work with Series, DataFrames, and Panels, as well as perform data selection, filtering, and\\nsorting. The module covers critical tasks like handling missing data, reindexing, and applying\\nstatistical functions to datasets. You’ll also gain hands-on experience with data visualization and\\nadvanced indexing techniques, empowering you to efficiently analyze and manipulate complex\\ndatasets.\\nMastering Data Handling with Pandas\\nTopics\\nData Structures & Fundamentals\\nSeries, DataFrame, Panel, Basic\\nFunctionality, Indexing & Selecting, Re-\\nindexing, Iteration\\nData Operations & Transformations\\nSorting, Working with Text Data, Options\\n& Customization, Categorical Data, Date\\nFunctionality, Time Delta\\nData Analysis & Statistical Functions\\nData Statistical Functions, Window\\nFunctions\\nReading, Writing & Visualization\\nReading Data from Different File\\nSystems, Visualization, Tools\\nUltimate Data Science & GenAI Bootcamp       Page  8\\nModule 3'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 8}, page_content='This module introduces you to NumPy, a key library for numerical computing in Python. You’ll learn\\nhow to create and manipulate NumPy arrays, perform advanced indexing, and understand\\nbroadcasting. The module covers essential mathematical and statistical functions, including array\\nmanipulations, binary operations, and vectorized operations. By the end, you’ll have the skills to\\nefficiently perform complex numerical computations and leverage NumPy for machine learning\\nand deep learning applications.\\nMastering NumPy\\nTopics\\nNumPy Basics & Array Creation\\nNdArray Object, Data Types, Array\\nAttributes, Array Creation Routines,\\nArray from Existing Data, Data Array from\\nNumerical Ranges\\nIndexing, Slicing & Advanced Indexing\\nIndexing & Slicing, Advanced Indexing\\nArray Operations & Manipulation\\nArray Manipulation, Binary Operators,\\nString Functions, Arithmetic Operations,\\nMathematical Functions\\nMathematical & Statistical Analysis\\nStatistical Functions, Sort, Search &\\nCounting Functions, Matrix Library,\\nLinear Algebra\\nAdvanced Concepts\\nBroadcasting, Iterating Over Array, Byte\\nSwapping, Copies & Views\\nUltimate Data Science & GenAI Bootcamp       Page  9\\nModule 4'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 9}, page_content=\"In this module, you’ll learn how to visualize data effectively using Python's popular libraries,\\nMatplotlib, Seaborn, and Plotly. You’ll cover essential plot types like line charts, bar graphs, and\\nscatter plots, and learn how to customize these visualizations to highlight key insights. Additionally,\\nthe module teaches you how to visualize statistical data, correlations, and distributions, helping\\nyou communicate data-driven findings in a visually compelling way.\\nData Visualization with Python\\nTopics\\nIntroduction to Data Visualization\\nOverview of Data Visualization, Principles\\nof Good Visualization\\nMatplotlib\\nIntroduction to Matplotlib, Creating\\nBasic Plots (Line, Bar, Scatter),\\nCustomizing Axes, Titles, Legends, and\\nLabels, Working with Subplots, Saving\\nand Exporting Figures\\nSeaborn\\nIntroduction to Seaborn, Visualizing\\nDistributions, Relationship Plots\\n(Pairplots, Heatmaps), Categorical Data\\nVisualization, Advanced Plot\\nCustomizations\\nPlotly\\nIntroduction to Plotly, Creating\\nInteractive Plots (Line, Bar, Scatter),\\nCustomizing Plots, Dashboards and\\nInteractive Layouts, Plotly Express\\nUltimate Data Science & GenAI Bootcamp       Page  10\\nModule 5\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 10}, page_content='This module dives into advanced SQL techniques, including complex queries, joins, and indexing\\nfor efficient data retrieval. You’ll learn how to implement stored procedures, triggers, and\\nfunctions, and explore the use of window functions and partitions. The module covers key\\ndatabase design concepts like primary and foreign keys and normalization. By the end, you’ll be\\nproficient in managing large-scale databases and optimizing SQL queries for performance.\\nAdvanced SQL and Database Management\\nTopics\\nIntroduction to SQL\\nIntroduction to SQL, SQL Queries:\\nSELECT, INSERT, UPDATE, DELETE\\nSQL Functions and Procedures\\nSQL Functions (Aggregate, Scalar),\\nStored Procedures, User-defined\\nFunctions (UDFs), Function and\\nProcedure Syntax\\nDatabase Constraints\\nPrimary and Foreign Keys, Data Integrity,\\nReferential Integrity\\nAdvanced SQL Techniques\\nWindow Functions, Partitioning, CTE\\n(Common Table Expressions), Indexing\\nSQL Joins and Unions\\nInner Join, Left Join, Right Join, Full Outer\\nJoin, Cross Join, Union\\nTriggers and Case Statements\\nTriggers (Before, After), CASE\\nStatements, Conditional Logic\\nUltimate Data Science & GenAI Bootcamp       Page  11\\nModule 6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 11}, page_content='Advanced SQL and Database Management\\nTopics\\nNormalization and Pivoting\\nNormalization Forms (1NF, 2NF, 3NF),\\nPivot Tables, Data Aggregation\\nUltimate Data Science & GenAI Bootcamp       Page  12\\nModule 6'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 12}, page_content=\"In this module, you'll explore the world of NoSQL databases with MongoDB. You'll learn how to\\ncreate and manage databases, collections, and documents, and perform CRUD operations. The\\nmodule covers querying, sorting, and indexing, providing a comprehensive understanding of\\nMongoDB's flexible data model. By the end, you’ll be able to efficiently work with NoSQL\\ndatabases, particularly for use cases that involve unstructured or semi-structured data.\\nIntroduction to NoSQL with MongoDB\\nTopics\\nGetting Started with MongoDB\\nMongoDB Introduction, Setting up\\nMongoDB, MongoDB Shell Commands\\nDatabase and Collection Management\\nMongoDB Create Database, MongoDB\\nCreate Collection\\nCRUD Operations\\nMongoDB Insert, MongoDB Find,\\nMongoDB Update, MongoDB Delete\\nQuerying MongoDB\\nMongoDB Query, MongoDB Sort,\\nMongoDB Limit\\nManaging Collections\\nMongoDB Drop Collection, MongoDB\\nDelete (Specific)\\nUltimate Data Science & GenAI Bootcamp       Page  13\\nModule 7\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 13}, page_content='This module provides a foundation in statistics and probability, covering essential terms, concepts,\\nand methods. You’ll learn about different types of data, levels of measurement, and key statistical\\nmeasures like mean, median, variance, and standard deviation. The module introduces random\\nvariables, probability distributions, and various types of probability functions, giving you a strong\\nbase to analyze and interpret data from a statistical perspective.\\nFoundations of Statistics and Probability\\nTopics\\nIntroduction to Statistics\\nIntroduction to Basic Statistics Terms,\\nTypes of Statistics, Types of Data, Levels\\nof Measurement, Measures of Central\\nTendency, Measures of Dispersion\\nExploring Random Variables and\\nProbability\\nRandom Variables, Set Theory,\\nSkewness, Covariance and Correlation,\\nProbability Density/Distribution Function\\nDistributions and Their Applications\\nTypes of Probability Distributions,\\nBinomial Distribution, Poisson\\nDistribution, Normal Distribution\\n(Gaussian Distribution), Probability\\nDensity Function and Mass Function,\\nCumulative Density Function, Examples\\nof Normal Distribution, Bernoulli\\nDistribution, Uniform Distribution\\nStatistical Inference\\nZ-Statistics, Central Limit Theorem,\\nEstimation, Hypothesis Testing\\nUltimate Data Science & GenAI Bootcamp       Page  14\\nModule 8'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 14}, page_content=\"In this module, you'll delve deeper into statistical inference techniques, including hypothesis\\ntesting, confidence intervals, and the types of errors in statistical tests. You’ll explore advanced\\nconcepts like P-values, T-tests, and Chi-square tests, learning how to interpret results in the\\ncontext of real-world data. By the end, you’ll be equipped to conduct sophisticated statistical\\nanalysis and make informed decisions based on data-driven evidence.\\nAdvanced Statistical Inference and\\nHypothesis Testing\\nTopics\\nHypothesis Testing and Errors\\nHypothesis Testing Mechanism, Type 1 &\\nType 2 Error, T-Tests vs. Z-Tests:\\nOverview, When to Use a T-Test vs. Z-\\nTest\\nStatistical Distributions and Tests\\nT-Stats, Student T Distribution, Chi-\\nSquare Test, Chi-Square Distribution\\nUsing Python, Chi-Square for Goodness\\nof Fit Test\\nBayesian Statistics and Confidence\\nIntervals\\nBayes Statistics (Bayes Theorem),\\nConfidence Interval (CI), Confidence\\nIntervals and the Margin of Error,\\nInterpreting Confidence Levels and\\nConfidence Intervals\\nStatistical Significance and\\nInterpretation\\nP-Value, T-Stats vs. Z-Stats: Overview\\nUltimate Data Science & GenAI Bootcamp       Page  15\\nModule 9\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 15}, page_content='This module covers essential techniques for preparing and transforming data before applying\\nmachine learning models. You’ll learn how to handle missing values, deal with imbalanced data,\\nand scale or encode features. The module also explores methods for handling outliers, feature\\nselection (including forward/backward elimination), and dimensionality reduction techniques. By\\nthe end, you’ll be proficient in preparing high-quality datasets that are ready for modeling.\\nFeature Engineering and Data\\nPreprocessing\\nTopics\\nHandling Missing and Imbalanced\\nData\\nHandling Missing Data, Handling\\nImbalanced Data\\nOutliers and Scaling\\nHandling Outliers, Feature Scaling\\nData Transformation and Encoding\\nData Encoding\\nFeature Selection Techniques\\nBackward Elimination, Forward\\nElimination, Recursive Feature\\nElimination\\nCorrelation and Multicollinearity\\nCovariance and Correlation, VIF\\nUltimate Data Science & GenAI Bootcamp       Page  16\\nModule 10'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 16}, page_content='In this module, you’ll learn how to perform Exploratory Data Analysis (EDA) to uncover patterns,\\ntrends, and relationships in your data. You’ll master techniques for visualizing distributions,\\nidentifying correlations, and detecting anomalies. The module emphasizes the importance of\\nsummary statistics, data cleaning, and feature engineering. By the end, you’ll be able to extract\\nmeaningful insights from raw data and prepare it for further analysis or modeling.\\nExploratory Data Analysis (EDA) for\\nDetailed Insights\\nTopics\\nTrend Analysis and Segmentation\\nAnalyzing Bike Sharing Trends,\\nCustomer Segmentation and Effective\\nCross-Selling\\nSentiment and Quality Analysis\\nAnalyzing Movie Reviews Sentiment,\\nAnalyzing Wine Types and Quality\\nRecommendation and Forecasting\\nAnalyzing Music Trends and\\nRecommendations, Forecasting Stock\\nand Commodity Prices\\nUltimate Data Science & GenAI Bootcamp       Page  17\\nModule 11'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 17}, page_content='This module provides a comprehensive introduction to machine learning, covering key algorithms\\nand techniques. You’ll learn the differences between supervised and unsupervised learning, as\\nwell as the core concepts of regression, classification, and clustering. The module introduces\\nmodel evaluation metrics like accuracy, precision, recall, and F1-score, giving you the foundation to\\nunderstand and implement machine learning models in real-world scenarios.\\nMachine Learning Foundations and\\nTechniques\\nTopics\\nIntroduction to Machine Learning\\nAI vs ML vs DL vs DS, Types of ML\\nTechniques, Supervised vs Unsupervised\\nvs Semi-Supervised vs Reinforcement\\nLearning\\nLinear Regression\\nSimple Linear Regression, Multiple Linear\\nRegression, MSE, MAE, RMSE, R-\\nsquared, Adjusted R-squared, Linear\\nRegression with OLS\\nRegularization Techniques\\nRidge Regression, Lasso Regression,\\nElasticNet\\nLogistic Regression\\nLogistic Regression, Performance\\nMetrics: Confusion Matrix, Accuracy,\\nPrecision, Recall, F-Beta Score, ROC-\\nAUC Curve\\nSupport Vector Machines (SVM)\\nSupport Vector Classifiers, Support\\nVector Regressor, Support Vector\\nKernels\\nUltimate Data Science & GenAI Bootcamp       Page  18\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 18}, page_content='Machine Learning Foundations and\\nTechniques\\nTopics\\nBayes Theorem and Naive Bayes\\nIntroduction to Bayes Theorem, Naive\\nBayes Classifier\\nK-Nearest Neighbors (KNN)\\nKNN Classifier, KNN Regressor\\nDecision Trees\\nDecision Tree Classifier, Decision Tree\\nRegressor\\nEnsemble Methods\\nBagging, Boosting, Random Forest\\nClassifier, Random Forest Regressor,\\nOut-of-Bag Evaluation, XGBoost\\nClassifier, XGBoost Regressor\\nSupport Vector Machines (SVM)\\nSupport Vector Classifiers, Support\\nVector Regressor, Support Vector\\nKernels\\nIntroduction to Unsupervised Learning\\nOverview of Unsupervised Learning, Use\\nCases, and Applications\\nClustering Techniques\\nKMeans Clustering, Hierarchical\\nClustering, DBSCAN Clustering\\nUltimate Data Science & GenAI Bootcamp       Page  19\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 19}, page_content='Machine Learning Foundations and\\nTechniques\\nTopics\\nClustering Evaluation\\nSilhouette Coefficient, Evaluation\\nMetrics for Clustering Algorithms\\nUltimate Data Science & GenAI Bootcamp       Page  20\\nModule 12'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 20}, page_content='In this module, you’ll explore the basics of Natural Language Processing (NLP) for machine\\nlearning applications. Topics include text preprocessing (stemming, lemmatization), tokenization,\\nand POS tagging. You’ll also learn how to implement key NLP techniques like Named Entity\\nRecognition, word embeddings (Word2Vec), and TF-IDF. By the end of this module, you’ll have the\\nskills to work with textual data and apply machine learning models to solve NLP tasks.\\nNatural Language Processing for\\nMachine Learning\\nTopics\\nIntroduction to NLP for ML\\nRoadmap to Learn NLP for ML, Practical\\nUse Cases of NLP in Machine Learning\\nText Preprocessing\\nTokenization, Basic Terminology,\\nStemming, Lemmatization, Stopwords\\nText Representation\\nOne-Hot Encoding, N-Gram, Bag of\\nWords (BoW), TF-IDF Intuition\\nPart of Speech (POS) Tagging\\nPOS Tagging using NLTK, Understanding\\nPOS Tags\\nNamed Entity Recognition (NER)\\nIntroduction to NER, Implementing NER\\nwith NLTK\\nWord Embeddings\\nIntroduction to Word Embeddings,\\nBenefits of Using Word Embeddings in\\nML\\nUltimate Data Science & GenAI Bootcamp       Page  21\\nModule 13'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 21}, page_content='Natural Language Processing for\\nMachine Learning\\nTopics\\nWord2Vec\\nIntuition behind Word2Vec, Training\\nWord2Vec Models, Skip-gram and\\nCBOW Architectures\\nUltimate Data Science & GenAI Bootcamp       Page  22\\nModule 13'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 22}, page_content='This module introduces you to deep learning and the fundamental concepts behind artificial\\nneural networks (ANNs). You’ll learn about the architecture and workings of a neural network,\\nincluding activation functions, loss functions, and optimization techniques. The module also covers\\nbackpropagation and the vanishing gradient problem. By the end, you’ll be equipped to build and\\ntrain basic neural networks and understand how deep learning models are used in AI applications.\\nIntroduction to Deep Learning and Neural\\nNetworks\\nTopics\\nIntroduction to Deep Learning\\nWhy Deep Learning Is Becoming\\nPopular?\\nPerceptron Intuition\\nUnderstanding the Perceptron Model,\\nBasic Working Principle\\nArtificial Neural Network (ANN)\\nWorking\\nStructure of ANN, Neurons, Layers, and\\nHow Data Passes Through the Network\\nBackpropagation in ANN\\nThe Backpropagation Process, Gradient\\nDescent, and Training Networks\\nVanishing Gradient Problem\\nExplanation, Causes, and Solutions\\nExploding Gradient Problem\\nCauses and Mitigation Techniques\\nUltimate Data Science & GenAI Bootcamp       Page  23\\nModule 14'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 23}, page_content=\"Introduction to Deep Learning and Neural\\nNetworks\\nTopics\\nActivation Functions\\nDifferent Types of Activation Functions\\n(Sigmoid, ReLU, Tanh, etc.)\\nLoss Functions\\nCommon Loss Functions for Regression\\nand Classification\\nOptimizers\\nTypes of Optimizers (SGD, Adam,\\nRMSprop, etc.)\\nWeight Initialization Techniques\\nMethods for Initializing Weights (Xavier,\\nHe Initialization)\\nDropout Layer\\nConcept of Dropout and its Role in\\nRegularization\\nBatch Normalization\\nHow Batch Normalization Works and\\nWhy It's Important\\nKeras Framework Fundamentals\\nIntroduction to Keras, Building Models\\nwith Keras, Basic Operations\\nPyTorch Framework Fundamentals\\nIntroduction to PyTorch, Tensor\\nOperations, Building Models with\\nPyTorch\\nUltimate Data Science & GenAI Bootcamp       Page  24\\nModule 14\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 24}, page_content='In this module, you’ll dive into Convolutional Neural Networks (CNNs), a cornerstone of deep\\nlearning in computer vision. You’ll learn the architecture of CNNs, including convolution layers,\\npooling layers, and fully connected layers. The module covers practical applications like image\\nclassification, object detection, and segmentation using CNNs. By the end, you’ll have hands-on\\nexperience building and training CNNs for real-world vision tasks.\\nDeep Learning : Convolutional Neural\\nNetworks (CNN) Fundamentals and\\nApplications\\nTopics\\nIntroduction to CNN\\nCNN Fundamentals, What is\\nConvolutional Neural Network, CNN\\nArchitecture Overview\\nExplaining CNN in Detail\\nCNN Explained in Detail, Understanding\\nTensor Space, CNN Explainer\\nCNN-Based Architectures\\nVarious CNN Architectures, Deep Dive\\ninto ResNet and its Variants\\nTraining CNN from Scratch\\nSteps to Train CNNs, Hyperparameter\\nTuning, Overfitting, and Underfitting\\nBuilding Web Apps for CNN\\nDeploying CNN Models into Web\\nApplications, Using Flask or Django,\\nServing Models with TensorFlow.js\\nExploding Gradient Problem\\nCauses and Mitigation Techniques\\nUltimate Data Science & GenAI Bootcamp       Page  25\\nModule 15'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 25}, page_content='Deep Learning : Convolutional Neural\\nNetworks (CNN) Fundamentals and\\nApplications\\nTopics\\nObject Detection Using YOLO\\nIntroduction to YOLO (You Only Look\\nOnce), YOLO Architecture, Training and\\nDeployment\\nObject Detection Using Detectron2\\nUnderstanding Detectron2 for Object\\nDetection, Using Pre-trained Models and\\nFine-tuning\\nSegmentation Using YOLO\\nSemantic and Instance Segmentation\\nwith YOLO, Implementing YOLO for\\nSegmentation Tasks\\nSegmentation Using Detectron2\\nUsing Detectron2 for Semantic and\\nInstance Segmentation, Implementing\\nPre-trained Models for Image\\nSegmentation\\nUltimate Data Science & GenAI Bootcamp       Page  26\\nModule 15'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 26}, page_content=\"This module covers Recurrent Neural Networks (RNNs) and Transformer models, focusing on their\\napplications in sequential data processing. You’ll learn how RNNs and LSTMs are used for time\\nseries analysis, speech recognition, and language modeling. The module also explores the\\nTransformer architecture, which powers models like BERT and GPT. By the end, you'll have a\\nstrong grasp of these advanced neural network architectures and their applications in NLP and\\nbeyond.\\nDeep Learning : Recurrent Neural\\nNetworks (RNN) and Transformer\\nModels\\nTopics\\nIntroduction to RNNs\\nRecurrent Neural Networks (RNN)\\nFundamentals, How RNNs Work,\\nApplications of RNN\\nLong Short Term Memory (LSTM)\\nLSTM Cells, How LSTM Solves Vanishing\\nGradient Problem, LSTM for Sequence\\nModeling, Training and Tuning LSTM\\nGated Recurrent Units (GRU)\\nGRU vs LSTM, Understanding GRU\\nArchitecture, Advantages of GRU in\\nSequence Modeling\\nEncoders and Decoders\\nEncoder-Decoder Architecture,\\nApplications in Machine Translation,\\nSequence-to-Sequence Models\\nAttention Mechanism\\nWhat is Attention, Types of Attention\\nMechanisms, Soft and Hard Attention\\nUltimate Data Science & GenAI Bootcamp       Page  27\\nModule 16\"),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 27}, page_content='Deep Learning : Recurrent Neural\\nNetworks (RNN) and Transformer\\nModels\\nTopics\\nAttention Neural Networks\\nSelf-Attention in Neural Networks,\\nApplying Attention to RNNs, Transformer\\nvs RNN\\nBERT Model\\nBERT (Bidirectional Encoder\\nRepresentations from Transformers),\\nPre-training and Fine-tuning BERT,\\nApplications of BERT in NLP\\nGPT-2 Model\\nGPT-2 (Generative Pre-trained\\nTransformer 2), Autoregressive\\nLanguage Modeling, Fine-tuning GPT-2\\nfor Text Generation\\nUltimate Data Science & GenAI Bootcamp       Page  28\\nModule 16'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 28}, page_content='In this module, you’ll explore the world of Generative AI, understanding how these models\\ngenerate new data based on patterns learned from existing data. You’ll compare generative and\\ndiscriminative models and discover their applications in text, image, and audio generation. The\\nmodule also covers advancements in generative models, including GANs and VAEs. By the end,\\nyou’ll be familiar with key concepts and applications of Generative AI.\\nIntroduction to Generative AI\\nTopics\\nOverview of Generative AI\\nWhat is Generative AI?, Overview of\\nGenerative vs. Discriminative Models,\\nSignificance and Applications of\\nGenerative AI\\nUnderstanding Generative Models\\nHow Generative Models Work, Key\\nTypes of Generative Models (e.g., GANs,\\nVAEs), Advantages of Generative Models\\nGenerative AI vs. Discriminative\\nModels\\nKey Differences, Use Cases,\\nPerformance Comparison\\nRecent Advancements and Research\\nLatest Breakthroughs in Generative AI,\\nState-of-the-Art Models and\\nTechniques, Future Trends in Generative\\nAI\\nKey Applications of Generative\\nModels\\nApplications in Art and Creativity (e.g.,\\nImage Synthesis), Healthcare (e.g., Drug\\nDiscovery), Natural Language\\nProcessing, and More\\nUltimate Data Science & GenAI Bootcamp       Page  29\\nModule 17'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 29}, page_content='This module introduces you to the concept of vector databases, which are designed to store and\\nretrieve high-dimensional data vectors. You’ll learn how vector databases differ from traditional\\nSQL and NoSQL databases, and explore their use cases, including similarity searches and machine\\nlearning applications. The module also covers popular vector databases like Faiss, Pinecone, and\\nChromaDB. By the end, you’ll be equipped to work with vector databases for handling complex\\ndata queries.\\nIntroduction to Vector Databases\\nTopics\\nOverview of Vector Databases\\nWhat are Vector Databases?, Key\\nConcepts and Use Cases of Vector\\nDatabases, Difference Between Vector\\nDatabases and Traditional Databases\\nComparison with SQL and NoSQL\\nDatabases\\nSQL vs. NoSQL vs. Vector Databases:\\nKey Differences, Use Cases, and\\nPerformance Considerations\\nCapabilities of Vector Databases\\nHandling High-Dimensional Data, Fast\\nSimilarity Search, Efficient Storage and\\nQuerying, Real-Time Processing\\nData Storage and Architecture of\\nVector Databases\\nStructure of Vector Data, Indexing\\nTechniques, Optimizations for Vector\\nSearch, Performance Considerations\\nTypes of Vector Databases\\nIn-Memory Vector Databases: Benefits\\nand Limitations, Local Disk-based Vector\\nDatabases, Cloud-Based Vector\\nDatabases and Their Use Cases\\nUltimate Data Science & GenAI Bootcamp       Page  30\\nModule 18'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 30}, page_content='Introduction to Vector Databases\\nTopics\\nExploring Popular Vector Databases\\nChroma DB, Faiss, Quadrant, Pinecone,\\nLanceDB: Overview, Features, and Use\\nCases\\nVector Search with NoSQL Databases\\nIntegrating Vector Search with\\nMongoDB and Cassandra, Best\\nPractices for Implementing Vector\\nSearch in NoSQL Databases\\nUltimate Data Science & GenAI Bootcamp       Page  31\\nModule 18'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 31}, page_content='This module introduces the concept of Retrieval-Augmented Generation (RAG), which combines\\nretrieval-based search with generative models for enhanced language generation tasks. You’ll\\nlearn about the end-to-end RAG pipeline, including how to implement it with tools like LangChain,\\nvector databases, and LLMs. The module also covers hybrid search, reranking, and multimodal\\nretrieval techniques. By the end, you’ll understand how to implement advanced RAG systems for\\nvarious use cases.\\nIntroduction to Retrieval-Augmented\\nGeneration (RAG)\\nTopics\\nOverview of Retrieval-Augmented\\nGeneration (RAG)\\nWhat is RAG?, Key Components of a\\nRAG System, Why RAG is Important for\\nAdvanced AI Systems\\nUnderstanding the End-to-End RAG\\nPipeline\\nOverview of the RAG Workflow, Data\\nRetrieval, Contextualization, and\\nGeneration Phases, Challenges and\\nOpportunities in RAG\\nIntegrating LangChain in RAG\\nIntroduction to LangChain Framework,\\nBuilding End-to-End RAG Pipelines with\\nLangChain\\nLeveraging Vector Databases in RAG\\nUsing Vector Databases for Efficient\\nRetrieval in RAG, Popular Vector\\nDatabases for RAG (e.g., Pinecone,\\nFAISS, Chroma DB)\\nRole of LLMs in RAG\\nHow LLMs (Large Language Models)\\nEnhance Generation in RAG, Fine-\\nTuning LLMs for Retrieval-Augmented\\nTasks\\nUltimate Data Science & GenAI Bootcamp       Page  32\\nModule 19'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 32}, page_content='Introduction to Retrieval-Augmented\\nGeneration (RAG)\\nTopics\\nRAG with Hybrid Search and\\nReranking\\nCombining Multiple Retrieval Methods,\\nReranking Results for Improved\\nRelevance, Hybrid Search\\nImplementation Techniques\\nRAG with Various Retrieval Methods\\nExact vs Approximate Retrieval Methods,\\nFiltering and Ranking Retrieved Data,\\nCustomizing Retrieval Approaches for\\nSpecific Applications\\nIntegrating Memory in RAG Systems\\nHow Memory Can Improve RAG,\\nPersisting and Recalling Information for\\nConsistent Results, Implementing Long-\\nTerm Memory in RAG\\nMultimodal Retrieval-Augmented\\nGeneration\\nCombining Text, Images, and Other\\nModalities in RAG, Techniques for\\nMultimodal Retrieval and Generation,\\nPractical Applications of Multimodal\\nRAG Systems\\nUltimate Data Science & GenAI Bootcamp       Page  33\\nModule 19'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-01-30T20:27:03+00:00', 'source': './syllabus.pdf', 'file_path': './syllabus.pdf', 'total_pages': 34, 'format': 'PDF 1.4', 'title': 'Ultimate Data Science & GenAI Bootcamp', 'author': 'monal singh', 'subject': '', 'keywords': 'DAGdmhcqnYw,BAEmsmap8Lg,0', 'moddate': '2025-01-30T20:26:59+00:00', 'trapped': '', 'modDate': \"D:20250130202659+00'00'\", 'creationDate': \"D:20250130202703+00'00'\", 'page': 33}, page_content='In this course, you’ll gain hands-on experience in implementing end-to-end AI projects. You’ll learn\\nhow to manage the entire project lifecycle, from data collection and preprocessing to model\\ndevelopment, evaluation, and deployment. The module includes working on real-world AI projects,\\nwith a focus on best practices for integration, testing, and scalability. By the end, you’ll be\\nprepared to take on AI projects from start to finish, applying machine learning and deep learning\\ntechniques to solve real-world problems.\\nEnd-to-End AI Project Implementation\\nTopics\\nPython Project: Building End-to-End\\nApplications\\nOverview of Python Projects, Project\\nDesign and Architecture, Key\\nConsiderations in Python Projects\\n(Performance, Scalability, etc.), Best\\nPractices for Code Quality\\nEnd-to-End Machine Learning\\nProjects\\nUnderstanding End-to-End ML Projects,\\nKey Components of an End-to-End ML\\nProject, Project Example: Real-World ML\\nApplication\\nDeep Learning Projects\\nDeep Learning Fundamentals in Projects,\\nEnd-to-End Deep Learning Projects \\nGenerative AI End-to-End Projects\\nIntroduction to Generative AI Projects,\\nSteps in Building Generative AI Projects\\nUltimate Data Science & GenAI Bootcamp       Page  34\\nPROJECT')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdf loader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_loader1 = PyMuPDFLoader('../syllabus.pdf')\n",
    "pdf_docs1 = pdf_loader1.load()\n",
    "pdf_docs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "893551a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(PyMuPDFLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca232581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from beautifulsoup4->bs4) (4.14.0)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ---------------------------------------- 3/3 [bs4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ae8c396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | 🦜️🔗 LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to load PDFs | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to load PDFsOn this pageHow to load PDFs\\nPortable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\\nThis guide covers how to load PDF documents into the LangChain Document format that we use downstream.\\nText in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:\\n\\nAgglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\\nRun OCR on images to detect text therein;\\nClassify text as belonging to paragraphs, lists, tables, or other structures;\\nStructure text into table rows and columns, or key-value pairs.\\n\\nLangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\\nWe will demonstrate these approaches on a sample file:\\nfile_path = (    \"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\")\\nA note on multimodal modelsMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the Use of multimodal models section below.\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of Document objects-- one per page-- containing a single string of the page\\'s text in the Document\\'s page_content attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the pypdf Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\n%pip install -qU pypdf\\nfrom langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(file_path)pages = []async for page in loader.alazy_load():    pages.append(page)API Reference:PyPDFLoader\\nprint(f\"{pages[0].metadata}\\\\n\")print(pages[0].page_content)\\n{\\'source\\': \\'../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\\', \\'page\\': 0}LayoutParser : A Uniﬁed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( �), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University{melissadell,jacob carlson }@fas.harvard.edu4University of Washingtonbcgl@cs.washington.edu5University of Waterloow422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have beenprimarily driven by the application of neural networks. Ideally, researchoutcomes could be easily deployed in production and extended for furtherinvestigation. However, various factors like loosely organized codebasesand sophisticated model conﬁgurations complicate the easy reuse of im-portant innovations by a wide audience. Though there have been on-goingeﬀorts to improve reusability and simplify deep learning (DL) modeldevelopment in disciplines like natural language processing and computervision, none of them are optimized for challenges in the domain of DIA.This represents a major gap in the existing toolkit, as DIA is central toacademic research across a wide range of disciplines in the social sciencesand humanities. This paper introduces LayoutParser , an open-sourcelibrary for streamlining the usage of DL in DIA research and applica-tions. The core LayoutParser library comes with a set of simple andintuitive interfaces for applying and customizing DL models for layout de-tection, character recognition, and many other document processing tasks.To promote extensibility, LayoutParser also incorporates a communityplatform for sharing both pre-trained models and full document digiti-zation pipelines. We demonstrate that LayoutParser is helpful for bothlightweight and large-scale digitization pipelines in real-word use cases.The library is publicly available at https://layout-parser.github.io .Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis·Character Recognition ·Open Source library ·Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range ofdocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\nNote that the metadata of each document stores the corresponding page number.\\nVector search over PDFs\\u200b\\nOnce we have loaded PDFs into LangChain Document objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)for doc in docs:    print(f\\'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings\\nPage 13: 14 Z. Shen et al.6 ConclusionLayoutParser provides a comprehensive toolkit for deep learning-based documentimage analysis. The oﬀ-the-shelf library is easy to install, and can be used tobuild ﬂexible and accurate pipelines for processing documents with complicatedstructures. It also supports hiPage 0: LayoutParser : A Uniﬁed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( �), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University\\nLayout analysis and extraction of text from images\\u200b\\nIf you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of Document objects, where each object represents a structure on the page. The Document\\'s metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nUnstructured supports multiple parameters for PDF parsing:\\n\\nstrategy (e.g., \"fast\" or \"hi-res\")\\nAPI or local processing. You will need an API key to use the API.\\n\\nThe hi-res strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See local parsing section below for considerations when running locally.\\n%pip install -qU langchain-unstructured\\nimport getpassimport osif \"UNSTRUCTURED_API_KEY\" not in os.environ:    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")\\nUnstructured API Key: ········\\nAs before, we initialize a loader and load documents lazily:\\nfrom langchain_unstructured import UnstructuredLoaderloader = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",    partition_via_api=True,    coordinates=True,)docs = []for doc in loader.lazy_load():    docs.append(doc)API Reference:UnstructuredLoader\\nINFO: Preparing to split document for partition.INFO: Starting page number set to 1INFO: Allow failed set to 0INFO: Concurrency level set to 5INFO: Splitting pages 1 to 16 (16 total)INFO: Determined optimal split size of 4 pages.INFO: Partitioning 4 files with 4 page(s) each.INFO: Partitioning set #1 (pages 1-4).INFO: Partitioning set #2 (pages 5-8).INFO: Partitioning set #3 (pages 9-12).INFO: Partitioning set #4 (pages 13-16).INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: Successfully partitioned set #1, elements added to the final result.INFO: Successfully partitioned set #2, elements added to the final result.INFO: Successfully partitioned set #3, elements added to the final result.INFO: Successfully partitioned set #4, elements added to the final result.\\nHere we recover 171 distinct structures over the 16 page document:\\nprint(len(docs))\\n171\\nWe can use the document metadata to recover content from a single page:\\nfirst_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]for doc in first_page_docs:    print(doc.page_content)\\nLayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r aZejiang Shen® (<), Ruochen Zhang?, Melissa Dell®, Benjamin Charles Germain Lee?, Jacob Carlson®, and Weining Li®1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.Keywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiﬁcation [11,\\nExtracting tables and other structures\\u200b\\nEach Document we load represents a structure, like a title, paragraph, or table.\\nSome structures may be of special interest for indexing or question-answering tasks. These structures may be:\\n\\nClassified for easy identification;\\nParsed into a more structured representation.\\n\\nBelow, we identify and extract a table:\\nClick to expand code for rendering pages%pip install -qU matplotlib PyMuPDF pillowimport fitzimport matplotlib.patches as patchesimport matplotlib.pyplot as pltfrom PIL import Imagedef plot_pdf_with_boxes(pdf_page, segments):    pix = pdf_page.get_pixmap()    pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    fig, ax = plt.subplots(1, figsize=(10, 10))    ax.imshow(pil_image)    categories = set()    category_to_color = {        \"Title\": \"orchid\",        \"Image\": \"forestgreen\",        \"Table\": \"tomato\",    }    for segment in segments:        points = segment[\"coordinates\"][\"points\"]        layout_width = segment[\"coordinates\"][\"layout_width\"]        layout_height = segment[\"coordinates\"][\"layout_height\"]        scaled_points = [            (x * pix.width / layout_width, y * pix.height / layout_height)            for x, y in points        ]        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")        categories.add(segment[\"category\"])        rect = patches.Polygon(            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"        )        ax.add_patch(rect)    # Make legend    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]    for category in [\"Title\", \"Image\", \"Table\"]:        if category in categories:            legend_handles.append(                patches.Patch(color=category_to_color[category], label=category)            )    ax.axis(\"off\")    ax.legend(handles=legend_handles, loc=\"upper right\")    plt.tight_layout()    plt.show()def render_page(doc_list: list, page_number: int, print_text=True) -> None:    pdf_page = fitz.open(file_path).load_page(page_number - 1)    page_docs = [        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number    ]    segments = [doc.metadata for doc in page_docs]    plot_pdf_with_boxes(pdf_page, segments)    if print_text:        for doc in page_docs:            print(f\"{doc.page_content}\\\\n\")\\nrender_page(docs, 5)\\n\\nLayoutParser: A Uniﬁed Toolkit for DL-Based DIA5Table 1: Current layout detection models in the LayoutParser model zooDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.layout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.3.1 Layout Detection ModelsIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Diﬀerent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:1 import layoutparser as lp 2 image = cv2 . imread ( \" image_file \" ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( \" lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config \" ) 4 5 layout = model . detect ( image )LayoutParser provides a wealth of pre-trained model weights using various datasets covering diﬀerent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As document structures and layouts vary greatly in diﬀerent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.\\nNote that although the table text is collapsed into a single string in the document\\'s content, the metadata contains a representation of its rows and columns:\\nfrom IPython.display import HTML, displaysegments = [    doc.metadata    for doc in docs    if doc.metadata.get(\"page_number\") == 5 and doc.metadata.get(\"category\") == \"Table\"]display(HTML(segments[0][\"text_as_html\"]))\\nable 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200Dataset| Base Model\\'|NotesPubLayNet [38]F/MLayouts of modern scientific documentsPRImAMLayouts of scanned modern magazines and scientific reportsNewspaperFLayouts of scanned US newspapers from the 20th centuryTableBank [18]FTable region on modern scientific and business documentHJDatasetF/MLayouts of history Japanese documents\\nExtracting text from specific sections\\u200b\\nStructures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nBelow, we extract all text associated with the document\\'s \"Conclusion\" section:\\nrender_page(docs, 14, print_text=False)\\n\\nconclusion_docs = []parent_id = -1for doc in docs:    if doc.metadata[\"category\"] == \"Title\" and \"Conclusion\" in doc.page_content:        parent_id = doc.metadata[\"element_id\"]    if doc.metadata.get(\"parent_id\") == parent_id:        conclusion_docs.append(doc)for doc in conclusion_docs:    print(doc.page_content)\\nLayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oﬀ-the-shelf library is easy to install, and can be used to build ﬂexible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.\\nExtracting text from images\\u200b\\nOCR is run on images, enabling the extraction of text therein:\\nrender_page(docs, 11)\\n\\nLayoutParser: A Uniﬁed Toolkit for DL-Based DIAfocuses on precision, eﬃciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and ﬂexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.115.1 A Comprehensive Historical Document Digitization PipelineThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese ﬁrm ﬁnancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diﬀerent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.‘Active Learning Layout Annotate Layout Dataset | +—— Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing — Handy Data Structures & \\\\ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models ¥ Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization PipelineAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiﬁed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\\nNote that the text from the figure on the right is extracted and incorporated into the content of the Document.\\nLocal parsing\\u200b\\nParsing locally requires the installation of additional dependencies.\\nPoppler (PDF analysis)\\n\\nLinux: apt-get install poppler-utils\\nMac: brew install poppler\\nWindows: https://github.com/oschwartz10612/poppler-windows\\n\\nTesseract (OCR)\\n\\nLinux: apt-get install tesseract-ocr\\nMac: brew install tesseract\\nWindows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\\n\\nWe will also need to install the unstructured PDF extras:\\n%pip install -qU \"unstructured[pdf]\"\\nWe can then use the UnstructuredLoader much the same way, forgoing the API key and partition_via_api setting:\\nloader_local = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",)docs_local = []for doc in loader_local.lazy_load():    docs_local.append(doc)\\nWARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model nameINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...\\nThe list of documents can then be processed similarly to those obtained from the API.\\nUse of multimodal models\\u200b\\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\\nIn principle we can use any LangChain chat model that supports multimodal inputs. A list of these models is documented here. Below we use OpenAI\\'s gpt-4o-mini.\\nFirst we define a short utility function to convert a PDF page to a base64-encoded image:\\n%pip install -qU PyMuPDF pillow langchain-openai\\nimport base64import ioimport fitzfrom PIL import Imagedef pdf_page_to_base64(pdf_path: str, page_number: int):    pdf_document = fitz.open(pdf_path)    page = pdf_document.load_page(page_number - 1)  # input is one-indexed    pix = page.get_pixmap()    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    buffer = io.BytesIO()    img.save(buffer, format=\"PNG\")    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\\nfrom IPython.display import Image as IPImagefrom IPython.display import displaybase64_image = pdf_page_to_base64(file_path, 11)display(IPImage(data=base64.b64decode(base64_image)))\\n\\nWe can then query the model in the usual way. Below we ask it a question on related to the diagram on the page.\\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")API Reference:ChatOpenAI\\nfrom langchain_core.messages import HumanMessagequery = \"What is the name of the first step in the pipeline?\"message = HumanMessage(    content=[        {\"type\": \"text\", \"text\": query},        {            \"type\": \"image_url\",            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},        },    ],)response = llm.invoke([message])print(response.content)API Reference:HumanMessage\\nINFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"``````outputThe first step in the pipeline is \"Annotate Layout Dataset.\"\\nOther PDF loaders\\u200b\\nFor a list of available LangChain PDF loaders, please see this table.Edit this pageWas this page helpful?PreviousHow to load Microsoft Office filesNextHow to load web pagesSimple and fast text extractionVector search over PDFsLayout analysis and extraction of text from imagesExtracting tables and other structuresExtracting text from specific sectionsExtracting text from imagesLocal parsingUse of multimodal modelsOther PDF loadersCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://python.langchain.com/docs/how_to/document_loader_pdf/\", )\n",
    ")\n",
    "\n",
    "web_docs = web_loader.load()\n",
    "web_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761fe9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://quotes.toscrape.com'}, page_content=\"\\n“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\\nby Albert Einstein\\n(about)\\n\\n\\n            Tags:\\n            \\nchange\\ndeep-thoughts\\nthinking\\nworld\\n\\n\\n“It is our choices, Harry, that show what we truly are, far more than our abilities.”\\nby J.K. Rowling\\n(about)\\n\\n\\n            Tags:\\n            \\nabilities\\nchoices\\n\\n\\n“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\\nby Albert Einstein\\n(about)\\n\\n\\n            Tags:\\n            \\ninspirational\\nlife\\nlive\\nmiracle\\nmiracles\\n\\n\\n“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\\nby Jane Austen\\n(about)\\n\\n\\n            Tags:\\n            \\naliteracy\\nbooks\\nclassic\\nhumor\\n\\n\\n“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\\nby Marilyn Monroe\\n(about)\\n\\n\\n            Tags:\\n            \\nbe-yourself\\ninspirational\\n\\n\\n“Try not to become a man of success. Rather become a man of value.”\\nby Albert Einstein\\n(about)\\n\\n\\n            Tags:\\n            \\nadulthood\\nsuccess\\nvalue\\n\\n\\n“It is better to be hated for what you are than to be loved for what you are not.”\\nby André Gide\\n(about)\\n\\n\\n            Tags:\\n            \\nlife\\nlove\\n\\n\\n“I have not failed. I've just found 10,000 ways that won't work.”\\nby Thomas A. Edison\\n(about)\\n\\n\\n            Tags:\\n            \\nedison\\nfailure\\ninspirational\\nparaphrased\\n\\n\\n“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\\nby Eleanor Roosevelt\\n(about)\\n\\n\\n            Tags:\\n            \\nmisattributed-eleanor-roosevelt\\n\\n\\n“A day without sunshine is like, you know, night.”\\nby Steve Martin\\n(about)\\n\\n\\n            Tags:\\n            \\nhumor\\nobvious\\nsimile\\n\\n\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraping specific classes\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://quotes.toscrape.com\",),\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "        class_=('quote', 'text', 'author', 'tags')\n",
    "    ))\n",
    ")\n",
    "\n",
    "web_docs = web_loader.load()\n",
    "web_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80c9fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from arxiv) (2.32.4)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.4.26)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "\n",
      "   ------------- -------------------------- 1/3 [feedparser]\n",
      "   ---------------------------------------- 3/3 [arxiv]\n",
      "\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700b8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"1706.03762\",  # This is the search term (Attention all you need paper)\n",
    "    load_max_docs=2      # Maximum number of results to fetch\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3675569e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-09-02', 'Title': 'Deep Reinforcement Learning in Surgical Robotics: Enhancing the Automation Level', 'Authors': 'Cheng Qian, Hongliang Ren', 'Summary': 'Surgical robotics is a rapidly evolving field that is transforming the\\nlandscape of surgeries. Surgical robots have been shown to enhance precision,\\nminimize invasiveness, and alleviate surgeon fatigue. One promising area of\\nresearch in surgical robotics is the use of reinforcement learning to enhance\\nthe automation level. Reinforcement learning is a type of machine learning that\\ninvolves training an agent to make decisions based on rewards and punishments.\\nThis literature review aims to comprehensively analyze existing research on\\nreinforcement learning in surgical robotics. The review identified various\\napplications of reinforcement learning in surgical robotics, including\\npre-operative, intra-body, and percutaneous procedures, listed the typical\\nstudies, and compared their methodologies and results. The findings show that\\nreinforcement learning has great potential to improve the autonomy of surgical\\nrobots. Reinforcement learning can teach robots to perform complex surgical\\ntasks, such as suturing and tissue manipulation. It can also improve the\\naccuracy and precision of surgical robots, making them more effective at\\nperforming surgeries.'}, page_content='Deep Reinforcement Learning in Surgical \\nRobotics: Enhancing the Automation Level \\n \\nCheng Qian# and Hongliang Ren* \\n#Department of Electrical Engineering and Information Technology, Technical University of \\nMunich, Germany \\n*Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong \\n \\nAbstract:  \\nSurgical robotics is a rapidly evolving field that is transforming the landscape of surgeries. \\nSurgical robots have been shown to enhance precision, minimize invasiveness, and alleviate \\nsurgeon fatigue. One promising area of research in surgical robotics is the use of reinforcement \\nlearning to enhance the automation level. Reinforcement learning is a type of machine learning \\nthat involves training an agent to make decisions based on rewards. This literature review aims \\nto comprehensively analyze existing research on reinforcement learning in surgical robotics. \\nThe review identified various applications of reinforcement learning in surgical robotics, \\nincluding pre-operative, intra-body, and percutaneous procedures, listed the typical studies, \\nand compared their methodologies and results. The findings show that reinforcement learning \\nhas great potential to improve the autonomy of surgical robots. Reinforcement learning can \\nteach robots to perform complex surgical tasks, such as suturing and tissue manipulation. It can \\nalso improve the accuracy and precision of surgical robots, making them more effective at \\nperforming surgeries. \\nKey Words:  \\nSurgical robotics, reinforcement learning, surgical autonomy, tissue manipulation, \\npercutaneous procedures, suturing  \\nI. Introduction \\nThe use of surgical robots has significantly increased in the last decade, driven by the need for \\nprecision, safety, and efficiency in surgeries [1]. Since the appearance of da Vinci robotic-\\nassisted surgical system in 2000 [3], surgical robots have proven to help perform minimally \\ninvasive surgeries (MIS), providing better visualization, higher precision, and reduced \\ninvasiveness, and helping reduce surgeons\\' fatigue [2]. However, the full potential of surgical \\nrobots has yet to be realized, and there is still a need to improve their autonomy. In the last \\ndecade, more and more studies have been conducted on autonomous surgical robots [4]. To \\nachieve autonomy in surgery, it is crucial for robots to understand the surgical task objectives, \\nperceive complex physical environments, and autonomously make decisions. One of the biggest \\nchallenges in the autonomy of surgical robots is the high variance of surgical tasks [13], which is \\nhard to address by explicitly modeling and planning. Therefore, Artificial Intelligence (AI) \\nsolutions emerged due to the model-free property and learning capability. \\nDeep Reinforcement Learning (DRL), a deep learning-based planning method that allows \\nrobots to learn from interaction with environments in a semi-supervised fashion without a pre-\\ndefined model, is one of the most promising approaches [5]. DRL has been increasingly \\nhighlighted in recent years, since its success in Atari [7]. It has demonstrated the possibility of \\nenabling intelligent agents to outperform human experts in multiple fields. Compared to \\nconventional planning methods, DRL provides advantages, e.g., end-to-end learning, complex \\ndecision-making, generalization, and transferability, handling uncertainties, and continuous \\nlearning. These properties enable DRL to handle high-dimensional inputs from cameras and \\nsensors in surgeries, apply its acquired knowledge and skills to different patients, handle \\nunforeseen variations, and continuously learn and refine their performance during surgery \\nprocedures. For these reasons, in the context of surgical robots, DRL provides a powerful \\nmodel-free framework and a set of tools for learning various complicated surgical tasks with \\ncomplex physical environments, which are hard to model [6]. Many studies have utilized DRL \\non robots under abundant surgical scenarios, e.g., ultrasound scanning, cutting and sewing, \\ntissue retraction, needle steering, and catheterization. Currently, there have been some reviews \\non DRL in the scope of medical imaging, e.g., radiation therapy, image registration [8], health \\ncare application, e.g., clinical decision support [9, 10, 12], medicine, e.g., medicine treatment or \\ndevelopment [11]. However, there still lacks a review specifically on the applications of DRL in \\nmedical robots. To fill this gap, this chapter will present a literature review highlighting the \\ntypical state-of-art works in the past 5 years (2018-2023) that utilize DRL in autonomous \\nsurgical robots, and comparing their methodologies, limitations, and results. We divide these \\nworks into three categories according to their access modes, namely: \\n1) Extra-body skin-interfaced procedure \\n2) Intra-body procedure \\n3) Percutaneous procedure, \\nwhich are three main procedures that we find the combination of DRL and surgical robot is \\nmainly applied to. The various surgical tasks learned by the robot in this review are illustrated \\nin Fig. 1, including steerable needle planning in keyhole neurosurgery, needle insertion in \\nophthalmic microsurgery, neck vessel and spine US scanning, tissue cutting and retraction, and \\nwound suturing. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 1. The seven different robotic surgical tasks contained in this review \\n \\nTo exhaust the published review articles of the concerning fields and extract the most relevant \\nones, we searched keywords on the database and excluded the irrelevant articles. The articles \\nextraction pipeline is shown in Fig. 2. We also counted the number of studies that applied DRL \\nOphthalmic \\nmicrosurgery \\nNeck vessel US \\nscanning \\nSpine US scanning \\nWound suturing \\nKeyhole neurosurgery \\nTissue retraction \\nTissue cutting \\nin medical scenarios in the last 5 years. In Fig. 3, the number of articles on the application of \\nDRL in medical imaging, medical robotics, and dynamic treatment regime in recent 5 years are \\nlisted.  \\n \\nWe can see that the number of studies combining DRL with different medical fields has quickly \\nemerged in recent years, which indicates a growing trend.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 2. Articles selection pipeline with keywords \"surgical robot\", “autonomous”, and “deep \\nreinforcement learning” according to PRISMA [81] \\nDatabase search \\nn = 1070 articles \\nn = 265 articles \\nn = 39 articles \\nn = 26 articles \\nn = 805 articles excluded \\nReview or Survey = 186 \\nEarlier than 2000 = 323 \\nNot focus on surgical robot = 232 \\nFocus on environment simulation = 64 \\nn = 226 articles excluded \\nFocus on surgeon training = 15 \\nFocus on development = 38 \\nNot focus on deep reinforcement learning = 93 \\nNot focus on autonomous robotic surgery = 76 \\nn = 13 articles excluded \\nFull text unavailable = 6 \\nLack of experiments = 7 \\n \\n \\nFig. 3. Statistics of the number of publications on RL in three main medical applications \\nin PubMed in the past 5 years. The combination of DRL with autonomous surgical \\nrobots and other medical fields have a rising trend in the last 5 years. \\nThe rest of the chapters are organized as follows: Section 2 briefly introduces the fundamental \\ntheories in RL. Sections 3, 4, and 5 discuss the latest work of DRL in the fields of pre-operative \\nscanning, intra-body surgery, and Image-guided autonomous robotic surgery, respectively. \\n \\nII. Basics of Reinforcement Learning \\nBefore discussing the state-of-art works of DRL in surgical robotics fields, we will first give a \\ngeneral introduction to the fundamental knowledge in Reinforcement learning (RL). Learning \\nthrough interaction with the environment is the essence of RL [14], which means an agent learns \\nto take action through rewards and penalties and refines its policy accordingly.  \\nThe fundamental of RL includes five essential elements: agent, environment, action, state, and \\nreward. In the context of surgical robots, an example of it can be illustrated in Fig. 4, where the \\nrobot (agent) works at the surgical site of a human body (environment), moving the probe to \\nfind a feasible scan plane for the sacrum, and obtaining the current position information of \\nprobe via real-time ultrasound (US) images. At each time step, the robot possibly gets a positive \\nor negative reward based on the current US image, which guides the robot toward the standard \\nscan plane.  \\n \\n0\\n10\\n20\\n30\\n40\\n2018\\n2019\\n2020\\n2021\\n2022\\nNumber of publications on RL in medical applications in PubMed in \\nthe past 5 years\\nMedical Imaging\\nMedical Robotics\\nDynamic Treatment Regime\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 4. An illustration of agent-environment interaction in RL under the context of \\nsurgical robots \\n[Markov Decision Process] \\nMarkov Decision Process (MDP) is always used to formally describe the above-mentioned \\nagent-environment interaction, which consists of [17, 18]: \\n\\uf0b7 \\nState Space (S): The set of possible states the agent can be. Each state represents a \\nparticular configuration or situation in the environment. In surgical robot scenarios, the \\nstate is often chosen as the robot\\'s pose and target. \\n\\uf0b7 \\nAction Space (𝒜): The set of possible actions that the agent can take. Actions are the \\nchoices available to the agent in each state. Actions can be discrete or continuous \\ndepending on the accuracy requirements. It is preferred to be chosen as continuous in \\nsome safety-critical scenarios, which need high accuracy of control, e.g., in intra-body \\nsurgery. \\n\\uf0b7 \\nTransition (𝒯): The transition probabilities that describe the dynamics of the \\nenvironment. They specify the probability of transitioning from one state 𝑠 to another \\nstate 𝑠′, 𝑖f the agent takes action 𝑎, which is represented as 𝑇(𝑠′|𝑠, 𝑎). It includes \\nimportant information about the robot-environment interaction, e.g., the interaction \\nbetween the US probe and tissue. \\n\\uf0b7 \\nReward (R): The immediate feedback that the agent receives from the environment for \\nits action. It quantifies the desirability or value associated with transitions between \\nEnvironment \\nAgent \\nAction(a) \\nState(s) \\nReward(r) \\n  \\nstates. The reward function is typically denoted as 𝑟= 𝑅(𝑠′, 𝑎, 𝑠). It is commonly \\ndesigned to guide the robot to achieve its goal. For example, for standard scan plane \\nnavigation in robotic ultrasound scanning, the reward is commonly designed to be the \\npose improvement of the probe to the target pose. \\n\\uf0b7 \\nDiscount factor (γ): A value between 0 and 1 that determines the importance of future \\nrewards compared to immediate rewards. It determines the preference of the agent for \\nimmediate rewards or long-term cumulative rewards.  \\nGiven 𝑀𝐷𝑃(𝑆, 𝐴, 𝑇, 𝑅, 𝛾), the agent chooses the action at state 𝑠 with the observation 𝑜 it receives \\naccording to the policy 𝜋(𝑎|𝑠). When the policy is deterministic, 𝜋 is a mapping from state 𝑠 to \\naction 𝑎; when the policy is stochastic, 𝜋 represents the possibility of selecting action 𝑎 at state 𝑠.  \\nThe goal of RL is to find an optimal policy 𝜋∗ that maximizes the expectation of cumulative \\nreturn, which is denoted as: \\n𝜋∗= 𝑚𝑎𝑥\\n𝜋\\n𝔼[∑\\n𝛾 𝑟𝑡]\\n𝑇\\n𝑡=0\\n , \\nwhere 𝑟𝑡 is the reward at time 𝑡, and 𝑇 is the time horizon. \\nTo be noticed that sometimes the full state information is not available for the agent, but only a \\npart of it, instead. The agent has to predict the state information given an observation. For \\nexample, the ultrasound scanning robot has to detect its current position according to the real-\\ntime US image. In this case, the process is a partially observable MDP (POMDP) [29]. And the \\nset of the state information that is observable for the agent is called Observation. In this case, the \\npolicy 𝜋 is dependent on observation 𝑜 instead of state 𝑠. \\nBesides, in this review, only model-free RL algorithms are focused on. Therefore, the transition \\nis assumed to be unknown. \\n \\nIII. Deep Reinforcement Learning in Surgical \\nRobotics \\nIn this section, we will highlight the state-of-art studies of DRL for surgical robotics applications \\nand discuss them in three parts: pre-operative scanning, intra-body surgery, and percutaneous \\nsurgery. We will focus on how they formulate the problem in a DRL-based framework and \\ndifferent methodologies applied to augment RL to meet some surgery-specific requirements, \\nsuch as risk analysis. \\n[Pre-operative Procedure] \\nSurgical images are obtained using various imaging modalities and ultrasound (US) scanning is \\nthe one that has been widely studied in combination with robotics. Over the past two decades, \\nresearchers have begun exploring the potential of robotics in applying US scanning. By \\nequipping the robot arm with a probe, the robot can move the probe to perform US scanning on \\nthe patient. The accuracy, consistency, skill and maneuverability of robotic manipulators can be \\nused to improve the acquisition and utility of real-time ultrasounds [25]. However, to obtain \\nhigh-quality ultrasound images, it is crucial to navigate the US probe to the correct scan plane \\n[26] and maintain reasonable and consistent probe-skin contact force [27], as illustrated in Fig. 5. \\nTherefore, standard scan plane localization and contact force control are two main challenges in \\nrobotic US and so far, there have been several studies utilizing DRL to address them. In Table 1, \\nthe methodologies, metrics, and results used in the 9 reviewed papers in this section are listed. \\na. Standard Scan Plane Navigation \\nA standard scan plane in ultrasound imaging refers to a recommended imaging plane or view \\ncommonly used for a specific anatomical structure or diagnostic purpose. Finding the \\nappropriate scan planes is crucial to obtaining good-quality US images. To enable autonomous \\nrobotic ultrasound scanning, the robot should be capable of detecting its own position and \\nfinding the way toward the standard scan plane of the specific anatomy with the real-time US \\nimage it obtains.  \\nHase et al. [28] proposed a framework in 2020 to train the robot to autonomously navigate to \\nthe standard scan plane of the sacrum with the information of a sequence of history US images \\nwith Deep Q-Network (DQN) [7]. The agent is trained on the 2D US images acquired by grid \\ncovering and moves with 2-DOF actions, namely moving forward or backward. When moving \\ncloser to or further from the desired scan plane, the agent receives positive or negative rewards. \\nA binary classifier that determines whether the robotic probe is at the standard scan plane, \\ndepending on the current US image, is used to make the agent stop at the correct position.  \\nOne of the limitations in [28] is that the probe is assumed to find the scan plane by moving in a \\n2D space, which is unavailable in real US scanning scenarios, where the relative pose between \\nthe probe and sacrum is not static. Therefore, in Li et al. [31], the state and action space is \\ndesigned to be the 6D-pose and -twist of the probe so that the learned policy is no longer \\nrestricted to the collected data. The agent receives a reward proportional to the pose \\nimprovement of the probe at each time step. Besides, the quality of the US image is also \\nconsidered in this work by evaluating the pixel-wise confidence and giving corresponding \\nrewards to the agent. \\nAn agent that can recognize different anatomies and find the nearest one according to its \\ncurrent position can be advantageous for its flexibility in a real US scanning scenario. In Li et al. \\n[33], an agent is trained to find three different spinal anatomies with a given standard view \\nrecognizer for different spinal anatomies. \\nReal US images as observations given to the agent can be noisy, which makes the agent hard to \\npredict the real state correctly. One of the methods addressing this issue is presented in Bi, et al. \\n[39], which navigates the agent to the scan plane of the neck vessel. The study segments the US \\nimages with a pre-trained U-Net [40] in advance and provides the segmented mask to the agent, \\nwhere the area of interest has pixel values of one, while other areas have values of zero. \\nCompared to real US images, it is much easier for the agent to extract information from \\nsegmented masks, which only contain binary values. \\nBesides, Li, et al. [41] and Milletari, Fausto, Vighnesh Birodkar, and Michal Sofka [42] proposed \\nDRL-based frameworks for training an agent that guides a novice operator to find the standard \\nscan planes in transesophageal echocardiography and chest sonography, respectively.  \\nb. Pose and Force Control \\nThe way in which the ultrasound probe is positioned and controlled can have a significant \\nimpact on both the quality of the resulting ultrasound images and the overall safety of the \\nrobotic ultrasound system. It is essential to carefully control the pose and force used when \\noperating the probe, as any errors or inconsistencies can compromise the quality of the imaging \\nand potentially cause harm to the patient or the system itself. A system can ensure imaging \\nquality while minimizing potential risks or complications by taking a deliberate approach to \\nprobe control. Unlike rigid objects, force control of the US probe should consider the \\ncompliance of the patient body. Besides, errors caused by target movement have to be also \\ncompensated. Both of them are hard to be accurately modeled. However, by taking advantage \\nof DRL\\'s model-free and end-to-end properties, the control of the US probe can be solved \\nwithout explicitly modeling.  \\nIn Ning, et al. [42], an agent is trained with Proximal Policy Optimization (PPO) [21] to \\nautonomously control the pose and force of the US probe with a force-to-displacement \\nadmittance controller. The agent has to provide proper 2D input command for the controller, \\nnamely the desired torque of the US probe in long- and short-horizontal-direction, as illustrated \\nin Fig. 5, to keep the vertical between the probe and the scanned surface with suitable contact \\nforce. A 6-D force sensor is attached to the robot end-effector to give the force feedback to the \\nagent. A positive reward is given when the vertical contact force is suitable and the horizontal \\ncontact force is small enough, which means the probe is approximately vertical to the scanned \\nsurface. A similar work is done in [46], however, with an inverse RL method to study the \\nreward function. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 5. The robot moves the probe to find the standard scan plane while keeping the contact \\nforce in a suitable range. \\nDiffering from [42] and [46], in Ning, et al. [44], the scene image captured by a RGB camera is \\nalso provided to the agent as observation. From the scene image, the agent can extract the \\ninformation of both its own pose and the target pose. In the study [45], a convolutional \\nautoencoder (CAE) and a reward prediction network are employed to achieve two objectives \\nsimultaneously. Firstly, the CAE is used to decrease the dimensionality of the observation \\nspace, allowing for more efficient data processing. Secondly, the reward prediction network \\nencodes force and ultrasound image information into the scene image, enhancing the resulting \\nimage\\'s quality. By utilizing these techniques, the researchers improved the overall efficiency of \\nthe system. \\n[Intra-body Procedure] \\nRecently, flexible surgical robotic systems have been developed to improve intra-body surgery \\nin the narrow areas of the human body. However, the teleoperation of surgical robots can be \\nexhausting and needs long-term training time. More and more researchers have been working \\non the possibility of automating difficult surgical handling tasks, e.g., tissue cutting, suturing, \\nknot tying, and tissue retraction, to reduce the surgeons’ workload [64]. However, the large \\nquantity of soft tissue in surgeries, including organs, blood vessels, and muscles, possess \\ninherent compliance and deformability, making their manipulation challenging and requiring \\nmodeling and planning with high accuracy and complexity. Therefore, some studies have tried \\nContact  \\n Force \\nt \\nto unleash the model-free property of DRL in automating the tissue manipulation tasks in MIS, \\nincluding tensioning, suturing and retraction. In Table 2, the methodologies, metrics, and \\nresults used in the 8 reviewed papers in this section are listed. \\n \\nRef. \\nDescription \\nAlgorithm \\nObservation \\nDOF \\nof \\nAction \\nReward \\nResult \\n[28] \\nNavigation towards the \\nstandard scan plane of sacrum \\nDQN \\nSequential US \\nimages \\n1 \\n+ moving closer  \\n- moving further \\nPolicy correctness of 79.53% \\nand reachability of 82.91% \\n[31] \\nNavigation towards the \\nstandard scan plane of the \\nspine with consideration of US \\nimage quality \\nDQN \\nSequential US \\nimages \\n6 \\n+ pose improvement or \\nimage quality improvement \\n- unallowable pose \\n92% and 46% success rate in \\nintra- and inter-patient \\nsettings, respectively \\n[33] \\nNavigation towards different \\nstandard scan planes of spinal \\nanatomies \\nDQN \\nSequential US \\nimages \\n6 \\n+ pose improvement or \\nimage quality improvement \\n- \\nunallowable pose \\nPose error ~ 5.18mm/5.25° \\nfor intra-patient settings; \\nPose error ~ 2.87mm/17.49° \\nfor inter-patient settings. \\n[39] \\nNavigation towards the \\nstandard scan plane of carotid \\nvessels \\nA2C \\nSequential \\nsegmented US \\nimages + \\nSequential \\nvessel area \\nchanges \\n3 \\n+ vessel area improvement  \\n- too small vessel area \\n91.5% and 80% success rate \\nin simulated and real \\nenvironment, respectively \\n[41] \\nGuidance for novice operators \\nin moving TEE probe towards \\nthe standard scan plane of heart \\nwith pressure awareness \\nDQN \\nSequential US \\nimages \\n3 \\n+ pose improvement \\n- unallowable pose \\nPose error of 2.72 mm / \\n2.69° and 8.15 mm / 5.58° \\nwithout and with pressure \\nawareness, respectively \\n[42] \\nGuidance for novice operators \\nin obtaining correct US images \\nof anatomy of interest \\nDQN \\nSequential US \\nimages \\n4 \\n+ moving closer \\n- moving further \\n86.1% success rate in giving \\ncorrect guidance \\n[43] \\nForce control between probe \\nand phantom \\nPPO \\n6-D contact \\nforce \\n2 \\n+ small horizontal force \\n- big horizontal force or too \\nbig or too small vertical force \\nDifference of skin area in US \\nimages within 3\\u2009±\\u20090.4% from \\nthe hand-free scanning \\napproach \\n[44] \\nForce control between probe \\nand phantom \\nPPO \\nSingle encoded \\nRBG scene \\nimage \\n3 \\n+ moving closer to the target \\nsurface, good US image \\nquality, correct relative \\nposition \\n- otherwise \\n93% success rate in getting \\nfeasible US images \\n[48] \\nForce control between probe \\nand phantom \\nPPO + \\nInverse RL \\nContact force \\nand torque, \\nand \\ncorresponding \\nlinear and \\nangular speed \\n6 \\nReward shaping via inverse \\nRL \\nPosture error of 2.3±1.3°and \\n1.9±1.2° in X and Y axis \\ncompared to manual \\noperation \\n \\nTable 1. The formulation, methodologies and results of the reviewed papers in the section on pre-operative planning\\na. Tensioning \\nRobotic surgery has revolutionized the medical field, and an electric knife is an effective tool for \\ncutting and removing thin tissues. However, the electric knife alone may not be enough to cut \\neffectively when it comes to deformable soft tissue. This is because soft tissue needs to be held \\nin tension to be cut most effectively. Therefore, a second tool is required to pinch and tension \\nthe material while cutting. This technique is illustrated in Fig. 6, which demonstrates the use of \\ntwo tools cooperatively to cut soft tissue. The first tool, the knife, cuts the tissue while the \\nsecond tool, which pinches and tensions the material, helps the knife cut more effectively. This \\ntechnique is particularly important in robotic surgery, where precision is critical, and using \\nmultiple tools can help ensure the surgery is successful. To let the robot autonomously assist the \\nsurgeon in cutting. The robot has to learn the tensioning policies for different cutting contours.  \\n  \\n \\n \\n \\n \\n \\n \\n \\nFig. 6. The blue manipulator pinches the grey point and tensions the tissue, while the green \\nmanipulator is responsible for cutting (e.g., in Endoscopic Submucosal Dissection, ESD) \\nIn Thananjeyan, et al. [50], a finite-element model is first developed for simulating the \\ndeformation and cutting of tissue. Then, considering the kinematics constraints of the surgical \\nrobot arm, the cutting outline is divided into several subdivision segments in advance. The \\nagent is lastly trained in Trust region policy optimization (TRPO) [22] to learn the optimal \\ntensioning policy to minimize the cutting error with a single fixed pinch point. The agent \\nreceives sparse rewards at the end of each episode according to the final cutting error. \\nThere needs to be more than a single tensioning point to assist cutting, when the cutting pattern \\nis complex, for example, the cutting contours are zigzag and have to be divided into many \\nsegments. Therefore, in [51, 52], an improved pipeline is proposed to address this limitation. \\nSpecifically, a pinch point is chosen for each cutting segment instead of the whole contour and \\nthe agent learns different tensioning policies for each pinch point in a similar way as in [50]. \\nCompared to [50], the improved method shows more accurate and robust performance, when \\nhandling complex cutting contours. \\nb. Suturing \\nSuturing is a critical step in wound closure during surgeries and in robot-assisted surgeries. \\nHowever, robotic suturing can be laborious for novice operators. A collaborative robot that \\nautonomously assists surgeons in performing some sub-tasks in robotic surgeries can effectively \\noperators’ fatigue. So far, utilizing DRL on surgical collaborative robots can learn how to \\nautonomously collaborate with surgeons in teleoperated suturing process, as illustrated in Fig. \\n7. In Table 3, the methodologies, metrics, and results used in the 9 reviewed papers in this \\nsection are listed. \\nIn Varier, et al. [53], an agent is trained to use an assistive Patient Side Manipulator (PSM) to \\npull the needle, translate it to the next suture point and hand it to the surgeon after the needle is \\ninserted through the tissue with the main PSM by the operator, as illustrated in Fig. 7. To \\naddress varying suture styles of users, the users are instructed to perform a running suture \\nwithout a collaborative robot. The trajectory of a single hand-off task is collected and an \\nalgorithm is designed to generate sparse rewards on the trajectory. Then, the agent imitates the \\noperator’s hand-off trajectory by maximizing the cumulative collected reward. \\n \\n \\n \\n \\n \\nFig. 7. The main manipulator (left) is operated by the surgeon, which inserts the needle through \\nthe tissue, while the assistive manipulator (right) pulls the needle out and hands it to the main \\nmanipulator. \\n \\nHowever, in [53], the state of the agent is respect to a fixed frame, which makes the learned \\npolicy strongly depend on the selection of the frame. In Chiu, et al. [55], an improved method is \\nproposed to address this issue by designing action spaces as being respect to the ego-centric \\nframe, which means the policy depends on the relative position and orientation of the assistive \\nPSM relative to the main PSM and therefore can be directly applied to different robot \\nconfigurations.  \\nc. Retraction \\nAnother kind of tissue manipulation task in robotic surgery is tissue retraction. I.e., to uncover \\nthe underlying anatomical region, the tissue is repeatedly held and pulled back in MIS [57]. To \\nautonomously perform the tissue retraction task, the robot has to find the position of the tissue, \\nmove closer to it and grasp it to the target position. \\nIn Pore, et al. [58], a robot learns to approach the tumor from its initial position and retract it to \\nthe target position from human demonstrations. The position of the tumor is assumed to be \\nknown from the pre-operative data. The agent gets the reward based on whether it moves closer \\nto the tumor or target position before or after grasping. Human demonstrations are collected to \\nenable imitation learning. The agent is trained with Generative adversarial imitation learning \\n(GAIL) and PPO, where PPO acts as the action generator. \\n Safety is always the priority in surgeries, especially in tissue retraction, where the robot directly \\ninteracts with the tissue. However, due to the model-free property of DRL, the safety of the \\nlearned policy is always hard to verify, which leads to significant potential risks in surgery. In \\nanother work by Pore, et al. [60], a framework for robotic tissue retraction incorporates the \\nsafety constraints during the DRL training with formal verification, which adds a penalty term \\nin the reward function for unsafe actions and evaluates the safety of the learned policy [61]. The \\nproposed method shows a large reduction in the safety violation rate, compared to [58]. \\nIn [58] and [60], the agents are assumed to have access to the full-state information, e.g., the \\nrobot joint angles and tissue position, which makes the policy largely depend on the accuracy of \\nstate extraction and lack robustness against the patient movements. In Scheikl, et al., [62], a \\nvision-based framework for robotic tissue retraction is proposed. The agent is trained with the \\nsimulated RGB scene image and a translation model is trained to translate the observation \\nfunction in simulation to the one in reality using domain adaptation. The trained agent achieves \\na success rate of 50% in real surgical scenarios. \\n \\n[Percutaneous Procedure] \\nPercutaneous techniques are increasingly used in many surgical scenarios, including \\nneurosurgery and ophthalmic surgery. The practical advantages include lower complexity rates \\nand faster recovery time. It involves the precise insertion of a thin, hollow needle into specific \\nanatomical structures for diagnostic, therapeutic, or monitoring purposes. This technique \\nallows surgeons to access internal body tissues, organs, or vessels without the need for open \\nsurgery, minimizing patient discomfort, reducing the risk of complications, and promoting \\nfaster recovery. \\na. Needle Insertion \\nNeedle insertion surgery is common in various medical fields, including keyhole neurosurgery \\n[65] and ophthalmic surgery [66]. Conventionally, a rigid needle is commonly used in these \\nprocedures. There has been some research on DRL-based needle path planning. In Keller et al. \\n[73], an agent is trained to control the yaw, pitch, and depth of the needle to achieve the goal \\nposition in ophthalmic surgery with optical coherence tomography (OCT) image as observation. \\nIn Gao, et al. [74], an agent is trained to provide a remote center of motion (RCM) [75] \\nrecommendation in brain surgery. The author considers three aspects to evaluate the quality of \\nRCM, namely clinical obstacle avoidance (COA), mechanically inverse kinematics (MIK) and \\nmechanically less motion (MLM) and the reward function is also designed based on these three \\naspects. \\nHowever, it can be challenging for rigid needles to find safe trajectories to insert toward the \\ntarget without touching some critical anatomies, e.g., blood vessels, especially when the \\nstructure is complicated. Therefore, steerable needles have attracted much attention in the last \\ndecade due to their flexibility. Accurate path planning is one of the most crucial factors for \\nsuccessful steerable needle insertion, where the tissue-needle interaction has to be considered. \\nIn Lee, et al. [68], an agent is trained to perform pre-operative path planning for steerable \\nneedles in keyhole neurosurgery with DQN. The environment is simulated by segmenting 2D \\nMRI images into obstacles and obstacle-free areas. The agent controls the bevel direction \\nrotation and insertion depth to insert the needle toward the target area. The agent is rewarded \\nwhen achieving the goal and punished when entering an unsafe area, e.g., a blood vessel. In \\nKumar, et al. [69] and Segato, et al. [72, 79], similar frameworks are proposed, however, with 3D \\nMRI images to enable 3D path planning and continuous actions. \\nPre-operative path planning provides initial planning, including the insertion point and a rough \\nglobal plan. However, due to unexpected anatomical movements or needle-tissue interactions, \\nthe pre-planned path can be violated and therefore, intra-operative replanning is needed. \\nFurthermore, it is also essential for the surgeon to easily detect the risk potential (possibility of \\nthe needle entering unsafe areas) of the re-planned path. In Tan, et al. [70], a universal \\ndistributional Q-learning (UDQL) [71] based training framework is proposed to enable fast \\nreplanning and risk management. In UDQL, the expected Q-value is parameterized with a \\nvalue distribution, so that only a distribution with a high mean Q-value and low variance can \\nbe considered a safe plan.\\n \\nRef. \\nDescription \\nAlgorithm \\nObservation \\nDOF \\nof \\nAction \\nReward \\nResult \\n[50] \\nTensioning policy for the selected \\npinch point \\nTRPO \\nCutting \\ntrajectory and \\nfiducial points \\nlocations \\n2 \\nSparse reward according to \\nthe final cutting error, when \\nepisode ends \\nImprovement of 43.3% \\ncompared to non-tension \\nbaseline in term of cutting \\nerror \\n[51,  \\n52] \\nAn improved pipeline enabling \\nselecting multiple pinch points \\nfor different cutting segments \\nTRPO \\nCutting \\ntrajectory and \\nfiducial points \\nlocations \\n2 \\nSparse reward according to \\nthe final cutting error, when \\nepisode ends \\nImprovement of 50.6% \\ncompared to non-tension \\nbaseline in term of cutting \\nerror \\n[53] \\nAutonomous collaborative \\nneedle hand-off task of PSM \\nQ-Learning \\n3-D Position of \\nrobotic tip \\n3 \\nSparse reward according to \\nthe data points on user-\\ndefined trajectory \\nDissimilarity between learned \\ntrajectory and reference \\ntrajectory with mean and \\nstandard deviation of \\n[2.857mm, 1.488mm, \\n0.774mm] and [3.388mm, \\n2.286mm, 1.808mm] \\n[55] \\nAutonomous collaborative \\nneedle hand-off task of PSM in \\nego-centric spaces \\nDDPG + \\nBC \\nRelative \\nposition and \\nquaternion \\n6 \\n+ reaching target pose \\n- collision  \\n97% and 73.3% success rate in \\nsimulation and real-world \\nenvironment, respectively \\n[58] \\nRobotic tissue retraction learning \\nfrom expert demonstration \\nPPO + \\nGAIL \\ngripper state, \\nend-effector \\nlocation, \\n3 \\n+ moving closer to tumor or \\ntarget position \\n+ moving further from tumor \\nor target position \\nAverage tumor exposure \\npercentage of 84% and 90% in \\nsimulation and real-world \\nenvironment, respectively \\ntarget location \\n[60] \\nRobotic tissue retraction \\nconsidering safety properties \\nPPO +  \\nFormal \\nVerification \\nGripper state, \\nend-effector \\nlocation, \\ntarget location \\n3 \\n+ moving closer to tumor or \\ntarget position \\n+ moving further from tumor \\nor target position or violating \\nsafety constraints \\nSafety violation rate of 3.07% \\nand violation rate reduction \\nof 24%, compared to non-\\nsafety method \\n[62] \\nRobotic tissue retraction with \\nsim-to-real \\nPPO +  \\nDCL \\nSequential \\ntranslated \\nscene image \\n2 \\n+ moving closer to tumor or \\ntarget position \\n+ moving further from tumor \\nor target position \\n50% success rate in real world \\nenvironment with raw \\ncamera images as input \\n \\nTable 2. The formulation, methodologies and results of the reviewed papers in the section of intra-body surgery.\\nb. Catheterization \\nCatheterization is one of the most commonly used procedures in endovascular intervention. \\nThe catheter is guided to the target of the disease in the vasculature along with treatments such \\nas stenting, embolization, and ablation, as illustrated in Fig. T8. However, the guidance of the \\ncatheter is not trivial. The surgeon needs to manipulate the catheter with limited 2D \\nfluoroscopic information and minimize unwanted excessive tissue contacts. Due to the \\ndifficulty of manually operating the catheter, robot-assisted catheterization has been researched \\nin the last decade and DRL is one of the promising methods for its path or trajectory planning. \\nIn Chi, et al. [76], an agent is trained to optimize the catheterization trajectory demonstrated by \\nthe experts. The expert demonstrations are first parameterized with dynamic motion primitives \\n(DMP). The agent adjusts the parameters of DMP to optimize the trajectory, guiding the \\ncatheter tip towards the desired vessel plane while matching the trajectory with the vessel \\ncenterline as much as possible. The agent is trained with Path Integral (𝑃𝐼2) [77] algorithm, \\nwhich is a robust RL implementation based on trajectory rollouts. The environment is based on \\nvascular models with flow simulation. Furthermore, Chi, et al. [78] proposed a closed-loop \\ncatheter control framework based on GAIL to imitate the expert catheterization demonstration. \\nAn electromagnetic (EM) tracking sensor is attached to the catheter tip to take into account its \\nreal-time pose to enable intra-operative control. The agent is trained to imitate the five-motion \\nprimitive of the expert’s hand demonstration, namely pull, push, clockwise rotation, anti-\\nclockwise rotation, and stand-by. Besides, in Omisore, et al. [80], DRL is utilized to tune the \\nparameters of a PID controller in real time, to let it adapt to different blood flow settings. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 9. The catheter is guided to the target position in vessel. The blue dot line is the planned \\npath. \\nRef. \\nDescription \\nAlgorithm \\nObservation \\nDOF \\nof \\nAction \\nReward \\nResult \\n[73] \\nNeedle path planning for \\nOphthalmic microsurgery \\nDDPGfD \\nHeight maps of \\ntwo corneal \\nsurfaces  \\n3 \\nSparse reward when reaching \\nthe target position \\nPerforation-free percent depth of \\n84.75% ± 4.91% \\n[74] \\nRCM recommendation for keyhole \\nneuro surgery \\nPPO \\nTarget position, \\nRobot joints \\nposition \\n2 \\nSparse reward according to \\npositioning accuracy, solvable \\ninverse kinematics and \\nmechanical joint motion \\n93% success rate of finding \\noptimal RCM \\n[68] \\nNavigation of steerable needle \\ntowards the target position in brain \\nin 2D space  \\nDQN \\n2D map based on \\nsegmented MRI \\nimages \\n2 \\n+ position improvement \\n93.6% success rate of achieving \\ntarget position \\n[69] \\nNavigation of steerable needle \\ntowards the target position in brain \\nin 3D space  \\nDDPG \\n3D map based on \\nsegmented MRI \\nimages \\n2 \\n+ achieve goal or in safe area \\n- in unsafe area \\nOutperforms RRT* under \\ndifferent quantiles of constraints \\n[72] \\nNavigation of steerable needle \\ntowards the target position in brain \\nin 3D space  \\nGAIL \\n3D map based on \\nsegmented MRI \\nimages \\n6 \\n+ achieve goal  \\n- obstacle collision \\nAverage targeting error of 1.34 ± \\n0.52 mm in position and 3.16 ± \\n1.06 degrees in orientation  \\n[79] \\nNavigation of steerable needle \\ntowards the target position in brain \\nin 3D space  \\nGA3C \\nSequential frames \\nof 3D map based \\non segmented MRI \\nimages \\n6 \\n+ achieve goal  \\n- obstacle collision \\nOutperforms RRT* under \\ndifferent quantiles of constraints \\n \\nTable 3. The formulation, methodologies and results of the reviewed papers in the section of percutaneous surgery. \\n[76] \\nOptimization of catheterization \\ntrajectory obtained from \\ndemonstration \\n𝑃𝐼2 \\nCatheter tip pose, \\nTarget position \\n2 \\n+ position improvement,  \\n vessel centerline alignment \\nAverage targeting error of \\n2.92mm and path length of \\n258.67mm \\n[78] \\nNavigation of catheter tip to the \\ntarget position \\nGAIL+ PPO \\nCatheter tip pose, \\nTarget position \\n3 \\n+ position improvement \\n- obstacle collision \\n82.4% success rate of achieving \\ntarget position \\n[80] \\nAdaptation of PID controller \\nparameters for catheterization \\nDQN \\nCatheter tip pose \\n3 \\n+ position improvement \\nAverage error of 0.003 ± 0.0058 \\nmm with respect to setting point \\n[Conclusion] \\nThis literature review has provided an overview of the application of Deep Reinforcement \\nLearning (DRL) in surgical robots. We divided the state-of-art works that applied DRL on \\nsurgical robots into three main fields: skin-interfaced, intra-body, and percutaneous, discuss \\nhow they formulate the problem based on RL-framework, and compare their methodologies \\nand limitations. Based on the outstanding performance of DRL in these works, the integration \\nof DRL algorithms into surgical robotic systems has the potential to revolutionize the field of \\nrobotic-assisted surgery by enhancing the autonomy and decision-making capabilities of these \\nsystems.  \\nThe technology of DRL is in its youth and still suffers from some limitations, e.g., low data \\nefficiency, expensive to train in the real world, and lack of safety guarantee. Looking forward, \\nfurther research is needed to refine and optimize DRL algorithms for surgical applications. This \\nincludes the following points: Firstly, more efficient training methodologies. Currently, most \\nDRL algorithms are very sample inefficient compared to other deep learning methods. \\nSecondly, a more accurate simulation environment. In surgical scenarios, there exist a lot of \\ndeformable bodies interaction, which are much harder to simulate, compared to rigid bodies \\ninteraction. Thirdly, addressing safety concerns. Safety is always the priority in surgeries. This \\ncould include risk analysis or interpretability of the model. Lastly, conducting clinical trials to \\nevaluate the effectiveness and reliability of DRL-based surgical robots because, so far, few DRL-\\nbased robots have been tested in real clinical scenarios. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n[1] Peters, Brian S., et al. \"Review of emerging surgical robotic technology.\" Surgical endoscopy 32 (2018): \\n1636-1655. \\n[2] Gomes, Paula, ed. Medical robotics: Minimally invasive surgery. Elsevier, 2012. \\n[3] Freschi, Cinzia, et al. \"Technical review of the da Vinci surgical telemanipulator.\" The International \\nJournal of Medical Robotics and Computer Assisted Surgery 9.4 (2013): 396-406. \\n[4] Moustris, George P., et al. \"Evolution of autonomous and semi‐autonomous robotic surgical systems: a \\nreview of the literature.\" The international journal of medical robotics and computer-assisted surgery 7.4 \\n(2011): 375-392. \\n[5] Ibarz, Julian, et al. \"(Dogangil, Davies et al. 2010, Moustris, Hiridis et al. 2011, Freschi, Ferrari et al. \\n2013, Peters, Armijo et al. 2018).\" The International Journal of Robotics Research 40.4-5 (2021): 698-721. \\n[6] Nguyen, Hai, and Hung La. \"Review of deep reinforcement learning for robot manipulation.\" 2019 \\nThird IEEE International Conference on Robotic Computing (IRC). IEEE, 2019. \\n[7] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint \\narXiv:1312.5602 (2013). \\n[8] Zhou, S. Kevin, et al. \"Deep reinforcement learning in medical imaging: A literature review.\" Medical \\nimage analysis 73 (2021): 102193. \\n[9] Yu, Chao, et al. \"Reinforcement learning in healthcare: A survey.\" ACM Computing Surveys \\n(CSUR) 55.1 (2021): 1-36. \\n[10] Coronato, Antonio, et al. \"Reinforcement learning for intelligent healthcare applications: A \\nsurvey.\" Artificial Intelligence in Medicine 109 (2020): 101964. \\n[11] Jonsson, Anders. \"Deep reinforcement learning in medicine.\" Kidney diseases 5.1 (2019): 18-22. \\n[12] Liu, Siqi, et al. \"Reinforcement learning for clinical decision support in critical care: comprehensive \\nreview.\" Journal of medical Internet research 22.7 (2020): e18477. \\n[13] Haidegger, Tamás. \"Autonomy for surgical robots: Concepts and paradigms.\" IEEE Transactions on \\nMedical Robotics and Bionics 1.2 (2019): 65-76. \\n[14] Arulkumaran, Kai, et al. \"Deep reinforcement learning: A brief survey.\" IEEE Signal Processing \\nMagazine 34.6 (2017): 26-38. \\n[15] Hua, Jiang, et al. \"Learning for a robot: Deep reinforcement learning, imitation learning, transfer \\nlearning.\" Sensors 21.4 (2021): 1278. \\n[16] Zhang, Tianhao, et al. \"Deep imitation learning for complex manipulation tasks from virtual reality \\nteleoperation.\" 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018. \\n[17] Sigaud, Olivier, and Olivier Buffet, eds. Markov decision processes in artificial intelligence. John \\nWiley & Sons, 2013. \\n[18] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018. \\n[19] Watkins, Christopher JCH, and Peter Dayan. \"Q-learning.\" Machine learning 8 (1992): 279-292. \\n[20] Kakade, Sham M. \"A natural policy gradient.\" Advances in neural information processing systems 14 \\n(2001). \\n[21] Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint \\narXiv:1707.06347 (2017). \\n[22] Schulman, John, et al. \"Trust region policy optimization.\" International conference on machine \\nlearning. PMLR, 2015. \\n[23] Yu, Yang. \"Towards Sample Efficient Reinforcement Learning.\" IJCAI. 2018. \\n[24] Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International \\nconference on machine learning. PMLR, 2016. \\n[25] Priester, Alan M., Shyam Natarajan, and Martin O. Culjat. \"Robotic ultrasound systems in \\nmedicine.\" IEEE transactions on ultrasonics, ferroelectrics, and frequency control 60.3 (2013): 507-523. \\n[26] Baumgartner, Christian F., et al. \"SonoNet: real-time detection and localisation of fetal standard scan \\nplanes in freehand ultrasound.\" IEEE transactions on medical imaging 36.11 (2017): 2204-2215. \\n[27] Virga, Salvatore, et al. \"Automatic force-compliant robotic ultrasound screening of abdominal aortic \\naneurysms.\" 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2016. \\n[28] Hase, Hannes, et al. \"Ultrasound-guided robotic navigation with deep reinforcement learning.\" 2020 \\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020. \\n[29] Spaan, Matthijs TJ. \"Partially observable Markov decision processes.\" Reinforcement learning: State-\\nof-the-art (2012): 387-414. \\n[30] Baisero, Andrea, and Christopher Amato. \"Learning complementary representations of the past using \\nauxiliary tasks in partially observable reinforcement learning.\" Proceedings of the 19th International \\nConference on Autonomous Agents and MultiAgent Systems. 2020. \\n[31] Li, Keyu, et al. \"Autonomous navigation of an ultrasound probe towards standard scan planes with \\ndeep reinforcement learning.\" 2021 IEEE International Conference on Robotics and Automation (ICRA). \\nIEEE, 2021. \\n[32] Karamalis, Athanasios, et al. \"Ultrasound confidence maps using random walks.\" Medical image \\nanalysis 16.6 (2012): 1101-1112. \\n[33] Li, Keyu, et al. \"Image-guided navigation of a robotic ultrasound probe for autonomous spinal \\nsonography using a shadow-aware dual-agent framework.\" IEEE Transactions on Medical Robotics and \\nBionics 4.1 (2021): 130-144. \\n[34] Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint arXiv:1511.05952 (2015). \\n[35] Van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep reinforcement learning with double q-\\nlearning.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 30. No. 1. 2016. \\n[36] Wang, Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\" International \\nconference on machine learning. PMLR, 2016. \\n[37] Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image \\nrecognition.\" arXiv preprint arXiv:1409.1556 (2014). \\n[38] Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" 2009 IEEE conference on \\ncomputer vision and pattern recognition. Ieee, 2009. \\n[39] Bi, Yuan, et al. \"VesNet-RL: Simulation-based reinforcement learning for real-world us probe \\nnavigation.\" IEEE Robotics and Automation Letters 7.3 (2022): 6638-6645. \\n[40] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for \\nbiomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention–\\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III \\n18. Springer International Publishing, 2015. \\n[41] Li, Keyu, et al. \"RL-TEE: Autonomous Probe Guidance for Transesophageal Echocardiography Based \\non Attention-Augmented Deep Reinforcement Learning.\" IEEE Transactions on Automation Science and \\nEngineering (2023). \\n[42] Milletari, Fausto, Vighnesh Birodkar, and Michal Sofka. \"Straight to the point: Reinforcement \\nlearning for user guidance in ultrasound.\" Smart Ultrasound Imaging and Perinatal, Preterm and \\nPaediatric Image Analysis: First International Workshop, SUSI 2019, and 4th International Workshop, \\nPIPPI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13 and 17, 2019, \\nProceedings 4. Springer International Publishing, 2019. \\n[43] Ning, Guochen, Xinran Zhang, and Hongen Liao. \"Autonomic robotic ultrasound imaging system \\nbased on reinforcement learning.\" IEEE Transactions on Biomedical Engineering 68.9 (2021): 2787-2797. \\n[44] Ning, Guochen, Xinran Zhang, and Hongen Liao. \"Autonomic robotic ultrasound imaging system \\nbased on reinforcement learning.\" IEEE Transactions on Biomedical Engineering 68.9 (2021): 2787-2797. \\n[45] Badrinarayanan, Vijay, Alex Kendall, and Roberto Cipolla. \"Segnet: A deep convolutional encoder-\\ndecoder architecture for image segmentation.\" IEEE transactions on pattern analysis and machine \\nintelligence 39.12 (2017): 2481-2495. \\n[46] Ning, Guochen, et al. \"Inverse-Reinforcement-Learning-Based Robotic Ultrasound Active \\nCompliance Control in Uncertain Environments.\" IEEE Transactions on Industrial Electronics (2023). \\n[47] Ng, Andrew Y., and Stuart Russell. \"Algorithms for inverse reinforcement learning.\" Icml. Vol. 1. \\n2000. \\n[48] Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint \\narXiv:1509.02971 (2015). \\n[49] Mahvash, Mohsen, et al. \"Modeling the forces of cutting with scissors.\" IEEE Transactions on \\nBiomedical Engineering 55.3 (2008): 848-856. \\n[50] Thananjeyan, Brijen, et al. \"Multilateral surgical pattern cutting in 2d orthotropic gauze with deep \\nreinforcement learning policies for tensioning.\" 2017 IEEE international conference on robotics and \\nautomation (ICRA). IEEE, 2017. \\n[51] Nguyen, Thanh, et al. \"A new tensioning method using deep reinforcement learning for surgical \\npattern cutting.\" 2019 IEEE international conference on industrial technology (ICIT). IEEE, 2019. \\n[52] Nguyen, Ngoc Duy, et al. \"Manipulating soft tissues by deep reinforcement learning for autonomous \\nrobotic surgery.\" 2019 IEEE International Systems Conference (SysCon). IEEE, 2019. \\n[53] Varier, Vignesh Manoj, et al. \"Collaborative suturing: A reinforcement learning approach to automate \\nhand-off task in suturing for surgical robots.\" 2020 29th IEEE international conference on robot and \\nhuman interactive communication (RO-MAN). IEEE, 2020. \\n[54] Müller, Meinard. \"Dynamic time warping.\" Information retrieval for music and motion (2007): 69-84. \\n[55] Chiu, Zih-Yun, et al. \"Bimanual regrasping for suture needles using reinforcement learning for rapid \\nmotion planning.\" 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021. \\n[56] Shahkoo, Amin Abbasi, and Ahmad Ali Abin. \"Deep reinforcement learning in continuous action \\nspace for autonomous robotic surgery.\" International Journal of Computer Assisted Radiology and \\nSurgery 18.3 (2023): 423-431. \\n[57] Patil, Sachin, and Ron Alterovitz. \"Toward automated tissue retraction in robot-assisted \\nsurgery.\" 2010 IEEE International Conference on Robotics and Automation. IEEE, 2010. \\n[58] Pore, Ameya, et al. \"Learning from demonstrations for autonomous soft-tissue retraction.\" 2021 \\nInternational Symposium on Medical Robotics (ISMR). IEEE, 2021. \\n[59] Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in neural \\ninformation processing systems 29 (2016). \\n[60] Pore, Ameya, et al. \"Safe reinforcement learning using formal verification for tissue retraction in \\nautonomous robotic-assisted surgery.\" 2021 IEEE/RSJ International Conference on Intelligent Robots and \\nSystems (IROS). IEEE, 2021. \\n[61] Corsi, Davide, et al. \"Formal verification for safe deep reinforcement learning in trajectory \\ngeneration.\" 2020 Fourth IEEE International Conference on Robotic Computing (IRC). IEEE, 2020. \\n[62] Scheikl, Paul Maria, et al. \"Sim-to-Real Transfer for Visual Reinforcement Learning of Deformable \\nObject Manipulation for Robot-Assisted Surgery.\" IEEE Robotics and Automation Letters 8.2 (2022): 560-\\n567. \\n[63] Wang, Mei, and Weihong Deng. \"Deep visual domain adaptation: A survey.\" Neurocomputing 312 \\n(2018): 135-153. \\n[64] H. Mayer et al., \"Automation of Manual Tasks for Minimally Invasive Surgery,\" Fourth International \\nConference on Autonomic and Autonomous Systems (ICAS\\'08), Gosier, France, 2008, pp. 260-265, doi: \\n10.1109/ICAS.2008.16. \\n[65] Reisch, Robert, et al. \"The keyhole concept in neurosurgery.\" World neurosurgery 79.2 (2013): S17-e9. \\n[66] Berry, Scott, and Kristin Ondecko Ligda. Ophthalmic surgery. Springer New York, 2015. \\n[67] Swaney, Philip J., et al. \"A flexure-based steerable needle: high curvature with reduced tissue \\ndamage.\" IEEE Transactions on Biomedical Engineering 60.4 (2012): 906-909. \\n[68] Lee, Yonggu, et al. \"Simulation of robot-assisted flexible needle insertion using deep Q-\\nnetwork.\" 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC). IEEE, 2019. \\n[69] Kumar, Jayesh, Chinmay Satish Raut, and Niravkumar Patel. \"Automated Flexible Needle Trajectory \\nPlanning for Keyhole Neurosurgery Using Reinforcement Learning.\" 2022 IEEE/RSJ International \\nConference on Intelligent Robots and Systems (IROS). IEEE, 2022. \\n[70] Tan, Xiaoyu, et al. \"Robot-assisted flexible needle insertion using universal distributional deep \\nreinforcement learning.\" International journal of computer assisted radiology and surgery 15 (2020): 341-\\n349. \\n[71] Tan, Xiaoyu, et al. \"Robust path planning for flexible needle insertion using Markov decision \\nprocesses.\" International Journal of Computer Assisted Radiology and Surgery 13 (2018): 1439-1451. \\n[72] Segato, Alice, et al. \"Inverse reinforcement learning intra-operative path planning for steerable \\nneedle.\" IEEE Transactions on Biomedical Engineering 69.6 (2021): 1995-2005. \\n[73] Keller, Brenton, et al. \"Optical coherence tomography-guided robotic ophthalmic microsurgery via \\nreinforcement learning from demonstration.\" IEEE Transactions on Robotics 36.4 (2020): 1207-1218. \\n[74] Gao, Huxin, et al. \"Remote-center-of-motion recommendation toward brain needle intervention \\nusing deep reinforcement learning.\" 2021 IEEE International Conference on Robotics and Automation \\n(ICRA). IEEE, 2021. \\n[75] Aksungur, Serhat. \"Remote center of motion (RCM) mechanisms for surgical \\noperations.\" International Journal of Applied Mathematics Electronics and Computers 3.2 (2015): 119-126. \\n[76] Chi, Wenqiang, et al. \"Trajectory optimization of robot-assisted endovascular catheterization with \\nreinforcement learning.\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems \\n(IROS). IEEE, 2018. \\n[77] Theodorou, Evangelos, Jonas Buchli, and Stefan Schaal. \"A generalized path integral control \\napproach to reinforcement learning.\" The Journal of Machine Learning Research 11 (2010): 3137-3181. \\n[78] Chi, Wenqiang, et al. \"Collaborative robot-assisted endovascular catheterization with generative \\nadversarial imitation learning.\" 2020 IEEE International conference on robotics and automation (ICRA). \\nIEEE, 2020. \\n[79] Segato, Alice, et al. \"Ga3c reinforcement learning for surgical steerable catheter path planning.\" 2020 \\nIEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020. \\n[80] Omisore, Olatunji Mumini, et al. \"A novel sample-efficient deep reinforcement learning with episodic \\npolicy transfer for pid-based control in cardiac catheterization robots.\" arXiv preprint \\narXiv:2110.14941 (2021). \\n[81] Page, Matthew J., et al. \"The PRISMA 2020 statement: an updated guideline for reporting systematic \\nreviews.\" International journal of surgery 88 (2021): 105906. \\n \\n \\n \\n \\n \\n'),\n",
       " Document(metadata={'Published': '2024-10-17', 'Title': 'MASQ: Multi-Agent Reinforcement Learning for Single Quadruped Robot Locomotion', 'Authors': 'Qi Liu, Jingxiang Guo, Sixu Lin, Shuaikang Ma, Jinxuan Zhu, Yanjie Li', 'Summary': 'This paper proposes a novel method to improve locomotion learning for a\\nsingle quadruped robot using multi-agent deep reinforcement learning (MARL).\\nMany existing methods use single-agent reinforcement learning for an individual\\nrobot or MARL for the cooperative task in multi-robot systems. Unlike existing\\nmethods, this paper proposes using MARL for the locomotion learning of a single\\nquadruped robot. We develop a learning structure called Multi-Agent\\nReinforcement Learning for Single Quadruped Robot Locomotion (MASQ),\\nconsidering each leg as an agent to explore the action space of the quadruped\\nrobot, sharing a global critic, and learning collaboratively. Experimental\\nresults indicate that MASQ not only speeds up learning convergence but also\\nenhances robustness in real-world settings, suggesting that applying MASQ to\\nsingle robots such as quadrupeds could surpass traditional single-robot\\nreinforcement learning approaches. Our study provides insightful guidance on\\nintegrating MARL with single-robot locomotion learning.'}, page_content='MASQ: Multi-Agent Reinforcement Learning for\\nSingle Quadruped Robot Locomotion\\nQi Liu†, Jingxiang Guo†, Sixu Lin, Shuaikang Ma, Jinxuan Zhu, Yanjie Li*\\nAbstract— This paper proposes a novel method to improve\\nlocomotion for a single quadruped robot using multi-agent\\ndeep reinforcement learning (MARL). Many existing methods\\nuse single-agent reinforcement learning for an individual robot\\nor MARL for the cooperative task in multi-robot systems.\\nUnlike existing methods, this paper proposes using MARL\\nfor the locomotion of a single quadruped robot. We pro-\\npose a learning structure called Multi-Agent Reinforcement\\nLearning for Single Quadruped Robot Locomotion (MASQ),\\nconsidering each leg as an agent to explore the action space\\nof the quadruped robot, sharing a global critic, and learning\\ncooperatively. Experimental results show that MASQ not only\\nspeeds up learning convergence but also enhances robustness in\\nreal-world settings, suggesting that applying MASQ to single\\nrobots such as quadrupeds could surpass traditional single-\\nrobot reinforcement learning approaches. Our study provides\\ninsightful guidance on integrating MARL with single-robot\\nlocomotion.\\nI. INTRODUCTION\\nReinforcement\\nlearning\\n(RL)\\nhas\\nmade\\nremarkable\\nprogress in various robot control learning [1], such as\\nquadruped robot [2], [3], biped robot [4], [5], and unmanned\\naerial vehicle [6]. This paper focuses on the quadruped robot\\ncontrol learning.\\nIn the process of applying deep RL to a single robot, it is\\nprevalent to use single-agent algorithms [7]–[10]. However,\\nsingle-agent algorithms may have limitations in managing\\ncoordination in specific problems. Many existing methods\\nuse single-agent RL algorithms for individual robot learning\\nor multi-agent deep reinforcement learning (MARL) for\\nmulti-robot systems in cooperative tasks [11]–[13]. Coop-\\nerative MARL algorithms have been widely demonstrated\\nin game-AI [14], [15] to have advantages in multi-agent\\ncooperation.\\nUnlike existing approaches, this paper proposes using\\nMARL for the locomotion of a single quadruped robot to\\nenhance cooperation between its four legs, thereby enabling\\nit to navigate complex terrains and perform intricate tasks.\\nBy proposing cooperative MARL, where each leg acts as an\\nagent, the quadruped robot can better coordinate its move-\\nments. This collaborative learning structure, termed Multi-\\nThis\\nwork\\nwas\\nsupported\\nby\\nthe\\nNational\\nNatural\\nScience\\nFoundation of China [61977019, U1813206] and Shenzhen Fundamental\\nResearch\\n[JCYJ20220818102415033,\\nJSGG20201103093802006,\\nKJZD20230923114222045].\\n(Corresponding\\nauthor:\\nYanjie\\nLi,\\nautolyj@hit.edu.cn)\\nThe authors are with the Guangdong Key Laboratory of Intelligent\\nMorphing Mechanisms and Adaptive Robotics and the School of Mechanical\\nEngineering and Automation, the Harbin Institute of Technology Shenzhen,\\n518055, China.\\n* Corresponding author.\\n† Equal contribution.\\n(a) Dog on grass\\n(b) Dog on rock\\n(c) Dog on flat\\n(d) Dog on rubber track\\nFig. 1: Quadruped robot on various terrains\\nAgent Reinforcement Learning for Single Quadruped Robot\\nLocomotion (MASQ), allows the robot to share experiences.\\nFig. 1 presents the deployment of the MASQ algorithm on\\na quadruped robot tested across different terrains, including\\ngrass, rocks, flat surfaces, and rubber tracks, demonstrating\\nits performance in diverse environments. Fig. 2 shows a\\nsim-to-real comparison of trot gaits in a quadruped robot,\\nhighlighting the consistency between simulated and real-\\nworld gait patterns.\\nThe main contributions of this paper can be summarized\\nas follows:\\n• This paper proposes MASQ, a method that treats each\\nleg of a quadruped robot as an individual agent. The\\nlocomotion learning is modeled as a cooperative multi-\\nagent reinforcement learning (MARL) problem and\\nsolved using a MARL algorithm.\\n• Experimental results show that the proposed method\\nenhances performance in executing gaits, improves\\ntraining efficiency and robustness, and achieves better\\nfinal performance, demonstrating the value and impact\\nof the approach.\\nII. RELATED WORK\\nA. Deep RL for Single Robot Control\\nRecent advances in deep RL for quadruped robots are\\ndriven by simulation technologies such as Isaac Gym [16],\\narXiv:2408.13759v2  [cs.RO]  17 Oct 2024\\n(a) Trot gaits in real-world\\n(b) Trot gaits in simulation\\nFig. 2: Sim-to-Real comparison of trot gaits\\n[17]. Hardware advancements have shown robust perfor-\\nmance on various tasks through Sim2Real transfer with zero\\nshot [18]–[21]. Current research focuses on task-specific\\nreward composition and training paradigms to bridge the\\nSim2Real gap [22]–[24], often using Proximal Policy Op-\\ntimization (PPO) [25] with an emphasis on task and reward\\ndesign rather than novel RL algorithms. Although some stud-\\nies have explored learning algorithms to improve efficiency\\n[26]–[31], most efforts remain centered on efficiency rather\\nthan modifying algorithms to control the characteristics\\nof the object. Additionally, research has delved into task-\\nspecific strategies like rapid motor adaptation (RMA) [22],\\nhierarchical RL for multi-skill tasks [32], and symmetry-\\nbased data enhancement [33], [34], yet challenges remain\\nin using robot symmetry through single-agent methods,\\nindicating that algorithmic research on this aspect is still\\nunderdeveloped.\\nB. MARL for Multi-robot Control\\nIn multi-agent settings, algorithms like Multi-Agent Prox-\\nimal Policy Optimization (MAPPO) [35], Temporally Ex-\\ntended Multi-Agent Reinforcement Learning (TEMP) [36]\\nhave demonstrated strong capabilities in addressing multi-\\nagent robotic challenges, such as drone fleet control [37],\\n[38] and autonomous vehicle fleets [39], [40]. Methods like\\nReinforced Inter-Agent Learning (RIAL) and Differentiable\\nInter-Agent Learning (DIAL) [41] further enhance collab-\\norative performance by developing communication proto-\\ncols among agents. This paper proposes modeling single-\\nquadruped robot locomotion as a cooperative MARL prob-\\nlem, where each leg is treated as an independent agent,\\ncontrasting with previous approaches that treat the robot as a\\nmonolithic entity [42]–[45] or cooperative groups of multi-\\nrobots [12], [46], [47].\\nIII. PRELIMINARIES\\nThis paper considers a finite-horizon Markov decision\\nprocess (MDP) [48], defined by a tuple (S, A, P, r, γ, T). S\\ndenotes the state space, A := {a0, a1, ..., a|A|−1} represents\\na finite action space, P : S × A × S →[0, 1] represents\\nthe staåte transition distribution, r : S × A →R denotes a\\nreward function, γ ∈[0, 1) denotes a discount factor, and T\\nis a time horizon. At each time step t, the policy π selects an\\naction at ∈A. After entering the next state by sampling from\\nP (st+1 | st, at), the agent receives an immediate reward\\nr (st, at). The agent continues to perform actions until it\\nenters a terminal state or t reaches the time horizon T. RL\\naims to learn a policy π : S×A →[0, 1] for decision-making\\nproblems by maximizing discounted rewards. For any policy\\nπ, the state-action value function (Q function) is defined as\\nQπ(s, a) = Eπ\\n\" T\\nX\\nt=0\\nγtr (st, at) | s0 = s, a0 = a\\n#\\n(1)\\nProximal Policy Optimization (PPO) [25] enhances the\\nstability and performance of policy gradient methods by\\nlimiting policy updates to prevent destabilizing deviations.\\nThe core PPO update rule optimizes a clipped surrogate\\nfunction:\\nLCLIP(θ) = Et\\nh\\nmin\\n\\x10\\nrt(θ) ˆAt, clip(rt(θ), 1 −ϵ, 1 + ϵ) ˆAt\\n\\x11i\\n(2)\\nwhere\\nrt(θ) = πθ(at|st)\\nπθold(at|st)\\n(3)\\nand\\nˆAt = Qπ(st, at) −Vψ(st)\\n(4)\\nwhere ψ denotes the parameters of value function (Vψ)\\nnetwork, ϵ denotes a coefficient. Policy parameters θ are\\nupdated as:\\nθ ←θ + α∇θLCLIP(θ)\\n(5)\\nPPO’s constrained updates stabilize training and improve\\nperformance, making it practical for complex single-agent\\nRL tasks.\\nIV. MULTI-AGENT REINFORCEMENT LEARNING FOR\\nSINGLE QUADRUPED ROBOT LOCOMOTION\\nIn this paper, we use the collaborative potential of multiple\\nagents to improve the learning process for a single robot’s\\nlocomotion, resulting in faster training convergence, robust-\\nness and better final performance in real-world environments.\\nSpecifically, each leg of the quadruped robot is treated as a\\nseparate agent within the multi-agent structure, with indi-\\nvidual observations and a shared global critic, significantly\\nimproving the cooperation among the robot’s limbs for more\\neffective locomotion.\\nFig. 3: The framework of MASQ\\nA. MASQ Modeling\\nThis paper models a single quadruped robot locomotion\\nas a cooperative multi-agent problem, which is described as\\na partially observable decentralized Markov decision process\\n(decPOMDP) [49]. The decPOMDP is defined by the tuple\\nG = (S, A, P, r, Z, O, N, γ, T). S is the state space, A\\nis the action space, P is the state transition distribution,\\nr is the reward function, Z is the observation space, O\\nis the observation function, N is the number of agents, γ\\nis the discount factor, and T is the time horizon. At each\\ntime step t, each agent n ∈{1, . . . , N} selects an action\\nan\\nt ∈A, resulting in a joint action at = {a1\\nt, a2\\nt, . . . , aN\\nt }.\\nThe environment transitions to a new state st+1 according\\nto P(st+1|st, at) and provides a shared reward r(st, at).\\nEach agent receives an observation zn\\nt from O(st, n) and\\nmaintains an observation-action history τ n\\nt . MARL aims to\\nlearn policies {πn}N\\nn=1 that maximize expected cumulative\\nrewards:\\nJ(π) = E\\n\" T\\nX\\nt=0\\nγtr(st, at)\\n#\\n(6)\\nThis paper proposes Multi-Agent Reinforcement Learning\\nfor Single Quadruped Robot Locomotion (MASQ), which\\napplies MARL principles to treat different parts of a single\\nquadruped robot as independent agents, trained collabora-\\ntively using shared rewards. Specifically, this paper uses\\nMAPPO [35] to sovle the modeled multi-agent problem.\\nMAPPO optimizes the following objective function in a\\nmulti-agent context:\\nLCLIP\\nMAPPO(θ) =\\nn\\nX\\ni=1\\nEt\\nh\\nmin\\n\\x10\\nri\\nt(θi) ˆAt,\\nclip(ri\\nt(θi), 1 −ϵ, 1 + ϵ) ˆAt\\n\\x11i\\n(7)\\nwhere\\nri\\nt(θi) =\\nπθi(ai\\nt|oi\\nt)\\nπθi,old(ai\\nt|oi\\nt)\\n(8)\\nwhere i denote the i-th agent in MARL. Each agent updates\\nits policy parameters as follows:\\nθi ←θi + α∇θiLCLIP\\ni\\n(θi)\\n(9)\\nThis paper uses centralized training with decentralized\\nexecution (CTDE) [50] to handle multi-agent learning chal-\\nlenges. This approach maintains stability in a constantly\\nchanging environment. In implementations, separate neural\\nnetworks learn a policy (πθ) and a value function (Vψ(s))\\nsimilar to the single-agent version. The value function helps\\nreduce training variability and can take in additional global\\ninformation. This approach leads to better final performance,\\nfaster learning speed, and improved robustness in deploy-\\nment.\\nB. MASQ Settings\\nState Space and Observation: The shared actor network\\ntakes a concatenated observation from four agents, each with\\n35 dimensions, including motor positions qt ∈R3, motor\\nspeeds ˙qt ∈R3, previous actions at−1 ∈R3 and at ∈R3, a\\ngait sequencing director dt ∈R1, projected gravity gt ∈R3,\\ncommand values vcmd\\nt\\n∈R15, body speeds vb ∈R3, and\\na one-hot encoding et ∈R1. The total input for the actor\\nnetwork is oactor\\nt\\n∈R140 (35x4). The architecture of the\\nactor network consists of a normalization layer, followed\\nby an MLP layer, a GRU layer, and another normalization\\nlayer, with the final output being the joint angle commands\\nfor each leg. Additionally, a bias is applied to the output\\nfor better precision in control. On the other hand, the critic\\nnetwork uses global observations and has a 73-dimensional\\ninput, including motor positions qt ∈R12, motor speeds\\n˙qt ∈R12, previous actions at−1 ∈R12 and at ∈R12, gait\\ndirectors dt ∈R4, projected gravity gt ∈R3, command\\nvalues vcmd\\nt\\n∈R15, and body speeds vb ∈R3. The output of\\nthe critic network consists of continuous V values Vt ∈R4,\\nwhich are used to calculate the advantage function.\\nAction Space: The output of the actor-network consists\\nof continuous actions at ∈R12, and the system then uses\\nthese to calculate the torques for the 12 motors. Details can\\nbe found in Section IV-D.\\nReward Function: The reward functions in Table I are\\ndesigned to optimize the robot’s performance by encourag-\\ning desired behaviors and penalizing undesired ones. Key\\nrewards include: tracking linear velocity, which uses an\\nexponential decay function to minimize velocity error; linear\\nvelocity Z and angular velocity XY, both penalizing unwanted\\nmotions to ensure stability; torques, DOF velocity, and DOF\\nacceleration, which promote energy efficiency and smoother\\nmovements; and collision, which penalizes excessive contact\\nforces. Additional rewards focus on action smoothness, ac-\\ncurate jumping, minimizing foot slip and impact velocities,\\nand enhancing locomotion stability using Raibert’s heuristic\\n[51] and foot velocity tracking.\\nC. Multi-agent Actor and Global Critic Networks\\nIn the simulation environment of Isaac Gym [16], the robot\\nreceives observations and rewards to facilitate its learning.\\nThe learning process involves dividing the observations into\\nshared and private features, and both the actor and critic\\nnetworks use the rewards to train the policy.\\nTABLE I: Reward function\\nREWARD SETTINGS, CORRESPONDING EQUATIONS, AND THEIR SCALES\\nReward Term\\nEquation\\nScale\\nTracking Linear Velocity\\nexp\\n\\x10\\n−∥vcmd−vb∥2\\nσt\\n\\x11\\n1.0\\nTracking Angular Velocity\\nexp\\n\\x12\\n−(ωcmd,z−ωb,z)2\\nσyaw\\n\\x13\\n0.5\\nLinear Velocity Z\\n∥vb,z∥2\\n−2 × 10−2\\nAngular Velocity XY\\nP\\ni ∥ωb∥2\\n−1 × 10−3\\nAngular Velocity Torques\\nP\\ni ∥τi∥2\\n−1 × 10−5\\nDOF Velocity\\nP\\ni ∥vd,i∥2\\n−1 × 10−4\\nDOF Acceleration\\nP\\ni\\n\\x10 vd,i,t−vd,i,t−1\\n∆t\\n\\x112\\n−2.5 × 10−7\\nCollision\\nP (1.0 · (∥fc∥> 0.1))\\n-5.0\\nAction Rate\\nP ∥at −at−1∥2\\n−1 × 10−2\\nJump\\n−(hb −hj)2\\n10.0\\nFeet Slip\\nP(cf · ∥vf∥2)\\n−4 × 10−2\\nAction Smoothness 1\\nP (at −at−1)2\\n-0.1\\nAction Smoothness 2\\nP (at −2at−1 + at−2)2\\n-0.1\\nFeet Impact Velocity\\nP(cs · ∥vf,p∥2)\\n-0.0\\nRaibert Heuristic\\nP ∥er∥2\\n-10.0\\nTracking Contacts Shaped Velocity\\nP \\x12\\ncd ·\\n\\x12\\n1 −exp\\n\\x12\\n−\\n∥vf ∥2\\nσgv\\n\\x13\\x13\\x13\\n4.0\\nThe actor-network consists of a multi-layer perception\\n(MLP) base and an activation layer that produces actions\\nand their associated log probabilities. Similarly, the critic\\nnetwork uses an MLP base, ending in an output layer that\\npredicts value functions. The actor-network outputs actions\\nbased on observations, while the critic network assesses\\nthe value of these actions to guide the learning process.\\nThe actions produced for the agents are processed by an\\nactuator network [52] to simulate real-world conditions,\\nenhancing deployment effectiveness in natural environments.\\nAfter training, the trained actor-network is deployed onto the\\nrobotic dog to perform actions directly. Fig. 3 illustrates the\\nentire learning process.\\nIn the context of a quadruped robot, we consider each\\nleg as an individual agent. All four agents share a standard\\nactor-network. Using a shared-parameter network instead of\\nfour separate actor networks helps reduce computational load\\nand better fits the nature of a quadruped robot. Unlike typical\\nmulti-agent environments, such as StarCraft [14], where each\\nsoldier is an independent agent, the quadruped robot is a sin-\\ngle entity with four legs symmetrically positioned around the\\nbody’s center. Therefore, a shared-parameter actor-network\\nis more suitable for this scenario.\\nWe express the policy for each leg as follows:\\nπθi(ai,t | si,t) = πθ(ai,t | si,t)\\n(10)\\nwhere i = 1, 2, 3, 4 corresponds to the four legs.\\nThe quadruped robot has four legs, each with three motors:\\nhip, thigh, and calf joint motors. Each motor’s position,\\nvelocity, and torque are observable, so we use these details\\nas the independent observations for each agent. Additionally,\\nto enhance the coordination among the agents, we augment\\neach independent observation with shared observations, in-\\ncluding speed and gravity projections calculated from inertial\\nmeasurement unit (IMU) data, temporal director, and agent\\nidentifier (ID). The temporal director Ti(t) guides the gait\\nsequence of each leg under different movement postures,\\nwhile the agent ID is necessary for the shared-parameter\\n(a) Robustness in outside disturbing\\n(b) Contact force in different legs while encountering force\\nFig. 4: Robustness test\\nnetwork. This setup ensures the independence of each agent\\nwhile improving their cooperative capabilities. The temporal\\ndirector helps to synchronize the movements of different legs,\\nensuring smooth gait patterns. It can be defined as:\\nTi(t) = sin (2π(kt + ∆i))\\n(11)\\nwhere\\n• k is the scaling factor of scaling factor of gait cycle.\\n• ∆i is the phase offset for the i-th leg, which determines\\nits relative timing within the gait cycle to ensure coor-\\ndinated movement.\\nThe Global Critic uses a centralized value function ap-\\nproach to consider global information, which fits into the\\nCTDE. It relies on a global value function to coordinate\\nindividual agents, such as single-agent algorithms like PPO.\\nThe Critic network takes in global observations, which\\nconsist of the separate observations of the four agents and\\nshared global information.\\nDuring the training in the simulation environment, we\\nused a multi-gait curriculum learning [53]. This curriculum\\ncomprises four gaits: pace, trot, bound, and pronk. In the\\nsimulation, the quadruped robot learns these four gaits simul-\\ntaneously, and the progress is updated based on evaluating\\nwhether the gaits’ rewards meet specific thresholds. This\\nmethod allows the robot to learn different gaits effectively\\nand is also helpful in testing the generalization capabilities of\\nour proposed approach across various tasks in experimental\\nsettings.\\nD. Sim-to-real\\nTo bridge the gap between simulation and real-world per-\\nformance, we used domain randomization and an Actuator\\nNetwork [52]. Domain randomization involves randomizing\\nvarious parameters to train a robust policy under different\\nconditions [30], [54], [55]. These parameters include robot\\nbody mass, motor strength, joint position calibration, ground\\nfriction, restitution, orientation, and magnitude of gravity.\\nWe also independently randomize friction coefficients like\\nground friction. Gravity is randomized every 8 seconds with\\na gravity impulse duration of 0.99 seconds. Time steps are\\nrandomized every 6 seconds, with the overall randomization\\noccurring every 4 seconds. These measures enhance the\\nmodel’s robustness and adaptability. The training process\\nfor the Actuator Network captures the non-ideal relationship\\nbetween PD error and the torque realized [52], thereby im-\\nproving the model’s performance in real-world applications.\\nV. EXPERIMENTS\\nWe conducted experiments using the Unitree Go2 quadru-\\npled robot with various experiments. Section V-A proposes\\nthe experimental setup. Section V-B shows the experimental\\nresults of the simulation. Section V-C shows the real-world\\nexperiments and comparisons.\\n(a) Flat terrain\\n(b) Uneven terrain\\nFig. 5: Various terrains\\nA. Experiments Setup\\nIn the simulation environment, we designed two types of\\nterrain: flat and uneven, shown in Fig. 5. Uneven terrain is\\na composite of pyramid-sloping terrain and random uniform\\nterrain. Fifteen commands, such as linear velocity, angular\\nvelocity and height of the base, sampled within a specified\\nrange, guide the gait-learning process. The robot learns to\\nadapt and develop various skills by following these com-\\nmands.\\nB. Simulation Experiments\\nThe MASQ was compared with the PPO student-teacher\\nbaseline in simulation environments, including two types:\\nflat and uneven terrain. The quadruped robot was trained\\nusing the curriculum learning method, with hyperparameters\\nsuch as the learning rate tuned for optimal performance\\nin both scenarios. Reward curves in Fig. 6 show that the\\nproposed method achieves faster convergence and better final\\nperformance compared to the PPO baseline. Experimental\\nresults are presented as the mean of rewards over five tests\\nfor a fair comparison.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nEnvironment Steps\\n1e8\\n0\\n2\\n4\\n6\\n8\\nMean Reward\\nMean Rewards over Time (Flat Terrain)\\nRewards of MASQ\\nRewards of PPO\\nSmoothed Rewards of MASQ\\nSmoothed Rewards of PPO\\n(a) Reward over time (flat terrain)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nEnvironment Steps\\n1e8\\n0\\n2\\n4\\n6\\n8\\nMean Reward\\nMean Rewards over Time (Uneven Terrain)\\nRewards of MASQ\\nRewards of PPO\\nSmoothed Rewards of MASQ\\nSmoothed Rewards of PPO\\n(b) Reward over time (uneven terrain)\\nFig. 6: Return on various terrains\\nC. Real-word Experiments\\nThis paper deploys MASQ on a quadruped robot and tests\\nit on various terrains, including flat ground, grass, and sand\\nin Fig. 1, where it performs excellently. In addition, we\\nconducted a series of robustness tests for heavy impacts and\\nside kicks. The robot can quickly return to its normal state\\nafter being disturbed.\\n1) Gaits test: Fig. 7 illustrates the periodic relationship\\nof quadruped force feedback for each of the four legs under\\nthe training of four different skills. They reflect the impact\\nof incorporating the temporal director in our observations on\\nthe learning and switching of other skills.\\n2) Robustness test: To validate the robustness of the gaits\\ntrained with our method, we conduct external disturbance\\ntests on the robot. We test the robot in a bounding gait\\nwith a cycle period of 20ms. During robustness tests, the\\nrobot performs continuous jumps in this bounding gait.\\nThe impacts of human steps on the robot are applied to\\npropose disturbances during its jump cycle. Under normal\\nconditions, the robot landed simultaneously on all four legs.\\nWe monitored this process by recording the force sensor\\nvalues in the robot’s feet, thereby documenting the transition\\nfrom a normal state through the induced disturbance and back\\nto the normal state. As shown in Fig. 4, the robot returns to\\nits normal state after experiencing a disturbance within just\\none gait cycle.\\nVI. CONCLUSION AND FUTURE WORK\\nThis paper proposed a multi-agent-based motion control\\nsystem for quadruped robots, utilizing a shared-parameter\\nFig. 7: Contact force of four legs in different gaits\\nactor network and a centralized critic network within the\\nCTDE framework. The proposed approach, implemented\\nin Isaac Gym, demonstrated substantial improvements in\\ntraining speed, robustness, and final performance, benefiting\\nfrom curriculum learning and domain randomization. These\\nadvances enabled efficient limb coordination and smoother\\nsim-to-real transitions. Experimental results confirmed the\\neffectiveness of the method in enhancing both performance\\nand efficiency in motion control for symmetric robots. Future\\nwork will extend the approach to other symmetric robots and\\nexplore its application in more complex dynamic environ-\\nments.\\nREFERENCES\\n[1] Y. Hou, H. Sun, J. Ma, and F. Wu, “Improving offline reinforcement\\nlearning with inaccurate simulators,” in 2024 IEEE International\\nConference on Robotics and Automation (ICRA), 2024, pp. 5162–\\n5168.\\n[2] G. B. Margolis and P. Agrawal, “Walk these ways: Tuning robot\\ncontrol for generalization with multiplicity of behavior,” in Conference\\non Robot Learning.\\nPMLR, 2023, pp. 22–31.\\n[3] I. M. Aswin Nahrendra, B. Yu, and H. Myung, “Dreamwaq: Learning\\nrobust quadrupedal locomotion with implicit terrain imagination via\\ndeep reinforcement learning,” in 2023 IEEE International Conference\\non Robotics and Automation (ICRA), 2023, pp. 5078–5084.\\n[4] H. Benbrahim and J. A. Franklin, “Biped dynamic walking using\\nreinforcement learning,” Robotics and Autonomous systems, vol. 22,\\nno. 3-4, pp. 283–302, 1997.\\n[5] H. Duan, B. Pandit, M. S. Gadde, B. Van Marum, J. Dao, C. Kim, and\\nA. Fern, “Learning vision-based bipedal locomotion for challenging\\nterrain,” in 2024 IEEE International Conference on Robotics and\\nAutomation (ICRA), 2024, pp. 56–62.\\n[6] H. Lu, Y. Li, S. Mu, D. Wang, H. Kim, and S. Serikawa, “Motor\\nanomaly detection for unmanned aerial vehicles using reinforcement\\nlearning,” IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2315–\\n2322, 2018.\\n[7] B. Jia and D. Manocha, “Sim-to-real robotic sketching using behavior\\ncloning and reinforcement learning,” in 2024 IEEE International\\nConference on Robotics and Automation (ICRA), 2024, pp. 18 272–\\n18 278.\\n[8] A. Lobbezoo, Y. Qian, and H.-J. Kwon, “Reinforcement learning for\\npick and place operations in robotics: A survey,” Robotics, vol. 10,\\nno. 3, p. 105, 2021.\\n[9] H. Jiang, T. Chen, J. Cao, J. Bi, G. Lu, G. Zhang, X. Rong, and\\nY. Li, “Sim-to-real: Quadruped robot control with deep reinforcement\\nlearning and parallel training,” in 2022 IEEE International Conference\\non Robotics and Biomimetics (ROBIO), 2022, pp. 489–494.\\n[10] S. Lyu, H. Zhao, and D. Wang, “A composite control strategy for\\nquadruped robot by integrating reinforcement learning and model-\\nbased control,” in 2023 IEEE/RSJ International Conference on In-\\ntelligent Robots and Systems (IROS), 2023, pp. 751–758.\\n[11] L. Canese, G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino,\\nM. Re, and S. Spanò, “Multi-agent reinforcement learning: A review\\nof challenges and applications,” Applied Sciences, vol. 11, no. 11, p.\\n4948, 2021.\\n[12] J. Orr and A. Dutta, “Multi-agent deep reinforcement learning for\\nmulti-robot applications: A survey,” Sensors, vol. 23, no. 7, p. 3625,\\n2023.\\n[13] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer,\\nH. Dong, S.-C. Zhu, and Y. Yang, “Towards human-level bimanual\\ndexterous manipulation with reinforcement learning,” in Advances in\\nNeural Information Processing Systems, vol. 35, 2022, pp. 5150–5163.\\n[14] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and\\nS. Whiteson, “Monotonic value function factorisation for deep multi-\\nagent reinforcement learning,” Journal of Machine Learning Research,\\nvol. 21, no. 178, pp. 1–51, 2020.\\n[15] D. Ye, G. Chen, P. Zhao, F. Qiu, B. Yuan, W. Zhang, S. Chen, M. Sun,\\nX. Li, S. Li, J. Liang, Z. Lian, B. Shi, L. Wang, T. Shi, Q. Fu, W. Yang,\\nand L. Huang, “Supervised learning achieves human-level performance\\nin moba games: A case study of honor of kings,” IEEE Transactions on\\nNeural Networks and Learning Systems, vol. 33, no. 3, pp. 908–918,\\n2022.\\n[16] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Mack-\\nlin, D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., “Isaac gym:\\nHigh performance gpu based physics simulation for robot learning,”\\nin Thirty-fifth Conference on Neural Information Processing Systems\\nDatasets and Benchmarks Track (Round 2), 2021.\\n[17] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk\\nin minutes using massively parallel deep reinforcement learning,” in\\nConference on Robot Learning.\\nPMLR, 2022, pp. 91–100.\\n[18] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,\\nV. Koltun, and M. Hutter, “Learning agile and dynamic motor skills\\nfor legged robots,” Science Robotics, vol. 4, no. 26, Jan. 2019.\\n[19] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,\\n“Learning quadrupedal locomotion over challenging terrain,” Science\\nRobotics, vol. 5, no. 47, p. eabc5986, 2020.\\n[20] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,\\n“Learning robust perceptive locomotion for quadrupedal robots in the\\nwild,” Science Robotics, vol. 7, no. 62, p. eabk2822, 2022.\\n[21] S. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, and I. Havoutis,\\n“Real-time trajectory adaptation for quadrupedal locomotion using\\ndeep reinforcement learning,” in 2021 IEEE International Conference\\non Robotics and Automation (ICRA).\\nIEEE, 2021, pp. 5973–5979.\\n[22] A. Kumar, Z. Fu, D. Pathak, and J. Malik, “Rma: Rapid motor\\nadaptation for legged robots,” Robotics: Science and Systems XVII,\\n2021.\\n[23] A. Kumar, Z. Li, J. Zeng, D. Pathak, K. Sreenath, and J. Malik, “Adapt-\\ning rapid motor adaptation for bipedal robots,” in 2022 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems (IROS),\\n2022, pp. 1161–1168.\\n[24] S. Choi, G. Ji, J. Park, H. Kim, J. Mun, J. H. Lee, and J. Hwangbo,\\n“Learning quadrupedal locomotion on deformable terrain,” Science\\nRobotics, vol. 8, no. 74, p. eade2256, 2023.\\n[25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\\n“Proximal\\npolicy\\noptimization\\nalgorithms,”\\narXiv\\npreprint\\narXiv:1707.06347, 2017.\\n[26] Y. Kim, H. Oh, J. Lee, J. Choi, G. Ji, M. Jung, D. Youm, and\\nJ. Hwangbo, “Not only rewards but also constraints: Applications on\\nlegged robot locomotion,” IEEE Transactions on Robotics, 2024.\\n[27] L. Ye, J. Li, Y. Cheng, X. Wang, B. Liang, and Y. Peng, “From\\nknowing to doing: Learning diverse motor skills through instruction\\nlearning,” arXiv preprint arXiv:2309.09167, 2023.\\n[28] B. L. Semage, T. G. Karimpanal, S. Rana, and S. Venkatesh,\\n“Zero-shot sim2real adaptation across environments,” arXiv preprint\\narXiv:2302.04013, 2023.\\n[29] X. Gu, Y.-J. Wang, and J. Chen, “Humanoid-gym: Reinforcement\\nlearning for humanoid robot with zero-shot sim2real transfer,” arXiv\\npreprint arXiv:2404.05695, 2024.\\n[30] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to-\\nreal transfer of robotic control with dynamics randomization,” in 2018\\nIEEE International Conference on Robotics and Automation (ICRA).\\nIEEE, May 2018.\\n[31] B. Wang, Z. Liu, Q. Li, and A. Prorok, “Mobile robot path plan-\\nning in dynamic environments through globally guided reinforcement\\nlearning,” IEEE Robotics and Automation Letters, vol. 5, no. 4, pp.\\n6932–6939, 2020.\\n[32] X. Zhou, X. Wen, Z. Wang, Y. Gao, H. Li, Q. Wang, T. Yang, H. Lu,\\nY. Cao, C. Xu, and F. Gao, “Swarm of micro flying robots in the\\nwild,” Science Robotics, vol. 7, 2022.\\n[33] C. Zhang, N. Rudin, D. Hoeller, and M. Hutter, “Learning agile\\nlocomotion on risky terrains,” arXiv preprint arXiv:2311.10484, 2023.\\n[34] M. Mittal, N. Rudin, V. Klemm, A. Allshire, and M. Hutter, “Symme-\\ntry considerations for learning task symmetric robot policies,” in 2024\\nIEEE International Conference on Robotics and Automation (ICRA).\\nIEEE, 2024.\\n[35] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and\\nY. Wu, “The surprising effectiveness of ppo in cooperative multi-\\nagent games,” in Advances in Neural Information Processing Systems,\\nvol. 35, 2022, pp. 24 611–24 624.\\n[36] M. C. Machado, A. Barreto, D. Precup, and M. Bowling, “Temporal\\nabstraction in reinforcement learning with the successor representa-\\ntion,” Journal of Machine Learning Research, vol. 24, no. 80, pp.\\n1–69, 2023.\\n[37] Y. Alon and H. Zhou, “Multi-agent reinforcement learning for un-\\nmanned aerial vehicle coordination by multi-critic policy gradient\\noptimization,” IEEE Internet of Things Journal, 2020.\\n[38] T. Jacob, D. Duran, T. Pfeiffer, M. Vignati, and M. Johnson, “Multi-\\nagent reinforcement learning for unmanned aerial vehicle capture-the-\\nflag game behavior,” in Intelligent Systems Conference.\\nSpringer,\\n2023, pp. 174–186.\\n[39] S.\\nSainz-Palacios,\\n“Deep\\nreinforcement\\nlearning\\nfor\\nshared\\nautonomous\\nvehicles\\n(sav)\\nfleet\\nmanagement,”\\narXiv\\npreprint\\narXiv:2201.05720, 2022.\\n[40] C. Schmidt, D. Gammelli, F. C. Pereira, and F. Rodrigues, “Learning to\\ncontrol autonomous fleets from observation via offline reinforcement\\nlearning,” in 2024 European Control Conference (ECC). IEEE, 2024,\\npp. 1399–1406.\\n[41] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning\\nto communicate with deep multi-agent reinforcement learning,” in\\nAdvances in Neural Information Processing Systems, vol. 29, 2016.\\n[42] T. R. X, “Lifelike agility and play on quadrupedal robots using\\nreinforcement learning and generative pre-trained models,” Nature\\nMachine Intelligence, 2023.\\n[43] Y. Wang, R. Sagawa, and Y. Yoshiyasu, “Learning advanced loco-\\nmotion for quadrupedal robots: A distributed multi-agent reinforce-\\nment learning framework with riemannian motion policies,” Robotics,\\nvol. 13, no. 6, p. 86, 2024.\\n[44] X. Han and M. Zhao, “Learning quadrupedal high-speed running on\\nuneven terrain,” Biomimetics, vol. 9, no. 1, p. 37, 2024.\\n[45] A. names not provided, “Deep reinforcement learning for real-world\\nquadrupedal locomotion: a comprehensive review,” OAEPublish, 2023.\\n[46] X. Lan, Y. Qiao, and B. Lee, “Towards pick and place multi robot\\ncoordination using multi-agent deep reinforcement learning,” in 2021\\n7th International Conference on Automation, Robotics and Applica-\\ntions (ICARA).\\nIEEE, 2021, pp. 85–89.\\n[47] Y. Liu, Z. Cao, H. Xiong, J. Du, H. Cao, and L. Zhang, “Dynamic\\nobstacle avoidance for cable-driven parallel robots with mobile bases\\nvia sim-to-real reinforcement learning,” IEEE Robotics and Automa-\\ntion Letters, vol. 8, no. 3, pp. 1683–1690, 2023.\\n[48] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\\nMIT press, 2018.\\n[49] S. C. Ong, S. W. Png, D. Hsu, and W. S. Lee, “Pomdps for robotic\\ntasks with mixed observability.” in Robotics: Science and Systems,\\nvol. 5, 2009, p. 4.\\n[50] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli,\\nand S. Whiteson, “Stabilising experience replay for deep multi-agent\\nreinforcement learning,” in Proceedings of the 34th International\\nConference on Machine Learning-Volume 70, 2017, pp. 1146–1155.\\n[51] M. H. Raibert and E. R. Tello, “Legged robots that balance,” IEEE\\nExpert, vol. 1, no. 4, pp. 89–89, 1986.\\n[52] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,\\nV. Koltun, and M. Hutter, “Learning agile and dynamic motor skills\\nfor legged robots,” Science Robotics, vol. 4, no. 26, p. eaau5872, 2019.\\n[53] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum\\nlearning,” in Proceedings of the 26th Annual International Conference\\non Machine Learning, ser. ICML ’09.\\nNew York, NY, USA:\\nAssociation for Computing Machinery, 2009, p. 41–48.\\n[54] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan,\\nJ. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, “Sim-to-real via sim-\\nto-sim: Data-efficient robotic grasping via randomized-to-canonical\\nadaptation networks,” in 2019 IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), 2019, pp. 12 619–12 629.\\n[55] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,\\n“Domain randomization for transferring deep neural networks from\\nsimulation to the real world,” in 2017 IEEE/RSJ International Con-\\nference on Intelligent Robots and Systems (IROS), 2017, pp. 23–30.\\n'),\n",
       " Document(metadata={'Published': '2023-01-30', 'Title': 'Guided Deep Reinforcement Learning for Articulated Swimming Robots', 'Authors': 'Jiaheng Hu, Tony Dear', 'Summary': 'Deep reinforcement learning has recently been applied to a variety of\\nrobotics applications, but learning locomotion for robots with unconventional\\nconfigurations is still limited. Prior work has shown that, despite the simple\\nmodeling of articulated swimmer robots, such systems struggle to find effective\\ngaits using reinforcement learning due to the heterogeneity of the search\\nspace. In this work, we leverage insight from geometric models of these robots\\nin order to focus on promising regions of the space and guide the learning\\nprocess. We demonstrate that our augmented learning technique is able to\\nproduce gaits for different learning goals for swimmer robots in both low and\\nhigh Reynolds number fluids.'}, page_content='Guided Deep Reinforcement Learning for Articulated Swimming Robots\\nJiaheng Hu1 and Tony Dear1\\nAbstract— Deep reinforcement learning has recently been\\napplied to a variety of robotics applications, but learning\\nlocomotion for robots with unconventional conﬁgurations is still\\nlimited. Prior work has shown that, despite the simple modeling\\nof articulated swimmer robots, such systems struggle to ﬁnd\\neffective gaits using reinforcement learning due to the hetero-\\ngeneity of the search space. In this work, we leverage insight\\nfrom geometric models of these robots in order to focus on\\npromising regions of the space and guide the learning process.\\nWe demonstrate that our augmented learning technique is able\\nto produce gaits for different learning goals for swimmer robots\\nin both low and high Reynolds number ﬂuids.\\nI. INTRODUCTION\\nArticulated swimming robots have attracted much interest\\nfrom researchers due to their effective locomotive capabilities\\nas well as the richness of their geometric structure. The basis\\nof their locomotion arises from the interaction between ac-\\ntuation of their joints and the surrounding ﬂuid environment.\\nSuch interactions depend highly on the nature of the ﬂuid,\\nbut previous work has shown that in the cases of extremely\\nlow or extremely high Reynolds number ﬂuids, a kinematic\\nsystem can be approximated, leading to great insights into\\ntrajectory planning [1].\\nEven for these idealized systems, however, it is still\\ndifﬁcult to derive optimal trajectories analytically. These\\ndifﬁculties are compounded when dealing with robots with\\nmore complex morphologies or higher-dimensional joint\\nspaces. Deep reinforcement learning (RL) has recently shown\\npromise to be an effective search strategy, as algorithms\\nhave developed to make techniques feasible on physical\\nsystems. However, the heterogeneity of the search space and\\nthe sparsity of the corresponding reward functions introduce\\nadditional challenges for motion planning with RL.\\nIn this paper, we exploit the geometric structure of three-\\nlink swimmer systems in low and high Reynolds number\\nﬂuids to restrict the search space of our reinforcement\\nlearning algorithm and learn effective locomoting gaits from\\na blank slate. We show that this approach is able to speed\\nup training time, as the robot is less likely to be trapped into\\nexecuting suboptimal gaits. At the same time, we show that\\nthe RL method is still ﬂexible enough to be optimized for\\ndifferent objectives, such as energy and speed.\\nTo the best of our knowledge, this is the ﬁrst attempt to\\nconﬁne RL policy search by utilizing the geometry of the\\nsystem at hand. This is also one of the ﬁrst attempts to the\\nlocomotion problem of articulated swimmers using model-\\nfree deep reinforcement learning.\\n1Computer Science Department, Columbia University, New York, NY\\n10027, USA {jh3916, tbd2115}@columbia.edu\\nFig. 1: A swimming snake robot comprised of three artic-\\nulated slender bodies. The coordinates (x, y, θ) denote the\\nSE(2) inertial conﬁguration of the proximal link, which also\\nhas velocities (ξx, ξy, ξθ) relative to a body-ﬁxed frame. The\\nrelative angles of the joints are denoted α = (α1, α2).\\nII. PRIOR WORK\\nA. Geometric Structure\\nIn recent decades, techniques and methods from geometric\\nmechanics have been a popular way to model and control\\nmechanical systems. A key idea is that of symmetries in a\\nsystem’s conﬁguration space, which allow for the reduction\\nof the equations of motion to a simpler form. This has been\\naddressed for general mechanical systems by [2], as well\\nas nonholonomic systems by [3]. For locomoting systems,\\ngeometric reduction is often leveraged in tandem with a\\ndecomposition of the conﬁguration variables into actuated\\nshape variables and unactuated position variables. If such\\na splitting is possible, then the conﬁguration space often\\ntakes on a ﬁber bundle structure, whereby a mapping called\\nthe connection relates trajectories between each subspace.\\nAnalysis of the connection can then give us intuition into\\nways to perform motion planning and control of the system,\\nas detailed by [4] and [5]. This mathematical structure also\\nlends itself to visualization and design tools, detailed by [6].\\nMuch of the progress in the geometric mechanics of loco-\\nmotion is predicated on the assumption that the symmetries\\nof a system coincide exactly with the position degrees of\\nfreedom. Robots that can be modeled with nonholonomic\\nconstraints are examples in which these symmetries occur.\\nNonholonomic wheeled snake robots have received consid-\\nerable attention from researchers such as [7] and [8] treating\\nthem as kinematic systems, so named because constraints\\nthat eliminate the need to consider second-order dynamics\\nwhen modeling its locomotion. This allows for the treatment\\nof the system’s locomotion, and subsequent motion planning,\\nas a result of geometric phase (see [9], [10], [11], [12]).\\nGeometric methods have also examined systems locomot-\\ning in ﬂuids. As with terrestrial systems, such a description\\narXiv:2301.13072v1  [cs.RO]  30 Jan 2023\\nis most useful if the position degrees of freedom correspond\\nto system symmetries and the rest to internal shape. For\\nsingle bodies, motion may be achieved through temporal\\ndeformation of the body’s shape. For articulated swimmers\\nlike the three-link robot shown in Fig. 1, deformation occurs\\nnaturally when joints are moved relative to each other (see\\n[1], [13], [14]), analogous to the terrestrial version of the\\nsystem.\\nArticulated swimmer robots belong to a family of gen-\\neral snake-like robots, which are characterized by a large\\nnumber of degrees of freedom and locomotion patterns\\nthat exhibit cyclic motions through coordination of their\\njoints. Therefore, snake-like robots are usually controlled\\nthrough kinematics-based methods [15], [16]. These meth-\\nods, however, often rely on hand-tuning a number of different\\nparameters, which can be costly as well as inﬂexible in new\\nenvironments.\\nB. Gait Optimization and Reinforcement Learning\\nThe problem of gait optimization has been approached\\nthrough a variety of traditional optimization methods, such\\nas evolutionary algorithms [17], gradient-based methods [18]\\nand Bayesian optimization [19]. However, these methods\\noften suffer from local optima, and while the resulting gaits\\nappear effective in locomoting the robots, they are often\\nstill quite inefﬁcient when compared to the natural motion\\nachieved by animals.\\nReinforcement learning is a data-driven method that\\nsearches for a reward-maximizing policy under a given\\nenvironment. As an algorithm based on trial-and-error, it\\nhas the advantage of not requiring a speciﬁc model of the\\nenvironment or expert knowledge of the problem. With re-\\ncent advancements in deep neural network and reinforcement\\nlearning algorithms, reinforcement learning has become a\\nuseful tool for solving robot control tasks such as walker’s lo-\\ncomotion [20], dexterous manipulation [21], and autonomous\\ndriving [22].\\nThere have been a few attempts to solve the problem of\\ngait optimization through reinforcement learning. Bing et\\nal. [23] used PPO to train a forward-locomotion controller\\nfor a wheeled snake robot and were able to generate gaits\\nthat out-perform those derived from Bayesian optimization\\nand grid search. Sharma and Kitani [24] proposed phase-\\nDDPG, where they explicitly trained a cyclic policies for a\\nwalker robot by oscillating the weight of the policy network\\nwith the phase of the robot. These methods were able to\\ngenerate fairly natural gaits on certain robots, but often failed\\nto converge to global optima as the robot environment grew\\nmore complex. For example, none of the methods were able\\nto solve the swimmer environment [25].\\nIII. MODEL AND METHODS\\nA. Swimmer Model\\nAs shown in Fig. 1, our swimmer robot consists of three\\nrigid links, each of length R, which can rotate relative to\\none another. Its conﬁguration is deﬁned by q ∈Q = G × B,\\nwhere g = (x, y, θ)T ∈G = SE(2) speciﬁes the position\\nand orientation of the ﬁrst link in an inertial frame; we\\nmeasure a link’s position at the center of the link. The\\njoint angles α = (α1, α2)T ∈B specify the links’ relative\\norientation. We can view Q as a principal ﬁber bundle,\\nin which trajectories in the shape or base space B lift to\\ntrajectories in the group G (see [11]).\\n1) Low Reynolds Number: Following the treatment of [1],\\nwe assume that the swimmer is comprised of three slender\\nbodies and suspended in a planar ﬂuid. In the low Reynolds\\nnumber case, viscous drag forces dominate inertial forces.\\nThis allows us to approximate the drag forces as linear\\nfunctions of the system’s body and shape velocities ξ and\\n˙α; we also assume that net forces acting on the system are\\nzero for all time due to damping out by drag forces. We can\\nthen derive a Pfafﬁan constraint on the swimming system’s\\nvelocities as\\nF =\\n\\uf8eb\\n\\uf8ed\\nFx\\nFy\\nFθ\\n\\uf8f6\\n\\uf8f8=\\n\\uf8eb\\n\\uf8ed\\n0\\n0\\n0\\n\\uf8f6\\n\\uf8f8= ω1(α)ξ + ω2(α) ˙α,\\n(1)\\nwhere ω1 ∈R3×3 and ω2 ∈R3×2. The variables ξ =\\n(ξx, ξy, ξθ)T give us the body velocity of the system, as\\nshown in Fig. 1. In SE(2), the mapping that takes body\\nvelocities to inertial velocities is given by ˙g = TeLgξ, where\\nTeLg =\\n\\uf8eb\\n\\uf8ed\\ncos θ\\n−sin θ\\n0\\nsin θ\\ncos θ\\n0\\n0\\n0\\n1\\n\\uf8f6\\n\\uf8f8.\\n(2)\\nThe full forms of these components can be found in [1].\\nThe general approach would be to ﬁrst compute local drag\\nforces on each link, and then combine them to ﬁnd the total\\nforce components for each of the body frame directions. In\\naddition to the system link length R, the kinematics also\\nutilize the drag constant of the ﬂuid, characterized by k.\\nSince the number of independent constraints is equal to\\nthe dimension of the group, these equations are sufﬁcient\\nto derive a kinematic connection for the system ([8]). In\\nother words, the constraint equations fully describe the ﬁrst-\\norder dynamics of the group variables in terms of the shape\\nvariables only. Thus, Eq. (3) can be rearranged to show this\\nexplicitly as the kinematic reconstruction equation:\\nξ = −A(α) ˙α = −ω−1\\n1 ω2 ˙α.\\n(3)\\nA(α) is called the local connection form, a mapping that\\ndepends only on the shape variables, in this case α1 and α2.\\n2) High Reynolds Number: The high Reynolds number\\ncase is opposite from the low Reynolds number environment\\nin that inertial forces dominate viscous forces. Despite the\\nentirely different swimming conditions, the model of the\\nswimmer robot can once again be approximated as kine-\\nmatic. A Lagrangian for the robot can be expressed in terms\\nof its kinetic energy, as there is no means of storing energy\\nor application of external forces:\\nL = 1\\n2\\n\\x00ξ\\n˙α\\n\\x01\\nM(α)\\n\\x12ξ\\n˙α\\n\\x13\\n.\\n(4)\\nThe mass matrix M is a function of the system conﬁguration\\nα, and it can be decomposed into blocks containing the\\nsystem’s local connection [8]:\\nM(α) =\\n\\x12\\nI(α)\\nI(α)A(α)\\n(I(α)A(α))T\\nm(α)\\n\\x13\\n.\\nTo derive the mass matrix M, we recognize that the\\nLagrangian of the three-link system is equal to the sum of the\\nLagrangians Li of each of the individual links. Each link has\\nan associated inertia tensor Ii dependent on the shape that\\nwe use to model it. In addition, each link has an added mass\\nMi, which arises due to the inertia of a displaced ﬂuid as a\\nbody moves through it; like the inertia tensor, Mi is solely\\na function of the body geometry. [1] gives an example of the\\nadded mass tensor for an elliptical body. The total effective\\ninertia of a single link is then Ii + Mi, which gives us a\\nLagrangian of the form\\nL =\\n3\\nX\\ni=1\\nLi =\\n3\\nX\\ni=1\\n1\\n2ξT\\ni (Ii + Mi)ξi\\n(5)\\nOnce the total Lagrangian is written down, it can be\\nrearranged into the form of Eq. (4), from which the local\\nconnection A(α) can then be extracted.\\n3) Connection Visualization: The structure of the connec-\\ntion form in Eq. (3) can be visualized in order to understand\\nthe response of ξ to input trajectories without regard to time,\\naccording to [6]. We can ﬁrst integrate each row of Eq. (3)\\nover time to obtain a measure of displacement corresponding\\nto the body frame directions. In the world frame, this measure\\nprovides the exact rotational displacement, i.e., ˙θ = ξθ for\\nthe third row, and an approximation of the translational\\ncomponent for the ﬁrst two rows. If our input trajectories\\nare periodic, we can transform this “body velocity integral”\\ninto one over the trajectory ψ : [0, T] →B in the joint\\nspace, since the kinematics are independent of input pacing.\\nStokes’ theorem can then be applied to perform a second\\ntransformation into an area integral over β, the region of the\\njoint space enclosed by ψ:\\n−\\nZ T\\n0\\nA(α(τ)) ˙α(τ) dτ = −\\nZ\\nψ\\nA(α) dα = −\\nZ\\nβ\\ndA(α).\\n(6)\\nThe integrand in the rightmost integral is the exterior deriva-\\ntive of A, computed as the curl of A in two dimensions.\\nFor example, the connection exterior derivative of Eq. (3)\\nhas three components, one for each row i given by\\ndAi(α) = ∂Ai,2\\n∂α1\\n−∂Ai,1\\n∂α2\\n,\\n(7)\\nwhere Ai,j is the element corresponding to the ith row and\\njth column of A.\\nThe magnitudes of the body-x component (ﬁrst row) of\\nthe connection exterior derivative of each swimmer over the\\nα1-α2 joint space, for a ﬁxed set of sample parameters,\\nare depicted in Fig. 2. The area integral over an enclosed\\nregion is the geometric phase, a measure of the expected\\ndisplacement in the corresponding direction. In particular, a\\nFig. 2: Visualizations of the body-x components of the\\nlocal connection’s exterior derivative for the low and high\\nReynolds swimmers, respectively. Periodic trajectories can\\nbe represented as closed curves on these surfaces, and the\\nrobot’s associated displacement corresponds to the enclosed\\nvolume.\\ntrajectory that advances in a counter-clockwise direction over\\ntime in joint space will yield positive displacement, since that\\ncorresponds to a positive area integral; negative displacement\\nis achieved with a clockwise trajectory.\\nFor both swimmers, we see that a high value of the\\nbody velocity integral, and thus a high displacement per\\ngait cycle, is generally achieved by executing gaits that\\nencircle a zero contour of these exterior derivative surfaces.\\nHowever, the optimal parameters of these gaits differ for the\\ntwo swimmers, with a larger range for the low Reynolds\\ncase and a smaller range for the high Reynolds case. In\\naddition, the means of ﬁnding a gait is not obvious when\\nthe joint angles are restricted to be smaller than the zero\\ncontour. Finally, while we do not show them here we may\\nalso be concerned with the y and θ components as well.\\nAnalytically optimizing gaits is thus equivalent to solving\\na multi-objective constrained optimization problem over a\\ncontinuous space, a task that becomes exponentially more\\ndifﬁcult with increasing system complexity.\\nB. Baseline-Guided Policy Search (BGPS)\\nBased on the geometric models of the robots, we pro-\\npose an augmented reinforcement learning algorithm called\\nBaseline-Guided Policy Search (BGPS), in which we restrict\\nthe policy search space of the learning algorithm by utilizing\\na baseline policy approximated from the geometric structure.\\n1) Robot Environment Setup: In this work, we focus on\\nlocomotion for three-link swimmer robots; the study of more\\ncomplex robots will the subject of future work. The state of\\nthe robot at time t is st = (α1, α2, θ, t) , which contains\\nboth the joint angles and orientation of the swimmer. The\\naction taken by the robot at time t is at = ( ˙α1, ˙α2), the\\nvelocities of the two joints. We investigate two choices of\\nreward functions, which corresponding to two tasks with\\ndifferent optimization goals.\\nThe ﬁrst task is to optimize the total distance the robot\\ntravels in a pre-determined direction in a given amount of\\ntime. The reward is therefore very straightforward: after the\\nrobot makes a transition (st, at, st+1), the value of the reward\\nfunction Rt is set to be\\nRt = xt+1 −xt.\\n(8)\\nThe second task is to simultaneously maximize the dis-\\ntance travelled and minimize the energy spent. We use a\\nkinetic energy metric and deﬁne the reward function as\\nRt = xt+1 −xt −β∥˙α∥,\\n(9)\\nwhere β is a coefﬁcient that controls the weight of the energy\\npenalty.\\n2) Proximal Policy Optimization: A number of reinforce-\\nment learning algorithms have been shown to be effective\\nfor different physical systems, although the comparison of\\ntheir various performances is not the focus of this paper. For\\nthis work, we choose the proximal policy optimization (PPO)\\nalgorithm by Schulman et al. [26], in which an agent seeks\\nto optimize the surrogate objective within the trust region\\nby clipping the probability ratio. PPO has been shown to\\noutperform other online policy gradient methods, with the\\nadvantage of being easy to implement.\\n3) Baseline from Geometric Structure: The key idea of\\nthis work is that we can exploit what we know about\\nthe system structure, e.g., as shown in Fig. 2, to help\\nrestrict the search space in which reinforcement learning\\noperates. Speciﬁcally, the exterior derivative plots suggest\\nthat the optimal gaits for moving forward can be roughly\\napproximated as single-frequency sinusoidal functions whose\\njoint-space loops overlay the blue ridges and whose phases\\nare large enough to encircle the widths of the same. Note\\nthat the actual optimal policies have no such restriction, e.g.\\nas single-frequency sinusoidal functions. This is particularly\\nthe case if we have joint limits that prevent the joint angles\\nfrom extending all the way out to the zero contour at the ends\\nof the ridges. However, such an approximation is sufﬁcient\\nfor formulating a baseline policy from which RL techniques\\ncan then improve upon with a large number of degrees of\\nfreedom.\\n4) RL Policy from Baseline: Once we obtain a baseline\\npolicy πbase(s) through the method described above, we then\\nuse reinforcement learning to search for a separate policy\\nFig. 3: The training curve of different action ranges for opti-\\nmizing the travelled distance for the low Reynolds swimmer.\\nRed: 0.1, orange: 0.2, cyan: 0.3, blue: 0.6.\\nπRL(s). Our eventual policy is then\\nπfinal(s) = πbase(s) + πRL(s)\\n(10)\\nThe most important reason for using a baseline is that we can\\nnow control the size of the policy search space by reducing\\nthe action range of our RL-learned policy, |πRL|. By doing\\nso, we limit the policy search to be within the vicinity of our\\nbaseline policy, thus guiding the policy search. A properly\\nsmall action range can shape the policy search space to be\\nnear convex, allowing gradient-based methods like RL to be\\nparticularly suitable.\\n5) Action Range from Geometric Structure: Given an\\nenvironment step length t, the amount of deviation δ that\\nthe robot is allowed from the baseline policy, and an action\\nrange α, we can relate these quantities as δ = αt. Thus, for\\neach action cycle of length T, the maximum deviation per\\ncycle is δtotal = αT = Nδ, where N is the number of steps\\nper cycle.\\nThe choice of action range α is another parameter whose\\nvalue can be informed by the system’s geometric structure.\\nα can be interpreted as the maximum amount that we would\\nallow the policy to “stray” away from the baseline. Since\\nthe baseline is just an approximation for the optimal policy,\\nα needs to be sufﬁciently large to allow exploration of the\\npolicy space to occur. However, the exterior derivative plots\\ncan also give us an upper bound on the action range, as there\\nis a ﬁnite distance away from our chosen baseline at which\\nthe effectiveness of an action would start to drop.\\nIV. LEARNING AND RESULTS\\nWe implement BGPS with different action ranges, and\\ncompare the performances directly with PPO and phase-\\nDDPG [24]. Our results show that our algorithm generally\\noutperforms the other methods, and that a smaller action\\nrange is able to boost the performance of the learned policy,\\nconﬁrming the importance of conﬁning the policy search\\nspace.\\nA. Parameters\\nWe implemented both a low and high Reynolds three-link\\nswimmer for our simulations. We used a link length of 0.3\\nBFG\\nPPO\\nPhase-DDPG\\nBGPS (0.6)\\nBGPS (0.3)\\nBGPS (0.2)\\nBGPS (0.15)\\nBGPS (0.1)\\nDistance\\n111.05\\n31.79\\n1.14\\n32.08\\n39.39\\n117.6\\n133.3\\n130.8\\nEnergy\\n75.08\\n28.58\\n0.08\\n21.63\\n15.61\\n29.88\\n37.58\\n85.22\\nBFG\\nPPO\\nPhase-DDPG\\nBGPS (0.6)\\nBGPS (0.3)\\nBGPS (0.2)\\nBGPS (0.15)\\nBGPS (0.1)\\nDistance\\n94.71\\n19.73\\n13.27\\n122.8\\n116.5\\n141.8\\n126.2\\n121.4\\nEnergy\\n58.75\\n13.20\\n9.62\\n9.04\\n9.47\\n72.87\\n77.31\\n76.47\\nTABLE I: The average reward of the learned policy for the low Reynolds swimmer (top) and high Reynolds swimmer\\n(bottom). BFG refers to the baseline policy that we observed from the robots’ geometric structures (no learning). PPO and\\nphase-DDPG are the main algorithms to which we compared results. BGPS refers to Baseline-Guided Policy Search (our\\nmethod), with results provided for several choices of action range for different trials.\\nm for the low Reynolds case, a nod toward the prevalence of\\nmicro-swimmers in this category. For the high Reynolds case,\\nwe set the ﬂuid density to ρ = 1 kg/m3, and treat the links as\\nellipses with semi-major axis a = 4 m and semi-minor axis\\nb = 1 m. The exterior derivative plots of the swimmers in\\nFig. 2 were obtained using the same parameter values. Our\\nenvironment step time was set to 0.04 s per step. For both\\nthe low and high Reynolds swimmer, we run separate trials\\nfor optimizing the speed with and without energy concern.\\nWe set β to 0.1 for the task of optimizing for energy usage.\\nB. Network Architecture\\nWe followed the settings outlined in Schulman et al. [26]\\nfor implementing PPO. Our policy network, which maps\\nfrom observation to action, consists of two hidden layers\\nof size 64 and a linear output layer at the end. Rectiﬁed\\nLinear units (ReLU) were used as the activation function for\\nevery layer except the output layer. Our value network has\\nthe same architecture as our policy network, except mapping\\nfrom (observation, action) to value space. No parameter is\\nshared between the two networks.\\nC. Training Settings\\nWe run our experiments on a a computer with an i7-8650U\\nCPU running at 1.90Ghz and an Nvidia GTX 1070 GPU. For\\neach given algorithms and settings, we run for 2.5 million\\ntime steps. For each single trial, our algorithm takes about 3\\nhours to run.\\nD. Results\\nTable\\nI shows the results of different algorithms for\\nlearning locomotive gaits for each swimmer. The ”Distance”\\nrow refers to the task of maximizing the distance traveled\\nper time in a given direction (the x axis), and the ”Energy”\\nrow refers to the task of locomoting the robot forward while\\nsimultaneously minimizing the energy spent.\\nBFG refers to “baseline from geometry,” which is the\\nbaseline gait we estimated by looking at the geometric model\\nof the robot. For both swimmers, we set a baseline of\\n0.6cos(t) for each joint, with a phase difference of 1 rad\\nbetween them. Baseline-Guided Policy Search (BGPS) is\\nour method, and the accompanying number on each column\\nheader marks the action range for that trial. Both PPO and\\nPhase-DDPG are learning from scratch without utilizing the\\ngeometric model, and both of them perform extremely poorly\\nFig. 4: Joint angle (top) and workspace (bottom) trajectories\\nof the low Reynolds swimmer from the best learning trian\\n(BGPS 0.15). The joint angle trajectories are similar to but\\nimprove upon the baseline derived from geometry.\\ncomparing to the other methods shown. In particular, they are\\nunable to learn a gait that performs even close to the baseline\\ngait derived from simple inspection.\\nBGPS also performs poorly when the action range is too\\nlarge, but beats all other baselines as the action range is\\nreduced. Fig. 3 shows the training curve of optimizing the\\ndistance for the low Reynolds swimmer. We can clearly see\\nfrom the plot that training tends to converge to a higher\\nreward when the action range is between 0.1 and 0.2, but\\nfails to converge when between 0.3 and 0.6. This shows that\\na smaller action range within the appropriate region is the key\\nto our algorithm’s success at locomoting the swimmer. For\\nboth the low and high Reynolds swimmers, our algorithm\\nproduced the best result for both the task of optimizing\\ndistance and of minimizing energy spent, among all the\\nmethods we tested.\\nThe joint angle and workspace trajectories of the low\\nReynolds swimmer learned from the best trial (BGPS 0.15)\\nare shown in Fig. 4. As expected, the joint angle trajectories\\nare not entirely too different from the baseline that we\\nwrote down from inspection of geometry. However, subtle\\ndifferences, such as the varying of the relative phases and\\namplitudes of the two joints over time, suggest the existence\\nof higher-frequency components that were not at all obvi-\\nous from simple inspection. The accompanying workspace\\ntrajectory maximizes the distance reward compared to the\\nother learning trials, as shown in the ﬁrst row of Table I.\\nV. CONCLUSIONS AND FUTURE WORK\\nWe have leveraged traditional motion planning techniques\\nfrom geometric mechanics to make deep reinforcement\\nlearning feasible for training articulated swimming robots.\\nSuch systems exhibit challenges, such as a policy search\\nspace with many local optima, that have previously made it\\ndifﬁcult for common DRL approaches. Our approach, which\\ncombines intuition with learning, is able to produce superior\\nresults for different robot models and different environments.\\nThe fact that our algorithm is able to work across different\\ntasks and robots suggests that this method may easily be\\ngeneralized. Other robots with similar kinematics or even\\ndynamics can beneﬁt from initialization with an informed\\nbaseline. Since the baseline need not be exact, this also opens\\npresents an opportunity for work with higher-dimensional\\nsystems for which pure optimization is very difﬁcult. Visu-\\nalization of geometry would not be necessary to determine\\nthe exact form of optimal gaits.\\nThe task of selecting a proper action range is still under\\ninvestigation. In this work we had the ability to compare\\ndifferent values of this parameter and found the best one\\nfor the given robot and environment, and the interpretation\\nof this parameter will certainly vary for other systems. Real\\nsystems would not have the luxury of trying different values\\nuntil ﬁnding the one that works best. Thus, a direct line of\\nfuture work would be to determine whether the action range\\ncan also be guided by system geometry.\\nREFERENCES\\n[1] R. L. Hatton and H. Choset, “Geometric swimming at low and high\\nreynolds numbers,” IEEE Transactions on Robotics, vol. 29, no. 3, pp.\\n615–624, 2013.\\n[2] J. E. Marsden and T. S. Ratiu, Introduction to Mechanics and Symme-\\ntry: A Basic Exposition of Classical Mechanical Systems.\\nSpringer\\nScience & Business Media, 2013, vol. 17.\\n[3] A. M. Bloch, P. S. Krishnaprasad, J. E. Marsden, and R. M. Murray,\\n“Nonholonomic mechanical systems with symmetry,” Archive for\\nRational Mechanics and Analysis, vol. 136, no. 1, pp. 21–99, 1996.\\n[4] S. D. Kelly and R. M. Murray, “The geometry and control of\\ndissipative systems,” in Proceedings of the 35th IEEE Conference on\\nDecision and Control, vol. 1.\\nIEEE, 1996, pp. 981–986.\\n[5] J. P. Ostrowski and J. W. Burdick, “The geometric mechanics of\\nundulatory robotic locomotion,” The International Journal of Robotics\\nResearch, vol. 17, no. 7, pp. 683–701, 1998.\\n[6] R. L. Hatton and H. Choset, “Geometric motion planning: The local\\nconnection, stokes’ theorem, and the importance of coordinate choice,”\\nThe International Journal of Robotics Research, vol. 30, no. 8, pp.\\n988–1014, 2011.\\n[7] J. Ostrowski, “Computing reduced equations for robotic systems\\nwith constraints and symmetries,” Robotics and Automation, IEEE\\nTransactions on, vol. 15, no. 1, pp. 111–123, Feb 1999.\\n[8] E. A. Shammas, H. Choset, and A. A. Rizzi, “Geometric motion plan-\\nning analysis for two classes of underactuated mechanical systems,”\\nThe International Journal of Robotics Research, vol. 26, no. 10, pp.\\n1043–1073, 2007.\\n[9] R. M. Murray and S. S. Sastry, “Nonholonomic motion planning:\\nSteering using sinusoids,” IEEE Transactions on Automatic Control,\\nvol. 38, no. 5, pp. 700–716, 1993.\\n[10] R. Mukherjee and D. P. Anderson, “Nonholonomic motion planning\\nusing stoke’s theorem,” in Robotics and Automation, 1993. Proceed-\\nings., 1993 IEEE International Conference on. IEEE, 1993, pp. 802–\\n809.\\n[11] S. D. Kelly and R. M. Murray, “Geometric phases and robotic\\nlocomotion,” Journal of Robotic Systems, vol. 12, no. 6, pp. 417–431,\\n1995.\\n[12] F. Bullo and K. M. Lynch, “Kinematic controllability for decou-\\npled trajectory planning in underactuated mechanical systems,” IEEE\\nTransactions on Robotics and Automation, vol. 17, no. 4, pp. 402–412,\\n2001.\\n[13] J. B. Melli, C. W. Rowley, and D. S. Rufat, “Motion planning for an\\narticulated body in a perfect planar ﬂuid,” SIAM Journal on applied\\ndynamical systems, vol. 5, no. 4, pp. 650–669, 2006.\\n[14] L. Burton, R. L. Hatton, H. Choset, and A. Hosoi, “Two-link swim-\\nming using buoyant orientation,” Physics of Fluids, vol. 22, no. 9, p.\\n091703, 2010.\\n[15] Z. Zuo, Z. Wang, B. Li, and S. Ma, “Serpentine locomotion of a\\nsnake-like robot in water environment,” 03 2009, pp. 25 – 30.\\n[16] M. Tesch, K. Lipkin, I. Brown, R. Hatton, A. Peck, J. Rembisz,\\nand H. Choset, “Parameterized and scripted gaits for modular snake\\nrobots,” Advanced Robotics, vol. 23, no. 9, pp. 1131–1158, 2009.\\n[17] S. Chernova and M. Veloso, “An evolutionary approach to gait learning\\nfor four-legged robots,” in 2004 IEEE/RSJ International Conference\\non Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566),\\nvol. 3, Sep. 2004, pp. 2562–2567 vol.3.\\n[18] N. Kohl and P. Stone, “Machine learning for fast quadrupedal loco-\\nmotion,” in AAAI, 2004.\\n[19] R. Calandra, N. Gopalan, A. Seyfarth, J. Peters, and M. Deisenroth,\\n“Bayesian gait optimization for bipedal locomotion,” 02 2014, pp.\\n274–290.\\n[20] X. B. Peng, G. Berseth, K. Yin, and M. van de Panne, “Deeploco:\\nDynamic locomotion skills using hierarchical deep reinforcement\\nlearning,” ACM Transactions on Graphics (Proc. SIGGRAPH 2017),\\nvol. 36, no. 4, 2017.\\n[21] A. Rajeswaran, V. Kumar, A. Gupta, J. Schulman, E. Todorov,\\nand S. Levine, “Learning complex dexterous manipulation with\\ndeep\\nreinforcement\\nlearning\\nand\\ndemonstrations,”\\nCoRR,\\nvol.\\nabs/1709.10087, 2017. [Online]. Available: http://arxiv.org/abs/1709.\\n10087\\n[22] P. Long, T. Fan, X. Liao, W. Liu, H. Zhang, and J. Pan, “Towards\\noptimally decentralized multi-robot collision avoidance via deep rein-\\nforcement learning,” 2017.\\n[23] Z. Bing, C. Lemke, Z. Jiang, K. Huang, and A. Knoll, “Energy-\\nefﬁcient slithering gait exploration for a snake-like robot based on\\nreinforcement learning,” 2019.\\n[24] A. Sharma and K. M. Kitani, “Phase-parametric policies for reinforce-\\nment learning in cyclic environments,” in AAAI, 2018.\\n[25] R. Coulom, “Reinforcement learning using neural networks, with\\napplications to motor control,” 2002.\\n[26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\\n“Proximal\\npolicy\\noptimization\\nalgorithms,”\\narXiv\\npreprint\\narXiv:1707.06347, 2017.\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"reinforcement learning and robotics\",  # This is the search term\n",
    "    load_max_docs=2                                # Maximum number of results to fetch\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8b683c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25d57d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\bapan bairagya\\miniconda3\\envs\\agentic_ai\\lib\\site-packages (from beautifulsoup4->wikipedia) (4.14.0)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11785 sha256=9d25d6f85d6c5ffc6de344cac9e7d85f08a83c36d803e0b38e769ed2123c4be4\n",
      "  Stored in directory: c:\\users\\bapan bairagya\\appdata\\local\\pip\\cache\\wheels\\63\\47\\7c\\a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'wikipedia' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wikipedia'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84abd864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Agentic AI', 'summary': 'Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks without human intervention. The independent systems automatically respond to conditions, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence.', 'source': 'https://en.wikipedia.org/wiki/Agentic_AI'}, page_content=\"Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks without human intervention. The independent systems automatically respond to conditions, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence. \\n\\n\\n== Overview ==\\nThe core concept of agentic AI is the use of AI agents to perform automated tasks but without human intervention. While robotic process automation (RPA) and AI agents can be programmed to automate specific tasks or support rule-based decisions, the rules are usually fixed. Agentic AI operates independently, making decisions through continuous learning and analysis of external data and complex data sets. Functioning agents can require various AI techniques, such as natural language processing, machine learning (ML), and computer vision, depending on the environment.\\nParticularly, reinforcement learning (RL) is essential in assisting agentic AI in making self-directed choices by supporting agents in learning best actions through the trial-and-error method. Agents using RL continuously to explore their surroundings will be given rewards or punishment for their actions, which refines their decision-making capability over time. All the while deep learning, as opposed to rule-based methods, supports agentic AI through multi-layered neural networks to learn features from extensive and complex sets of data. RL combined with deep learning thus supports the use of AI agents to adjust dynamically, optimize procedures, and engage in complex behaviors with limited control from humans.\\n\\n\\n== History ==\\nSome scholars trace the conceptual roots of agentic AI to Alan Turing's mid-20th century work with machine intelligence and Norbert Wiener's work on feedback systems. The term agent-based process management system was used as far back as 1998 to describe the concept of using autonomous agents for business process management. The psychological principle of agency was also discussed in the 2008 work of sociologist Albert Bandura, who studied how humans can shape their environments. This research would shape how humans modeled and developed artificial intelligence agents. \\nSome additional milestones of agentic AI include IBM's Deep Blue, demonstrating how agency could work within a confined domain, advances in machine learning in the 2000s, AI being integrated into robotics, and the rise of generative AI such as OpenAI's GPT models and Salesforce's Agentforce platform.\\nIn the last decade, significant advances in AI have spurred the development of agentic AI. Breakthroughs in deep learning, reinforcement learning, and neural networks allowed AI systems to learn on their own and make decision with minimal human guidance. Consilience of agentic AI across autonomous transportation, industrial automation, and tailored healthcare has also supported its viability. Self-driving cars use agentic AI to handle complex road scenarios.\\nIn 2025, research firm Forrester named agentic AI a top emerging technology for 2025.\\n\\n\\n== Applications ==\\nApplications using agentic AI include:\\n\\nSoftware development - AI coding agents can write large pieces of code, and review it. Agents can even perform non-code related tasks such as reverse engineering specifications from code.\\nCustomer support automation - AI agents can improve customer service by improving the ability of chatbots to answer a wider variety of questions, rather than having a limited set of answers pre-programmed by humans.\\nEnterprise workflows - AI agents can automatically automate routine tasks by processing pooled data, as opposed to a company needing APIs preprogrammed for specific tasks.\\nCybersecurity and threat detection - AI agents deployed for cybersecurity can automatically detect and mitigate threats in\"),\n",
       " Document(metadata={'title': 'Intelligent agent', 'summary': 'In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".', 'source': 'https://en.wikipedia.org/wiki/Intelligent_agent'}, page_content='In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n\\n== Intelligent agents as the foundation of AI ==\\n\\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\\n\\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\\nArtificial Intelligence (as a field): The study and creation of these rational agents.\\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system\\'s ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\\nDefining AI in terms of intelligent agents offers several key advantages:\\n\\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle\\'s Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare differen')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## wikipedia loader\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "wiki_loader = WikipediaLoader(query=\"Agentic AI\", load_max_docs=2)\n",
    "wiki_docs = wiki_loader.load()\n",
    "wiki_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709784d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
